
Train 11916, Val 1583, Test 618
TOTAL TRAINABLE PARAMETERS : 97319
RATIOS: Temporal  88.5% , Classifier  11.5%
EPOCH 1/10
Step [100/11916], Loss: 1.9383, Acc : 40.69
Step [200/11916], Loss: 1.2639, Acc : 61.82
Step [300/11916], Loss: 0.9717, Acc : 70.41
Step [400/11916], Loss: 0.8089, Acc : 75.04
Step [500/11916], Loss: 0.7038, Acc : 77.97
Step [600/11916], Loss: 0.6270, Acc : 80.19
Step [700/11916], Loss: 0.5699, Acc : 81.82
Step [800/11916], Loss: 0.5259, Acc : 83.06
Step [900/11916], Loss: 0.4909, Acc : 84.07
Step [1000/11916], Loss: 0.4612, Acc : 84.89
Step [1100/11916], Loss: 0.4363, Acc : 85.61
Step [1200/11916], Loss: 0.4144, Acc : 86.25
Step [1300/11916], Loss: 0.3960, Acc : 86.77
Step [1400/11916], Loss: 0.3794, Acc : 87.24
Step [1500/11916], Loss: 0.3655, Acc : 87.64
Step [1600/11916], Loss: 0.3529, Acc : 88.00
Step [1700/11916], Loss: 0.3410, Acc : 88.35
Step [1800/11916], Loss: 0.3309, Acc : 88.63
Step [1900/11916], Loss: 0.3213, Acc : 88.91
Step [2000/11916], Loss: 0.3128, Acc : 89.14
Step [2100/11916], Loss: 0.3048, Acc : 89.37
Step [2200/11916], Loss: 0.2976, Acc : 89.57
Step [2300/11916], Loss: 0.2910, Acc : 89.76
Step [2400/11916], Loss: 0.2848, Acc : 89.94
Step [2500/11916], Loss: 0.2789, Acc : 90.11
Step [2600/11916], Loss: 0.2730, Acc : 90.27
Step [2700/11916], Loss: 0.2677, Acc : 90.42
Step [2800/11916], Loss: 0.2628, Acc : 90.56
Step [2900/11916], Loss: 0.2581, Acc : 90.70
Step [3000/11916], Loss: 0.2537, Acc : 90.83
Step [3100/11916], Loss: 0.2498, Acc : 90.94
Step [3200/11916], Loss: 0.2457, Acc : 91.06
Step [3300/11916], Loss: 0.2422, Acc : 91.17
Step [3400/11916], Loss: 0.2386, Acc : 91.28
Step [3500/11916], Loss: 0.2349, Acc : 91.39
Step [3600/11916], Loss: 0.2316, Acc : 91.49
Step [3700/11916], Loss: 0.2285, Acc : 91.58
Step [3800/11916], Loss: 0.2256, Acc : 91.67
Step [3900/11916], Loss: 0.2228, Acc : 91.76
Step [4000/11916], Loss: 0.2201, Acc : 91.84
Step [4100/11916], Loss: 0.2176, Acc : 91.92
Step [4200/11916], Loss: 0.2151, Acc : 91.99
Step [4300/11916], Loss: 0.2127, Acc : 92.06
Step [4400/11916], Loss: 0.2105, Acc : 92.13
Step [4500/11916], Loss: 0.2083, Acc : 92.20
Step [4600/11916], Loss: 0.2061, Acc : 92.27
Step [4700/11916], Loss: 0.2041, Acc : 92.33
Step [4800/11916], Loss: 0.2021, Acc : 92.38
Step [4900/11916], Loss: 0.2002, Acc : 92.45
Step [5000/11916], Loss: 0.1982, Acc : 92.51
Step [5100/11916], Loss: 0.1962, Acc : 92.58
Step [5200/11916], Loss: 0.1944, Acc : 92.63
Step [5300/11916], Loss: 0.1927, Acc : 92.68
Step [5400/11916], Loss: 0.1910, Acc : 92.74
Step [5500/11916], Loss: 0.1893, Acc : 92.79
Step [5600/11916], Loss: 0.1878, Acc : 92.83
Step [5700/11916], Loss: 0.1863, Acc : 92.88
Step [5800/11916], Loss: 0.1848, Acc : 92.92
Step [5900/11916], Loss: 0.1833, Acc : 92.97
Step [6000/11916], Loss: 0.1818, Acc : 93.02
Step [6100/11916], Loss: 0.1804, Acc : 93.06
Step [6200/11916], Loss: 0.1790, Acc : 93.10
Step [6300/11916], Loss: 0.1777, Acc : 93.15
Step [6400/11916], Loss: 0.1764, Acc : 93.19
Step [6500/11916], Loss: 0.1752, Acc : 93.23
Step [6600/11916], Loss: 0.1741, Acc : 93.27
Step [6700/11916], Loss: 0.1729, Acc : 93.30
Step [6800/11916], Loss: 0.1717, Acc : 93.34
Step [6900/11916], Loss: 0.1706, Acc : 93.37
Step [7000/11916], Loss: 0.1695, Acc : 93.41
Step [7100/11916], Loss: 0.1684, Acc : 93.44
Step [7200/11916], Loss: 0.1674, Acc : 93.47
Step [7300/11916], Loss: 0.1664, Acc : 93.50
Step [7400/11916], Loss: 0.1654, Acc : 93.53
Step [7500/11916], Loss: 0.1643, Acc : 93.57
Step [7600/11916], Loss: 0.1633, Acc : 93.60
Step [7700/11916], Loss: 0.1624, Acc : 93.63
Step [7800/11916], Loss: 0.1614, Acc : 93.66
Step [7900/11916], Loss: 0.1605, Acc : 93.69
Step [8000/11916], Loss: 0.1595, Acc : 93.71
Step [8100/11916], Loss: 0.1587, Acc : 93.74
Step [8200/11916], Loss: 0.1578, Acc : 93.77
Step [8300/11916], Loss: 0.1570, Acc : 93.80
Step [8400/11916], Loss: 0.1562, Acc : 93.83
Step [8500/11916], Loss: 0.1555, Acc : 93.85
Step [8600/11916], Loss: 0.1547, Acc : 93.88
Step [8700/11916], Loss: 0.1539, Acc : 93.90
Step [8800/11916], Loss: 0.1531, Acc : 93.92
Step [8900/11916], Loss: 0.1524, Acc : 93.94
Step [9000/11916], Loss: 0.1517, Acc : 93.97
Step [9100/11916], Loss: 0.1510, Acc : 93.99
Step [9200/11916], Loss: 0.1503, Acc : 94.01
Step [9300/11916], Loss: 0.1495, Acc : 94.04
Step [9400/11916], Loss: 0.1488, Acc : 94.06
Step [9500/11916], Loss: 0.1481, Acc : 94.08
Step [9600/11916], Loss: 0.1474, Acc : 94.10
Step [9700/11916], Loss: 0.1467, Acc : 94.13
Step [9800/11916], Loss: 0.1461, Acc : 94.15
Step [9900/11916], Loss: 0.1455, Acc : 94.17
Step [10000/11916], Loss: 0.1448, Acc : 94.19
Step [10100/11916], Loss: 0.1442, Acc : 94.21
Step [10200/11916], Loss: 0.1436, Acc : 94.23
Step [10300/11916], Loss: 0.1430, Acc : 94.25
Step [10400/11916], Loss: 0.1424, Acc : 94.27
Step [10500/11916], Loss: 0.1418, Acc : 94.29
Step [10600/11916], Loss: 0.1412, Acc : 94.30
Step [10700/11916], Loss: 0.1407, Acc : 94.32
Step [10800/11916], Loss: 0.1401, Acc : 94.34
Step [10900/11916], Loss: 0.1395, Acc : 94.36
Step [11000/11916], Loss: 0.1390, Acc : 94.38
Step [11100/11916], Loss: 0.1385, Acc : 94.39
Step [11200/11916], Loss: 0.1379, Acc : 94.41
Step [11300/11916], Loss: 0.1374, Acc : 94.43
Step [11400/11916], Loss: 0.1368, Acc : 94.45
Step [11500/11916], Loss: 0.1364, Acc : 94.46
Step [11600/11916], Loss: 0.1358, Acc : 94.48
Step [11700/11916], Loss: 0.1353, Acc : 94.49
Step [11800/11916], Loss: 0.1348, Acc : 94.51
Step [11900/11916], Loss: 0.1343, Acc : 94.53
Validation . . .
Loss 0.2435,  Acc 89.99,  IoU 0.5550
Testing best epoch . . .
Loss 0.3424,  Acc 90.15,  IoU 0.5512
EPOCH 2/10
Step [100/11916], Loss: 0.0865, Acc : 96.07
Step [200/11916], Loss: 0.0816, Acc : 96.23
Step [300/11916], Loss: 0.0770, Acc : 96.43
Step [400/11916], Loss: 0.0778, Acc : 96.38
Step [500/11916], Loss: 0.0769, Acc : 96.39
Step [600/11916], Loss: 0.0770, Acc : 96.41
Step [700/11916], Loss: 0.0771, Acc : 96.41
Step [800/11916], Loss: 0.0763, Acc : 96.46
Step [900/11916], Loss: 0.0759, Acc : 96.46
Step [1000/11916], Loss: 0.0759, Acc : 96.47
Step [1100/11916], Loss: 0.0765, Acc : 96.44
Step [1200/11916], Loss: 0.0759, Acc : 96.47
Step [1300/11916], Loss: 0.0756, Acc : 96.46
Step [1400/11916], Loss: 0.0751, Acc : 96.48
Step [1500/11916], Loss: 0.0753, Acc : 96.48
Step [1600/11916], Loss: 0.0747, Acc : 96.49
Step [1700/11916], Loss: 0.0747, Acc : 96.49
Step [1800/11916], Loss: 0.0746, Acc : 96.50
Step [1900/11916], Loss: 0.0742, Acc : 96.52
Step [2000/11916], Loss: 0.0742, Acc : 96.52
Step [2100/11916], Loss: 0.0741, Acc : 96.52
Step [2200/11916], Loss: 0.0741, Acc : 96.52
Step [2300/11916], Loss: 0.0742, Acc : 96.51
Step [2400/11916], Loss: 0.0740, Acc : 96.52
Step [2500/11916], Loss: 0.0739, Acc : 96.53
Step [2600/11916], Loss: 0.0741, Acc : 96.53
Step [2700/11916], Loss: 0.0738, Acc : 96.53
Step [2800/11916], Loss: 0.0738, Acc : 96.52
Step [2900/11916], Loss: 0.0737, Acc : 96.53
Step [3000/11916], Loss: 0.0735, Acc : 96.54
Step [3100/11916], Loss: 0.0734, Acc : 96.54
Step [3200/11916], Loss: 0.0733, Acc : 96.55
Step [3300/11916], Loss: 0.0732, Acc : 96.55
Step [3400/11916], Loss: 0.0730, Acc : 96.56
Step [3500/11916], Loss: 0.0727, Acc : 96.57
Step [3600/11916], Loss: 0.0727, Acc : 96.57
Step [3700/11916], Loss: 0.0726, Acc : 96.58
Step [3800/11916], Loss: 0.0725, Acc : 96.58
Step [3900/11916], Loss: 0.0725, Acc : 96.58
Step [4000/11916], Loss: 0.0724, Acc : 96.58
Step [4100/11916], Loss: 0.0722, Acc : 96.59
Step [4200/11916], Loss: 0.0721, Acc : 96.59
Step [4300/11916], Loss: 0.0720, Acc : 96.59
Step [4400/11916], Loss: 0.0719, Acc : 96.60
Step [4500/11916], Loss: 0.0718, Acc : 96.60
Step [4600/11916], Loss: 0.0718, Acc : 96.60
Step [4700/11916], Loss: 0.0717, Acc : 96.61
Step [4800/11916], Loss: 0.0716, Acc : 96.61
Step [4900/11916], Loss: 0.0717, Acc : 96.61
Step [5000/11916], Loss: 0.0716, Acc : 96.61
Step [5100/11916], Loss: 0.0716, Acc : 96.61
Step [5200/11916], Loss: 0.0715, Acc : 96.62
Step [5300/11916], Loss: 0.0714, Acc : 96.62
Step [5400/11916], Loss: 0.0713, Acc : 96.63
Step [5500/11916], Loss: 0.0712, Acc : 96.63
Step [5600/11916], Loss: 0.0711, Acc : 96.64
Step [5700/11916], Loss: 0.0709, Acc : 96.64
Step [5800/11916], Loss: 0.0708, Acc : 96.65
Step [5900/11916], Loss: 0.0707, Acc : 96.65
Step [6000/11916], Loss: 0.0706, Acc : 96.65
Step [6100/11916], Loss: 0.0705, Acc : 96.65
Step [6200/11916], Loss: 0.0705, Acc : 96.66
Step [6300/11916], Loss: 0.0704, Acc : 96.66
Step [6400/11916], Loss: 0.0704, Acc : 96.66
Step [6500/11916], Loss: 0.0703, Acc : 96.66
Step [6600/11916], Loss: 0.0703, Acc : 96.66
Step [6700/11916], Loss: 0.0702, Acc : 96.66
Step [6800/11916], Loss: 0.0701, Acc : 96.67
Step [6900/11916], Loss: 0.0700, Acc : 96.67
Step [7000/11916], Loss: 0.0700, Acc : 96.67
Step [7100/11916], Loss: 0.0699, Acc : 96.67
Step [7200/11916], Loss: 0.0698, Acc : 96.67
Step [7300/11916], Loss: 0.0697, Acc : 96.68
Step [7400/11916], Loss: 0.0697, Acc : 96.68
Step [7500/11916], Loss: 0.0696, Acc : 96.68
Step [7600/11916], Loss: 0.0695, Acc : 96.69
Step [7700/11916], Loss: 0.0694, Acc : 96.69
Step [7800/11916], Loss: 0.0694, Acc : 96.70
Step [7900/11916], Loss: 0.0693, Acc : 96.70
Step [8000/11916], Loss: 0.0692, Acc : 96.70
Step [8100/11916], Loss: 0.0691, Acc : 96.71
Step [8200/11916], Loss: 0.0690, Acc : 96.71
Step [8300/11916], Loss: 0.0690, Acc : 96.71
Step [8400/11916], Loss: 0.0690, Acc : 96.71
Step [8500/11916], Loss: 0.0689, Acc : 96.72
Step [8600/11916], Loss: 0.0688, Acc : 96.72
Step [8700/11916], Loss: 0.0687, Acc : 96.72
Step [8800/11916], Loss: 0.0687, Acc : 96.72
Step [8900/11916], Loss: 0.0686, Acc : 96.73
Step [9000/11916], Loss: 0.0686, Acc : 96.73
Step [9100/11916], Loss: 0.0685, Acc : 96.74
Step [9200/11916], Loss: 0.0683, Acc : 96.75
Step [9300/11916], Loss: 0.0682, Acc : 96.75
Step [9400/11916], Loss: 0.0682, Acc : 96.75
Step [9500/11916], Loss: 0.0681, Acc : 96.75
Step [9600/11916], Loss: 0.0681, Acc : 96.75
Step [9700/11916], Loss: 0.0681, Acc : 96.76
Step [9800/11916], Loss: 0.0681, Acc : 96.76
Step [9900/11916], Loss: 0.0680, Acc : 96.76
Step [10000/11916], Loss: 0.0680, Acc : 96.76
Step [10100/11916], Loss: 0.0680, Acc : 96.76
Step [10200/11916], Loss: 0.0679, Acc : 96.77
Step [10300/11916], Loss: 0.0677, Acc : 96.77
Step [10400/11916], Loss: 0.0677, Acc : 96.77
Step [10500/11916], Loss: 0.0676, Acc : 96.77
Step [10600/11916], Loss: 0.0676, Acc : 96.77
Step [10700/11916], Loss: 0.0675, Acc : 96.78
Step [10800/11916], Loss: 0.0674, Acc : 96.78
Step [10900/11916], Loss: 0.0673, Acc : 96.79
Step [11000/11916], Loss: 0.0673, Acc : 96.79
Step [11100/11916], Loss: 0.0672, Acc : 96.79
Step [11200/11916], Loss: 0.0671, Acc : 96.79
Step [11300/11916], Loss: 0.0671, Acc : 96.79
Step [11400/11916], Loss: 0.0670, Acc : 96.80
Step [11500/11916], Loss: 0.0670, Acc : 96.80
Step [11600/11916], Loss: 0.0669, Acc : 96.80
Step [11700/11916], Loss: 0.0669, Acc : 96.80
Step [11800/11916], Loss: 0.0668, Acc : 96.81
Step [11900/11916], Loss: 0.0668, Acc : 96.81
Validation . . .
Loss 0.2515,  Acc 90.24,  IoU 0.5530
Testing best epoch . . .
Loss 0.3424,  Acc 90.15,  IoU 0.5512
EPOCH 3/10
Step [100/11916], Loss: 0.0809, Acc : 96.24
Step [200/11916], Loss: 0.0772, Acc : 96.41
Step [300/11916], Loss: 0.0746, Acc : 96.49
Step [400/11916], Loss: 0.0742, Acc : 96.44
Step [500/11916], Loss: 0.0752, Acc : 96.42
Step [600/11916], Loss: 0.0748, Acc : 96.44
Step [700/11916], Loss: 0.0747, Acc : 96.45
Step [800/11916], Loss: 0.0741, Acc : 96.49
Step [900/11916], Loss: 0.0743, Acc : 96.47
Step [1000/11916], Loss: 0.0741, Acc : 96.48
Step [1100/11916], Loss: 0.0745, Acc : 96.47
Step [1200/11916], Loss: 0.0749, Acc : 96.45
Step [1300/11916], Loss: 0.0746, Acc : 96.46
Step [1400/11916], Loss: 0.0748, Acc : 96.46
Step [1500/11916], Loss: 0.0749, Acc : 96.45
Step [1600/11916], Loss: 0.0748, Acc : 96.46
Step [1700/11916], Loss: 0.0752, Acc : 96.44
Step [1800/11916], Loss: 0.0750, Acc : 96.45
Step [1900/11916], Loss: 0.0750, Acc : 96.44
Step [2000/11916], Loss: 0.0750, Acc : 96.43
Step [2100/11916], Loss: 0.0748, Acc : 96.44
Step [2200/11916], Loss: 0.0748, Acc : 96.44
Step [2300/11916], Loss: 0.0748, Acc : 96.44
Step [2400/11916], Loss: 0.0745, Acc : 96.46
Step [2500/11916], Loss: 0.0743, Acc : 96.47
Step [2600/11916], Loss: 0.0739, Acc : 96.49
Step [2700/11916], Loss: 0.0736, Acc : 96.50
Step [2800/11916], Loss: 0.0736, Acc : 96.51
Step [2900/11916], Loss: 0.0735, Acc : 96.51
Step [3000/11916], Loss: 0.0734, Acc : 96.52
Step [3100/11916], Loss: 0.0732, Acc : 96.53
Step [3200/11916], Loss: 0.0732, Acc : 96.53
Step [3300/11916], Loss: 0.0731, Acc : 96.53
Step [3400/11916], Loss: 0.0731, Acc : 96.53
Step [3500/11916], Loss: 0.0728, Acc : 96.55
Step [3600/11916], Loss: 0.0727, Acc : 96.55
Step [3700/11916], Loss: 0.0725, Acc : 96.56
Step [3800/11916], Loss: 0.0725, Acc : 96.56
Step [3900/11916], Loss: 0.0723, Acc : 96.57
Step [4000/11916], Loss: 0.0722, Acc : 96.57
Step [4100/11916], Loss: 0.0720, Acc : 96.58
Step [4200/11916], Loss: 0.0719, Acc : 96.59
Step [4300/11916], Loss: 0.0718, Acc : 96.59
Step [4400/11916], Loss: 0.0717, Acc : 96.59
Step [4500/11916], Loss: 0.0716, Acc : 96.59
Step [4600/11916], Loss: 0.0716, Acc : 96.59
Step [4700/11916], Loss: 0.0716, Acc : 96.59
Step [4800/11916], Loss: 0.0715, Acc : 96.59
Step [4900/11916], Loss: 0.0714, Acc : 96.60
Step [5000/11916], Loss: 0.0713, Acc : 96.61
Step [5100/11916], Loss: 0.0712, Acc : 96.61
Step [5200/11916], Loss: 0.0712, Acc : 96.61
Step [5300/11916], Loss: 0.0711, Acc : 96.61
Step [5400/11916], Loss: 0.0711, Acc : 96.61
Step [5500/11916], Loss: 0.0709, Acc : 96.62
Step [5600/11916], Loss: 0.0709, Acc : 96.62
Step [5700/11916], Loss: 0.0707, Acc : 96.63
Step [5800/11916], Loss: 0.0707, Acc : 96.63
Step [5900/11916], Loss: 0.0707, Acc : 96.63
Step [6000/11916], Loss: 0.0706, Acc : 96.64
Step [6100/11916], Loss: 0.0705, Acc : 96.64
Step [6200/11916], Loss: 0.0704, Acc : 96.64
Step [6300/11916], Loss: 0.0704, Acc : 96.64
Step [6400/11916], Loss: 0.0703, Acc : 96.65
Step [6500/11916], Loss: 0.0703, Acc : 96.65
Step [6600/11916], Loss: 0.0703, Acc : 96.65
Step [6700/11916], Loss: 0.0702, Acc : 96.65
Step [6800/11916], Loss: 0.0700, Acc : 96.66
Step [6900/11916], Loss: 0.0700, Acc : 96.66
Step [7000/11916], Loss: 0.0699, Acc : 96.66
Step [7100/11916], Loss: 0.0699, Acc : 96.66
Step [7200/11916], Loss: 0.0698, Acc : 96.67
Step [7300/11916], Loss: 0.0697, Acc : 96.67
Step [7400/11916], Loss: 0.0697, Acc : 96.67
Step [7500/11916], Loss: 0.0695, Acc : 96.68
Step [7600/11916], Loss: 0.0695, Acc : 96.68
Step [7700/11916], Loss: 0.0695, Acc : 96.68
Step [7800/11916], Loss: 0.0693, Acc : 96.69
Step [7900/11916], Loss: 0.0693, Acc : 96.68
Step [8000/11916], Loss: 0.0692, Acc : 96.69
Step [8100/11916], Loss: 0.0691, Acc : 96.69
Step [8200/11916], Loss: 0.0690, Acc : 96.70
Step [8300/11916], Loss: 0.0690, Acc : 96.70
Step [8400/11916], Loss: 0.0689, Acc : 96.70
Step [8500/11916], Loss: 0.0689, Acc : 96.71
Step [8600/11916], Loss: 0.0688, Acc : 96.71
Step [8700/11916], Loss: 0.0687, Acc : 96.71
Step [8800/11916], Loss: 0.0686, Acc : 96.71
Step [8900/11916], Loss: 0.0685, Acc : 96.72
Step [9000/11916], Loss: 0.0684, Acc : 96.72
Step [9100/11916], Loss: 0.0683, Acc : 96.72
Step [9200/11916], Loss: 0.0683, Acc : 96.73
Step [9300/11916], Loss: 0.0682, Acc : 96.73
Step [9400/11916], Loss: 0.0681, Acc : 96.73
Step [9500/11916], Loss: 0.0680, Acc : 96.74
Step [9600/11916], Loss: 0.0680, Acc : 96.74
Step [9700/11916], Loss: 0.0679, Acc : 96.74
Step [9800/11916], Loss: 0.0678, Acc : 96.74
Step [9900/11916], Loss: 0.0678, Acc : 96.75
Step [10000/11916], Loss: 0.0676, Acc : 96.75
Step [10100/11916], Loss: 0.0675, Acc : 96.76
Step [10200/11916], Loss: 0.0675, Acc : 96.76
Step [10300/11916], Loss: 0.0675, Acc : 96.76
Step [10400/11916], Loss: 0.0674, Acc : 96.76
Step [10500/11916], Loss: 0.0674, Acc : 96.76
Step [10600/11916], Loss: 0.0673, Acc : 96.76
Step [10700/11916], Loss: 0.0672, Acc : 96.77
Step [10800/11916], Loss: 0.0671, Acc : 96.77
Step [10900/11916], Loss: 0.0671, Acc : 96.77
Step [11000/11916], Loss: 0.0671, Acc : 96.77
Step [11100/11916], Loss: 0.0670, Acc : 96.78
Step [11200/11916], Loss: 0.0669, Acc : 96.78
Step [11300/11916], Loss: 0.0669, Acc : 96.78
Step [11400/11916], Loss: 0.0668, Acc : 96.78
Step [11500/11916], Loss: 0.0668, Acc : 96.79
Step [11600/11916], Loss: 0.0667, Acc : 96.79
Step [11700/11916], Loss: 0.0666, Acc : 96.79
Step [11800/11916], Loss: 0.0665, Acc : 96.80
Step [11900/11916], Loss: 0.0664, Acc : 96.80
Validation . . .
Loss 0.2380,  Acc 90.84,  IoU 0.5568
Testing best epoch . . .
Loss 0.4677,  Acc 89.91,  IoU 0.5504
EPOCH 4/10
Step [100/11916], Loss: 0.0579, Acc : 97.06
Step [200/11916], Loss: 0.0555, Acc : 97.26
Step [300/11916], Loss: 0.0564, Acc : 97.26
Step [400/11916], Loss: 0.0569, Acc : 97.26
Step [500/11916], Loss: 0.0563, Acc : 97.28
Step [600/11916], Loss: 0.0559, Acc : 97.29
Step [700/11916], Loss: 0.0559, Acc : 97.29
Step [800/11916], Loss: 0.0568, Acc : 97.27
Step [900/11916], Loss: 0.0569, Acc : 97.27
Step [1000/11916], Loss: 0.0566, Acc : 97.27
Step [1100/11916], Loss: 0.0569, Acc : 97.26
Step [1200/11916], Loss: 0.0565, Acc : 97.26
Step [1300/11916], Loss: 0.0569, Acc : 97.23
Step [1400/11916], Loss: 0.0569, Acc : 97.25
Step [1500/11916], Loss: 0.0568, Acc : 97.25
Step [1600/11916], Loss: 0.0568, Acc : 97.24
Step [1700/11916], Loss: 0.0569, Acc : 97.23
Step [1800/11916], Loss: 0.0568, Acc : 97.24
Step [1900/11916], Loss: 0.0569, Acc : 97.24
Step [2000/11916], Loss: 0.0567, Acc : 97.25
Step [2100/11916], Loss: 0.0567, Acc : 97.24
Step [2200/11916], Loss: 0.0566, Acc : 97.25
Step [2300/11916], Loss: 0.0564, Acc : 97.25
Step [2400/11916], Loss: 0.0564, Acc : 97.26
Step [2500/11916], Loss: 0.0563, Acc : 97.25
Step [2600/11916], Loss: 0.0564, Acc : 97.25
Step [2700/11916], Loss: 0.0567, Acc : 97.24
Step [2800/11916], Loss: 0.0566, Acc : 97.24
Step [2900/11916], Loss: 0.0566, Acc : 97.24
Step [3000/11916], Loss: 0.0564, Acc : 97.25
Step [3100/11916], Loss: 0.0564, Acc : 97.25
Step [3200/11916], Loss: 0.0564, Acc : 97.25
Step [3300/11916], Loss: 0.0565, Acc : 97.25
Step [3400/11916], Loss: 0.0564, Acc : 97.25
Step [3500/11916], Loss: 0.0564, Acc : 97.25
Step [3600/11916], Loss: 0.0562, Acc : 97.26
Step [3700/11916], Loss: 0.0562, Acc : 97.25
Step [3800/11916], Loss: 0.0562, Acc : 97.26
Step [3900/11916], Loss: 0.0562, Acc : 97.25
Step [4000/11916], Loss: 0.0563, Acc : 97.25
Step [4100/11916], Loss: 0.0563, Acc : 97.26
Step [4200/11916], Loss: 0.0561, Acc : 97.26
Step [4300/11916], Loss: 0.0562, Acc : 97.26
Step [4400/11916], Loss: 0.0561, Acc : 97.26
Step [4500/11916], Loss: 0.0561, Acc : 97.26
Step [4600/11916], Loss: 0.0560, Acc : 97.27
Step [4700/11916], Loss: 0.0560, Acc : 97.27
Step [4800/11916], Loss: 0.0558, Acc : 97.27
Step [4900/11916], Loss: 0.0559, Acc : 97.27
Step [5000/11916], Loss: 0.0559, Acc : 97.27
Step [5100/11916], Loss: 0.0558, Acc : 97.27
Step [5200/11916], Loss: 0.0558, Acc : 97.27
Step [5300/11916], Loss: 0.0558, Acc : 97.28
Step [5400/11916], Loss: 0.0558, Acc : 97.28
Step [5500/11916], Loss: 0.0557, Acc : 97.28
Step [5600/11916], Loss: 0.0557, Acc : 97.28
Step [5700/11916], Loss: 0.0557, Acc : 97.28
Step [5800/11916], Loss: 0.0556, Acc : 97.28
Step [5900/11916], Loss: 0.0557, Acc : 97.28
Step [6000/11916], Loss: 0.0556, Acc : 97.28
Step [6100/11916], Loss: 0.0556, Acc : 97.28
Step [6200/11916], Loss: 0.0556, Acc : 97.28
Step [6300/11916], Loss: 0.0556, Acc : 97.28
Step [6400/11916], Loss: 0.0556, Acc : 97.28
Step [6500/11916], Loss: 0.0556, Acc : 97.27
Step [6600/11916], Loss: 0.0557, Acc : 97.27
Step [6700/11916], Loss: 0.0557, Acc : 97.27
Step [6800/11916], Loss: 0.0556, Acc : 97.27
Step [6900/11916], Loss: 0.0557, Acc : 97.28
Step [7000/11916], Loss: 0.0556, Acc : 97.28
Step [7100/11916], Loss: 0.0556, Acc : 97.28
Step [7200/11916], Loss: 0.0555, Acc : 97.28
Step [7300/11916], Loss: 0.0555, Acc : 97.28
Step [7400/11916], Loss: 0.0555, Acc : 97.28
Step [7500/11916], Loss: 0.0555, Acc : 97.28
Step [7600/11916], Loss: 0.0555, Acc : 97.28
Step [7700/11916], Loss: 0.0555, Acc : 97.28
Step [7800/11916], Loss: 0.0555, Acc : 97.28
Step [7900/11916], Loss: 0.0554, Acc : 97.28
Step [8000/11916], Loss: 0.0554, Acc : 97.28
Step [8100/11916], Loss: 0.0554, Acc : 97.28
Step [8200/11916], Loss: 0.0554, Acc : 97.28
Step [8300/11916], Loss: 0.0554, Acc : 97.28
Step [8400/11916], Loss: 0.0553, Acc : 97.29
Step [8500/11916], Loss: 0.0553, Acc : 97.29
Step [8600/11916], Loss: 0.0552, Acc : 97.29
Step [8700/11916], Loss: 0.0552, Acc : 97.29
Step [8800/11916], Loss: 0.0552, Acc : 97.29
Step [8900/11916], Loss: 0.0552, Acc : 97.28
Step [9000/11916], Loss: 0.0552, Acc : 97.28
Step [9100/11916], Loss: 0.0551, Acc : 97.29
Step [9200/11916], Loss: 0.0550, Acc : 97.29
Step [9300/11916], Loss: 0.0550, Acc : 97.29
Step [9400/11916], Loss: 0.0550, Acc : 97.29
Step [9500/11916], Loss: 0.0550, Acc : 97.29
Step [9600/11916], Loss: 0.0550, Acc : 97.29
Step [9700/11916], Loss: 0.0549, Acc : 97.29
Step [9800/11916], Loss: 0.0549, Acc : 97.29
Step [9900/11916], Loss: 0.0548, Acc : 97.29
Step [10000/11916], Loss: 0.0548, Acc : 97.30
Step [10100/11916], Loss: 0.0548, Acc : 97.30
Step [10200/11916], Loss: 0.0547, Acc : 97.30
Step [10300/11916], Loss: 0.0547, Acc : 97.30
Step [10400/11916], Loss: 0.0548, Acc : 97.30
Step [10500/11916], Loss: 0.0547, Acc : 97.30
Step [10600/11916], Loss: 0.0547, Acc : 97.30
Step [10700/11916], Loss: 0.0547, Acc : 97.30
Step [10800/11916], Loss: 0.0546, Acc : 97.31
Step [10900/11916], Loss: 0.0546, Acc : 97.31
Step [11000/11916], Loss: 0.0546, Acc : 97.31
Step [11100/11916], Loss: 0.0545, Acc : 97.31
Step [11200/11916], Loss: 0.0545, Acc : 97.31
Step [11300/11916], Loss: 0.0545, Acc : 97.31
Step [11400/11916], Loss: 0.0545, Acc : 97.31
Step [11500/11916], Loss: 0.0545, Acc : 97.31
Step [11600/11916], Loss: 0.0545, Acc : 97.31
Step [11700/11916], Loss: 0.0545, Acc : 97.31
Step [11800/11916], Loss: 0.0544, Acc : 97.32
Step [11900/11916], Loss: 0.0544, Acc : 97.32
Validation . . .
Loss 0.2785,  Acc 89.73,  IoU 0.5482
Testing best epoch . . .
Loss 0.4678,  Acc 89.91,  IoU 0.5504
EPOCH 5/10
Step [100/11916], Loss: 0.0576, Acc : 97.30
Step [200/11916], Loss: 0.0592, Acc : 97.20
Step [300/11916], Loss: 0.0595, Acc : 97.20
Step [400/11916], Loss: 0.0608, Acc : 97.13
Step [500/11916], Loss: 0.0608, Acc : 97.12
Step [600/11916], Loss: 0.0602, Acc : 97.11
Step [700/11916], Loss: 0.0602, Acc : 97.12
Step [800/11916], Loss: 0.0600, Acc : 97.13
Step [900/11916], Loss: 0.0597, Acc : 97.14
Step [1000/11916], Loss: 0.0594, Acc : 97.17
Step [1100/11916], Loss: 0.0594, Acc : 97.16
Step [1200/11916], Loss: 0.0594, Acc : 97.15
Step [1300/11916], Loss: 0.0593, Acc : 97.15
Step [1400/11916], Loss: 0.0588, Acc : 97.17
Step [1500/11916], Loss: 0.0585, Acc : 97.18
Step [1600/11916], Loss: 0.0587, Acc : 97.17
Step [1700/11916], Loss: 0.0586, Acc : 97.18
Step [1800/11916], Loss: 0.0585, Acc : 97.18
Step [1900/11916], Loss: 0.0584, Acc : 97.18
Step [2000/11916], Loss: 0.0583, Acc : 97.19
Step [2100/11916], Loss: 0.0581, Acc : 97.20
Step [2200/11916], Loss: 0.0581, Acc : 97.19
Step [2300/11916], Loss: 0.0580, Acc : 97.19
Step [2400/11916], Loss: 0.0578, Acc : 97.20
Traceback (most recent call last):
  File "train.py", line 270, in <module>
    main(config)
  File "train.py", line 190, in main
    train_metrics = train_epoch(model, optimizer, criterion, train_loader, device=device, config=config)
  File "train.py", line 41, in train_epoch
    optimizer.step()
  File "/share/etud/e2008987/.conda/envs/python_env/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/share/etud/e2008987/.conda/envs/python_env/lib/python3.7/site-packages/torch/optim/adam.py", line 119, in step
    group['eps']
  File "/share/etud/e2008987/.conda/envs/python_env/lib/python3.7/site-packages/torch/optim/functional.py", line 86, in adam
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
