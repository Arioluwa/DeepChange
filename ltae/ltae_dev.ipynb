{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82866ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "# from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from utils import load_npz\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4bae7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = 33\n",
    "n_channel = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0354f6-6fab-4408-9bf8-049de825ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def read_ids():\n",
    "    # set a random seed value within the range 1 -10 \n",
    "    start_time = time.time()\n",
    "    seed_value = np.random.randint(0,10)\n",
    "    np.random.seed(seed_value)\n",
    "    # # block id range 1 - 100 (splitted grid)\n",
    "    block_range = np.arange(1, 101)\n",
    "\n",
    "    # Train, Validation and Test\n",
    "    random.shuffle(block_range)\n",
    "    train_id = block_range[:80] # 80%\n",
    "    val_id = block_range[80:90] # 10%\n",
    "    test_id = block_range[90:] # 10%\n",
    "    print(\"Seed value: \", seed_value)\n",
    "    \n",
    "    if not os.path.exists(\"train_val_eval_seed_\" + str(seed_value)+\".txt\"):\n",
    "        with open(\"tst_train_val_eval_seed_\" + str(seed_value)+\".txt\", \"w\") as f:\n",
    "            f.write(\"Training: \" + str(list(train_id)) + \"\\n\")\n",
    "            f.write(\"Validation: \" + str(list(val_id)) + \"\\n\")\n",
    "            f.write(\"Evaluation: \" + str(list(test_id)) + \"\\n\")\n",
    "            f.close()\n",
    "    print('read id time: ', time.time() -start_time)\n",
    "    return train_id, val_id, test_id # need to be added inside the actual script not here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276ef82d-965b-4daa-bb1d-29d4d017d86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed value:  4\n",
      "read id time:  0.003038644790649414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 17,  46,  98,  76,  40,  12,  94,  95,  72,  25,  38,  47,   6,\n",
       "         86,  45,  16,  75,  41, 100,  65,  37,  58,  70,  54,  23,  49,\n",
       "         15,  99,  61,  28,  83,  19,  66,   9,  10,  30,  79,  36,  53,\n",
       "         64,  93,  80,  88,  77,  34,  33,  85,  11,  73,  92,  44,  20,\n",
       "         31,  60,   1,  42,  13,  21,  48,  56,   3,  63,   4,  59,  97,\n",
       "         84,  14,  26,  90,  43,  51,  50,  67,  74,  68,  24,  69,  39,\n",
       "         22,  18]),\n",
       " array([82, 32, 29, 52, 78, 96, 71,  5, 57, 27]),\n",
       " array([ 7, 91, 89, 81, 55, 62, 35,  2,  8, 87]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4919070f-262d-489f-9236-8ec55593fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardize(X, year=\"2018\"):\n",
    "#     \"\"\"\n",
    "#     X: (b,D,L)\n",
    "#     \"\"\"\n",
    "#     #print(X)\n",
    "#     print(X.shape)\n",
    "#     m = X.mean(axis=0) # axis=(0,2)\n",
    "#     #print(\"mean: \", m)\n",
    "#     s = X.std(axis=0)\n",
    "#     X = (X - m) / s\n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5802178",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sits = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "# train_val_eval = \"../RF_model/train_val_eval_rf.txt\"\n",
    "# start_time = time.time()\n",
    "# train_ids, val_ids, test_ids = read_ids()\n",
    "# X_source, y_source, block_ids_source = load_npz(source_sits)\n",
    "# _source = np.concatenate((X_source, y_source[:, None], block_ids_source[:, None]), axis=1)\n",
    "# _source = _source[np.isin(_source[:, -1], train_ids)]\n",
    "# Xtrain_source = _source[:, :-2]\n",
    "# ytrain_source = _source[:, -2]\n",
    "# Xtrain = np.concatenate((Xtrain_source, Xtrain_target), axis=0)\n",
    "# ytrain = np.concatenate((ytrain_source, ytrain_target), axis=0)\n",
    "# Xtrain = Xtrain_source\n",
    "# ytrain = ytrain_source\n",
    "# x, y = Xtrain, ytrain\n",
    "# x = x.reshape(x.shape[0], L, n_channel)\n",
    "# x = x.transpose(0,2,1)\n",
    "# startime =time.time()\n",
    "# x_ = (standardize(x)).astype('float16')\n",
    "# print(time.time() - startime)\n",
    "# x_ = x[idx]\n",
    "# y = y.astype('int8')[idx]\n",
    "# print(\"time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb91ee-6a36-4f9b-8fd8-b88c8476c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# def data_stardardization(data, case: int):\n",
    "#     n_channel = 10\n",
    "#     # L = 33 # but it's possible to read from the gap_filled dates\n",
    "#     with np.load(data) as dt:\n",
    "#         X = dt['X']\n",
    "#     X = X.reshape(X.shape[0], int(X.shape[1]/n_channel),n_channel)\n",
    "#     # mean\n",
    "#     X_mean = np.mean(X, axis=(0,1))\n",
    "#     # std\n",
    "#     X_std = np.std(X, axis=(0,1))\n",
    "#     print('mean shape: ', X_mean.shape)\n",
    "#     print('std shape: ', X_std.shape)\n",
    "#     # standardization\n",
    "#     # save X_mean and X_std sepearately for sits as txt file\n",
    "#     np.savetxt(os.path.join('mean_'+ str(case) +'.txt'), X_mean)\n",
    "#     np.savetxt(os.path.join('std_'+ str(case) +'.txt'), X_std)\n",
    "# ff = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "# data_stardardization(ff, 2)\n",
    "# print(\"run time: \", time.time() - start_time)\n",
    "# # X_std_3target.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89509a3-79ff-4932-98d5-24f0b60a16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 2\n",
    "mean = np.loadtxt('mean_'+str(case)+'.txt')\n",
    "std = np.loadtxt('std_'+str(case)+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c776b50b-bc83-4e1d-967d-a30d770cc74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mean_std(data, case: int):\n",
    "#     n_channel = 10\n",
    "#     # L = 33 # but it's possible to read from the gap_filled dates\n",
    "#     with np.load(data) as dt:\n",
    "#         X = dt['X']\n",
    "#     X = X.reshape(X.shape[0], int(X.shape[1]/n_channel),n_channel)\n",
    "#     # mean\n",
    "#     X_mean = np.mean(X, axis=(0,1))\n",
    "#     # std\n",
    "#     X_std = np.std(X, axis=(0,1))\n",
    "#     print('mean shape: ', X_mean.shape)\n",
    "#     print('std shape: ', X_std.shape)\n",
    "#     # standardization\n",
    "#     # save X_mean and X_std sepearately for sits as txt file\n",
    "#     np.savetxt(os.path.join('X_mean_'+ case +'.txt'), X_mean)\n",
    "#     np.savetxt(os.path.join('X_std_'+ case +'.txt'), X_std)\n",
    "# source_sits = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "# mean_std(source_sits, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2855aae-a3dc-4f32-9632-0998ac02417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIts(data.Dataset):\n",
    "    def __init__(self, case_: int,source_path, target_path, partition='train', transform=None):\n",
    "        self.case_ = case_\n",
    "        self.source_path = source_path\n",
    "        self.target_path = target_path\n",
    "        self.transform = transform\n",
    "        \n",
    "        # get partition ids\n",
    "        start_time = time.time()\n",
    "        self.train_ids, self.val_ids, self.test_ids = read_ids()\n",
    "        print(\"read ids completed: \", time.time() - start_time)\n",
    "\n",
    "        # select partition\n",
    "        if partition == 'train':\n",
    "            self.ids = self.train_ids\n",
    "        elif partition == 'val':\n",
    "            self.ids = self.val_ids\n",
    "        elif partition == 'test':\n",
    "            self.ids = self.test_ids\n",
    "        else:\n",
    "            raise ValueError('Invalid partition: {}'.format(partition))\n",
    "\n",
    "        # sits = either source_path or target or both based on the case (1,2,3)\n",
    "        # case = 1: both, case = 2: target, case = 3: target\n",
    "        if self.case_ == 1:\n",
    "            sits = [self.source_path, self.target_path]\n",
    "        elif self.case_ == 2:\n",
    "            sits = self.source_path\n",
    "        elif self.case_ == 3:\n",
    "            sits = self.target_path\n",
    "        else:\n",
    "            print('Wrong case!')\n",
    "            # return None\n",
    "        if isinstance(sits, str):\n",
    "            self.sits = sits\n",
    "            print(self.sits)\n",
    "            start_time = time.time()\n",
    "            X, y, block_ids = load_npz(self.sits)\n",
    "            print(\"load npz: \", time.time() - start_time)\n",
    "            print(X.shape)\n",
    "            print(y.shape)\n",
    "            print(block_ids.shape)\n",
    "            \n",
    "            # concatenate the data\n",
    "            start_time = time.time()\n",
    "            data_ = np.concatenate((X, y[:, None], block_ids[:, None]), axis=1)\n",
    "            print(\"Concatenating completed \", time.time() - start_time)\n",
    "\n",
    "            # filter by block_id\n",
    "            start_time = time.time()\n",
    "            data_ = data_[np.isin(data_[:, -1], self.ids)]\n",
    "            print(\"filtering ids completed \", time.time() - start_time)\n",
    "            \n",
    "            self.X_ = data_[:, :-2]\n",
    "            self.y_ = data_[:, -2]\n",
    "        elif isinstance(sits, list):\n",
    "            self.sits = sits\n",
    "            print(self.sits)\n",
    "            X_source, y_source, block_ids_source = load_npz(self.sits[0])\n",
    "            X_target, y_target, block_ids_target = load_npz(self.sits[1])\n",
    "            # concatenate the data\n",
    "            data_source = np.concatenate((X_source, y_source[:, None], block_ids_source[:, None]), axis=1)\n",
    "            data_target = np.concatenate((X_target, y_target[:, None], block_ids_target[:, None]), axis=1)\n",
    "\n",
    "            # filter by block_id\n",
    "            data_source = data_source[np.isin(data_source[:, -1], self.ids)]\n",
    "            data_target = data_target[np.isin(data_target[:, -1], self.ids)]\n",
    "\n",
    "            self.X_ = np.concatenate((data_source[:, :-2], data_target[:, :-2]), axis=0)\n",
    "            self.y_ = np.concatenate((data_source[:, -2], data_target[:, -2]), axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # standardization\n",
    "        self.X = self.X_[idx]\n",
    "        self.y = self.y_[idx]\n",
    "        self.X = self.X.reshape(int(self.X.shape[0]/n_channel), n_channel)\n",
    "            # transform\n",
    "        if self.transform:\n",
    "            self.X = self.transform(self.X)\n",
    "        self.X = (self.X).astype('float32')\n",
    "        self.y = (self.y).astype('float32')\n",
    "        \n",
    "        return torch.from_numpy(np.array(self.X)), torch.from_numpy(np.array(self.y)).float()\n",
    "    # return torch.from_numpy(np.array(x_)), torch.from_numpy(np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8190c2ef-d5ca-4ace-8b0b-34c6d4dc3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class band_standardise(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return (sample - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffff6478-4173-4f12-9510-3c5a0780844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([band_standardise(mean, std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85439710-1994-4049-ad54-04d0f385fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed value:  7\n",
      "read id time:  0.0043566226959228516\n",
      "read ids completed:  0.004605770111083984\n",
      "../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\n",
      "load npz:  57.42299962043762\n",
      "(15265763, 330)\n",
      "(15265763,)\n",
      "(15265763,)\n",
      "Concatenating completed  4.365331649780273\n",
      "filtering ids completed  9.326314926147461\n",
      "total running time:  71.14363551139832\n"
     ]
    }
   ],
   "source": [
    "case_ = 2\n",
    "source_path = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "target_path = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "start_time = time.time()\n",
    "dataset = SIts(case_, source_path, target_path, partition='train', transform=transform)\n",
    "print('total running time: ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4906a91-60ba-46c3-83e0-ffc84ef3dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5194dd4b-2894-4e8d-a7f1-01944eee3c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader time:  1.5742790699005127\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "x_train,y_train= next(iter(train_loader))\n",
    "print('DataLoader time: ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38aa7f74-7bee-4096-8878-e853bb9b31ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23.,  2.,  6.,  9.,  3.,  8.,  6.,  9.,  9.,  9.,  9.,  8.,  9.,  9.,\n",
       "         8.,  8.,  6., 13.,  9.,  9., 10.,  7.,  9.,  5.,  7.,  9.,  9.,  9.,\n",
       "         9.,  8.,  6.,  6.,  9.,  5.,  9.,  2., 10.,  4.,  9.,  2.,  9.,  9.,\n",
       "         2.,  9.,  5.,  9., 23.,  9.,  9.,  9.,  7.,  2.,  9.,  3.,  9., 10.,\n",
       "         8.,  9.,  9.,  6.,  9.,  2.,  9.,  9.,  9., 10.,  7.,  9.,  9.,  5.,\n",
       "         9.,  9.,  9.,  6.,  6.,  2.,  3.,  9.,  9.,  9.,  5., 12.,  6.,  9.,\n",
       "         9.,  8.,  8.,  2.,  2.,  9.,  7.,  9.,  9.,  8.,  9.,  9.,  9.,  6.,\n",
       "         2.,  2.,  8.,  6.,  9.,  8., 12., 10.,  9.,  3.,  9.,  2.,  2.,  9.,\n",
       "         6.,  9.,  9.,  9.,  9.,  7.,  9.,  5.,  9.,  9.,  9., 23.,  9.,  8.,\n",
       "         8., 10.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "958c3144-71ed-4173-b35e-0a3645f7d731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 33, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf94786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SITSData(data.Dataset):\n",
    "    def __init__(self, case: int, source_sits, target_sits, train_val_eval, set_= 'trainval', transform=None):\n",
    "        ## OPTION 1 (good)\n",
    "        # sits \n",
    "        ## OPTION 2 (not good)\n",
    "        # [source_sits] case 2\n",
    "        # [source_list, target_list] case 1\n",
    "        self.source_sits = source_sits\n",
    "        self.target_sits = target_sits\n",
    "        self.train_val_eval = train_val_eval\n",
    "        self.transform = transform\n",
    "        self.case = case\n",
    "        self.set_ = set_\n",
    "\n",
    "        # read the set ids\n",
    "        start_time = time.time()\n",
    "        self.train_ids, self.test_ids = read_ids(self.train_val_eval)\n",
    "        print(\"read ids %s seconds ---\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "        # case selection\n",
    "        if self.set_ == 'trainval':\n",
    "            ids = self.train_ids\n",
    "        elif self.set_ == 'test':\n",
    "            ids = self.test_ids\n",
    "        else:\n",
    "            raise ValueError(\"Please choose a set between trainval and test\")\n",
    "        print(\"case %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        # read the data \n",
    "        start_time = time.time()\n",
    "        X_source, y_source, block_ids_source = load_npz(self.source_sits)\n",
    "        X_target, y_target, block_ids_target = load_npz(self.target_sits)\n",
    "        print(\"load npz %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "        start_time = time.time()\n",
    "        _source = np.concatenate((X_source, y_source[:, None], block_ids_source[:, None]), axi =1)\n",
    "        _target = np.concatenate((X_target, y_target[:, None], block_ids_target[:, None]), axis=1)\n",
    "        print(\"concatenate %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "        start_time = time.time()\n",
    "        _source = _source[np.isin(_source[:, -1], ids)]\n",
    "        _target = _target[np.isin(_target[:, -1], ids)]\n",
    "        print(\"isin %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.Xtrain_source = _source[:, :-2]\n",
    "        self.ytrain_source = _source[:, -2]\n",
    "        print(\"remove ids source %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.Xtrain_target = _target[:, :-2]\n",
    "        self.ytrain_target = _target[:, -2]\n",
    "        print(\"remove ids target %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        start_time = time.time()\n",
    "        if self.case == 1:\n",
    "            self.Xtrain = np.concatenate((self.Xtrain_source, self.Xtrain_target), axis=0)\n",
    "            self.ytrain = np.concatenate((self.ytrain_source, self.ytrain_target), axis=0)\n",
    "        elif self.case == 2:\n",
    "            self.Xtrain = self.Xtrain_source\n",
    "            self.ytrain = self.ytrain_source\n",
    "        elif self.case == 3:\n",
    "            self.Xtrain = self.Xtrain_target\n",
    "            self.ytrain = self.ytrain_target\n",
    "        else:\n",
    "            raise ValueError(\"Please choose a case between 1 and 3\") \n",
    "        print(\"concatenate %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "        \n",
    "        # Reqd meqn qnd std\n",
    "        if year = \"2018\":\n",
    "            read(...)\n",
    "        else:\n",
    "            reas(...)\n",
    "        mean = np.mean(...)\n",
    "        std = np.mean(...)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ytrain)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = time.time()\n",
    "        x = self.Xtrain[idx]\n",
    "        x, y = self.Xtrain, self.ytrain\n",
    "        x = x.reshape(x.shape[0], L, n_channel)\n",
    "        x = x.transpose(0,2,1)\n",
    "        print(\"reshape and transpose %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        x = (standardize(x)).astype('float32')\n",
    "        x_ = x[idx]\n",
    "        y = y.astype('int8')[idx]\n",
    "        print(\"stardardization %s seconds ---\" % (time.time() - start_time))\n",
    "        # start_time = time.time()\n",
    "        # x_ = torch.from_numpy(np.array(x_))\n",
    "        # y = torch.from_numpy(np.array(y))\n",
    "        # print(\"to tenors %s seconds ---\" % (time.time() - start_time))\n",
    "        # return x, y\n",
    "        return torch.from_numpy(np.array(x_)), torch.from_numpy(np.array(y))\n",
    "\n",
    "def standardize(X, year=\"2018\"):\n",
    "    \"\"\"\n",
    "    X: (b,D,L)\n",
    "    \"\"\"\n",
    "    #print(X)\n",
    "    print(X.shape)\n",
    "    m = X.mean(axis=0) # axis=(0,2)\n",
    "    #print(\"mean: \", m)\n",
    "    s = X.std(axis=0)\n",
    "    X = (X - m) / s\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c5c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "# load_npz(source_sits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb980b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read ids 0.0020072460174560547 seconds ---\n",
      "case 2.6226043701171875e-06 seconds ---\n",
      "load npz 2.311331498622894 minutes ---\n",
      "concatenate 0.12044020891189575 minutes ---\n",
      "isin 0.21219375928243 minutes ---\n",
      "remove ids source 3.457069396972656e-05 seconds ---\n",
      "remove ids target 2.0265579223632812e-05 seconds ---\n",
      "concatenate 0.06584848562876383 minutes ---\n",
      "run time:  2.7105690161387126\n"
     ]
    }
   ],
   "source": [
    "#source_sits = \"../subset_data/2018_subset.npz\"\n",
    "#target_sits = \"../subset_data/2019_subset.npz\"\n",
    "# train_val_eval = \"train_val_eval_rf.txt\"\n",
    "source_sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "target_sits = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "train_val_eval = \"../RF_model/train_val_eval_rf.txt\"\n",
    "case = 1\n",
    "set_ = \"trainval\"\n",
    "transform = None\n",
    "import time\n",
    "start_time = time.time()\n",
    "train_dataset = SITSData(case, source_sits, target_sits, train_val_eval, set_, transform)\n",
    "test_dataset = \n",
    "train_loader = data.DataLoader(train_dataset, batch_size = 1, pin_memory=True)\n",
    "\n",
    "print(\"run time: \", ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e66e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,y_train= next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e66a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape and transpose 4.029273986816406e-05 seconds ---\n",
      "stardardization 82.94411087036133 seconds ---\n",
      "reshape and transpose 3.147125244140625e-05 seconds ---\n",
      "stardardization 82.9876160621643 seconds ---\n",
      "reshape and transpose 4.673004150390625e-05 seconds ---\n"
     ]
    }
   ],
   "source": [
    "for (x,y) in enumerate(train_loader):\n",
    "    y_ry = y\n",
    "    x_hr = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a532f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read ids 0.002400636672973633 seconds ---\n",
      "case 1.9073486328125e-06 seconds ---\n",
      "load npz 1.9369140625 minutes ---\n",
      "concatenate 0.12013090848922729 minutes ---\n",
      "isin 0.14379287560780843 minutes ---\n",
      "remove ids source 4.100799560546875e-05 seconds ---\n",
      "remove ids target 5.245208740234375e-06 seconds ---\n",
      "concatenate 0.0441957155863444 minutes ---\n",
      "run time:  134.75104451179504\n"
     ]
    }
   ],
   "source": [
    "source_sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "target_sits = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "train_val_eval = \"../RF_model/train_val_eval_rf.txt\"\n",
    "case = 1\n",
    "set_ = \"test\"\n",
    "transform = None\n",
    "import time\n",
    "st = time.time()\n",
    "dataset_test = SITSData(case, source_sits, target_sits, train_val_eval, set_, transform)\n",
    "test_loader = data.DataLoader(dataset_test, batch_size = 1, pin_memory=True)\n",
    "# x_test,y_test= next(iter(loader))\n",
    "print(\"run time: \", time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08a7229b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape and transpose 2.7179718017578125e-05 seconds ---\n",
      "stardardization 54.03708863258362 seconds ---\n",
      "to tenors 0.030365467071533203 seconds ---\n",
      "run time:  0.9012147784233093\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "x_test,y_test= next(iter(test_loader))\n",
    "print(\"run time: \", ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a341fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495f8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # useful functions that are needed in the dataset class\n",
    "# class SitsData(data.Dataset):\n",
    "#     def __init__(self, case: int, source_sits, target_sits, train_val_eval, set_= 'trainval', transform=None):\n",
    "#         super(SitsData, self).__init__()\n",
    "#         self.source_sits = source_sits\n",
    "#         self.target_sits = target_sits\n",
    "#         self.train_val_eval = train_val_eval\n",
    "#         self.transform = transform\n",
    "#         self.case = case\n",
    "#         self.set_ = set_\n",
    "        \n",
    "#         self.x, self.y = self._init_dataset()\n",
    "        \n",
    "#     def load_set(self, total_set):\n",
    "#         \"\"\"\n",
    "#             Load a set of data:\n",
    "#             to be used in prepare_data(), it organise dataset according to the set_name based block_ids\n",
    "#             - set_name: train or test (reads from the block_ids using read_ids() utils) \n",
    "#             - total_set: the concatenated set of data for source or target\n",
    "#             - returns X and Y:\n",
    "#                 X is the data, Y is the label\n",
    "#         \"\"\"\n",
    "#         self.train_ids, self.test_ids = read_ids(self.train_val_eval)\n",
    "\n",
    "#         if self.set_ == \"trainval\":\n",
    "#             ids = self.train_ids\n",
    "#         elif self.set_ == \"test\":\n",
    "#             ids = self.test_ids\n",
    "#         else:\n",
    "#             raise ValueError(\"Please choose a set between trainval and test\")\n",
    "\n",
    "#         set = total_set[np.isin(total_set[:, -1], ids)]\n",
    "#         X = set[:, :-2]\n",
    "#         Y = set[:, -2]\n",
    "            \n",
    "#         return X, Y\n",
    "        \n",
    "#     def _init_dataset(self):\n",
    "#         \"\"\"\n",
    "#             Prepare data:\n",
    "#             - load data from npz files(load_npz() in utils): returns X, Y and block_ids\n",
    "#             - concatenate the data from source and target [X, Y, block_ids] == total_set\n",
    "#             - split the data into train and test sets for source and target\n",
    "#             - returns Xtrain and Ytrain base on CASE value\n",
    "#             CASE: \n",
    "#             - 1: Train on Soucre and target, test on source and target\n",
    "#             - 2: Train on Source only, test on Source and target\n",
    "#             - 3: Train on Target only, test on Source and target\n",
    "#         \"\"\"\n",
    "#             # print(\"Preparing data.........\")\n",
    "\n",
    "#         X_s, Y_s, block_ids_s = load_npz(self.source_sits)\n",
    "#         X_t, Y_t, block_ids_t = load_npz(self.target_sits)\n",
    "#         # print(\"Loading npz files done.........\")\n",
    "            \n",
    "\n",
    "#         self.total_set_s = np.concatenate((X_s, Y_s[:, None], block_ids_s[:, None]), axis=1)\n",
    "#         self.total_set_t = np.concatenate((X_t, Y_t[:, None], block_ids_t[:, None]), axis=1)\n",
    "#             # print(\"Concatenating data done.........\")\n",
    "\n",
    "#             # Training set for target and source\n",
    "#         self.Xtrain_s, self.Ytrain_s = self.load_set(self.total_set_s)\n",
    "#         self.Xtrain_t, self.Ytrain_t = self.load_set(self.total_set_t)\n",
    "#             # print(\"Loading train set done.........\")\n",
    "\n",
    "#             # Test set for target and source\n",
    "#         self.Xtest_s, self.Ytest_s = self.load_set(self.total_set_s)\n",
    "#         self.Xtest_t, self.Ytest_t = self.load_set(self.total_set_t)\n",
    "#             # print(\"Loading test set done.........\")\n",
    "\n",
    "#         if self.case == 1:\n",
    "#                 # concatenate training set for target and source\n",
    "#             self.Xtrain = np.concatenate((self.Xtrain_s, self.Xtrain_t), axis=0)\n",
    "#             self.Ytrain = np.concatenate((self.Ytrain_s, self.Ytrain_t), axis=0)\n",
    "\n",
    "#         elif self.case == 2:\n",
    "#                 # Xtrain is the training set for source only\n",
    "#             self.Xtrain = self.Xtrain_s\n",
    "#             self.Ytrain = self.Ytrain_s\n",
    "\n",
    "#         elif self.case == 3:\n",
    "#                 # Xtrain is the training set for target only\n",
    "#             self.Xtrain = self.Xtrain_t\n",
    "#             self.Ytrain = self.Ytrain_t\n",
    "\n",
    "#         else:\n",
    "#             raise ValueError(\"Please choose a case between 1 and 3\")\n",
    "            \n",
    "#         return self.Xtrain, self.Ytrain\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         x_, y_ = self.x[idx], self.y[idx]\n",
    "            \n",
    "#         x = torch.from_numpy(np.array(x_, dtype=int))\n",
    "#         y = torch.from_numpy(np.array(y_, dtype=int))\n",
    "#         return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8225c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file= '../RF_model/models/rf_model_2.pkl'\n",
    "ref_file = '../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz'\n",
    "in_img = '../../../data/theiaL2A_zip_img/output/2018/2018_GapFilled_Image.tif'\n",
    "out_path = '../../../results/RF'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
