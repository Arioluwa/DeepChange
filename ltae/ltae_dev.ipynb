{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82866ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from models.stclassifier import dLtae\n",
    "from models.ltae import LTAE\n",
    "from learning.focal_loss import FocalLoss\n",
    "import torchnet as tnt\n",
    "from learning.metrics import mIou, confusion_matrix_analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utils import *\n",
    "# from learn_curve import plot_curve\n",
    "from altdataset import SITSData\n",
    "from altdataset import date_positions\n",
    "from models.stclassifier import dLtae\n",
    "from models.ltae import LTAE\n",
    "from learning.focal_loss import FocalLoss\n",
    "from learning.weight_init import weight_init\n",
    "from learning.metrics import mIou, confusion_matrix_analysis\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import argparse\n",
    "import pprint\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20aae8b-9f8a-47b2-ab6f-d9544582f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_positions(\"../../../data/theiaL2A_zip_img/output/2019/gapfilled_dates2019.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece625dd-766b-4078-b3ed-8daff62ba638",
   "metadata": {},
   "source": [
    "## Scenario 3 Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6ab244-5f2f-4d0f-a6e5-30f5b5d1124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def date_positions(gfdate_path):\n",
    "#     with open(gfdate_path, \"r\") as f:\n",
    "#         date_list = f.readlines()\n",
    "#     date_list = [x.strip() for x in date_list]\n",
    "#     date_list = [datetime.datetime.strptime(x, \"%Y%m%d\").timetuple().tm_yday for x in date_list]\n",
    "#     date_ = [x for x in date_list]\n",
    "#     return date_\n",
    "# f = date_positions(\"../../../data/theiaL2A_zip_img/output/2019/gapfilled_dates2019.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8579202-fb92-42dd-92b2-0691291e34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3072465d-ed6e-45a0-b834-15fb095762c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = '../../../results/ltae/model/2018_2019/Seed_0/model.pth.tar'\n",
    "state_dict = torch.load(m)['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d62544-1041-467c-ba51-6d930482ca31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f599daa-3d9a-4522-beab-07b74d4f661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parameters():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     # Set-up parameters\n",
    "#     # parser.add_argument('--dataset_folder', default='../../../data/theiaL2A_zip_img/output/2019', type=str, help='Path to the data folder.') #move npy and date into a folder\n",
    "#     # parser.add_argument('--res_dir', default='../../../results/ltae/trials', help='Path to the folder where the results should be stored')\n",
    "#     parser.add_argument('--num_workers', default=10, type=int, help='Number of data loading workers')\n",
    "#     parser.add_argument('--seed', default=0, type=int, help='Random seed')\n",
    "#     parser.add_argument('--device', default='cuda', type=str, help='Name of device to use for tensor computations (cuda/cpu)')\n",
    "#     parser.add_argument('--display_step', default=100, type=int, help='Interval in batches between display of training metrics')\n",
    "#     parser.add_argument('--scheduler_', default=False, type=bool, help='Enable scheduler')\n",
    "#     # parser.add_argument('--preload', dest='preload', action='store_true', help='If specified, the whole dataset is loaded to RAM at initialization')\n",
    "#     parser.set_defaults(preload=False)\n",
    "    \n",
    "\n",
    "#     # Training parameters\n",
    "#     parser.add_argument('--epochs', default=2, type=int, help='Number of epochs per fold')\n",
    "#     parser.add_argument('--batch_size', default=128, type=int, help='Batch size')\n",
    "#     parser.add_argument('--lr', default=0.001, type=float, help='Learning rate')\n",
    "#     parser.add_argument('--gamma', default=1, type=float, help='Gamma parameter of the focal loss')\n",
    "#     # parser.add_argument('--weight_decay', default=0, type=float, help='Weight decay rate')\n",
    "\n",
    "#     ## L-TAE \n",
    "#     parser.add_argument('--in_channels', default=10, type=int, help='Number of channels of the input embeddings')\n",
    "#     parser.add_argument('--n_head', default=16, type=int, help='Number of attention heads')\n",
    "#     parser.add_argument('--d_k', default=8, type=int, help='Dimension of the key and query vectors')\n",
    "#     parser.add_argument('--n_neurons', default=[256,128], type=str, help='Number of neurons in the layers of n_neurons')\n",
    "#     parser.add_argument('--T', default=1000, type=int, help='Maximum period for the positional encoding')\n",
    "#     parser.add_argument('--positions', default='None', type=str,\n",
    "#                         help='Positions to use for the positional encoding (bespoke / order)')\n",
    "#     parser.add_argument('--len_max_seq', default=53, type=int,\n",
    "#                         help='Maximum sequence length for positional encoding (only necessary if positions == order)')\n",
    "#     parser.add_argument('--dropout', default=0.2, type=float, help='Dropout probability')\n",
    "#     parser.add_argument('--d_model', default=256, type=int,\n",
    "#                         help=\"size of the embeddings (E), if input vectors are of a different size, a linear layer is used to project them to a d_model-dimensional space\")\n",
    "\n",
    "#     ## Classifier\n",
    "#     parser.add_argument('--num_classes', default=19, type=int, help='Number of classes')\n",
    "#     parser.add_argument('--mlp4', default='[128, 64, 32, 19]', type=str, help='Number of neurons in the layers of MLP (Decoder)')\n",
    "    \n",
    "#     config = parser.parse_args()\n",
    "#     config = vars(config)\n",
    "#     for k, v in config.items():\n",
    "#         if 'mlp' in k or k == 'nker':\n",
    "#             v = v.replace('[', '')\n",
    "#             v = v.replace(']', '')\n",
    "#             config[k] = list(map(int, v.split(',')))\n",
    "    \n",
    "#     return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f319a973-48ad-4a1e-8d69-b5a82f43c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "686772ff-65f0-451a-a77f-a1fa4d9cd768",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6ab7399-9fec-4e4f-94e3-23fab3d84df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = \"../../../results/ltae/model/2018_2019\"\n",
    "seed = 0\n",
    "def prepare_output():\n",
    "    os.makedirs(res_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(res_dir, 'Seed_{}'.format(seed)), exist_ok=True)\n",
    "\n",
    "def checkpoint(log):\n",
    "    with open(os.path.join(res_dir, 'Seed_{}'.format(seed), 'seed_{}_trainlog.json'.format(seed)), 'w') as outfile:\n",
    "        json.dump(log, outfile, indent=4)\n",
    "\n",
    "def save_results(metrics, conf_mat, report):\n",
    "    with open(os.path.join(res_dir, 'Seed_{}'.format(seed), 'seed_{}_test_metrics.json'.format(seed)), 'w') as outfile:\n",
    "        json.dump(metrics, outfile, indent=4)\n",
    "    pkl.dump(conf_mat, open(os.path.join(res_dir, 'Seed_{}'.format(seed), 'seed_{}_conf_mat.pkl'.format(seed)), 'wb'))\n",
    "\n",
    "    with open(os.path.join(res_dir, 'Seed_{}'.format(seed),'seed_{}_report.txt'.format(seed)), 'w') as f:\n",
    "        f.write(report)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4863fa5-c4e5-41f0-b345-41e0ebfe236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, data_loader, device, config):\n",
    "    acc_meter = tnt.meter.ClassErrorMeter(accuracy=True)\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        start_time = time.time()\n",
    "        y_true.extend(list(map(int, y)))\n",
    "\n",
    "        x = recursive_todevice(x, device)\n",
    "        y = y.to(device)\n",
    "        # print(x.is_cuda)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y.long())\n",
    "        loss.backward()\n",
    "        # if config['scheduler_']:\n",
    "        #     optimizer.step()\n",
    "        #     scheduler.step()\n",
    "        # else:\n",
    "        optimizer.step()\n",
    "            \n",
    "\n",
    "        pred = out.detach()\n",
    "        y_p = pred.argmax(dim=1).cpu().numpy()\n",
    "        y_pred.extend(list(y_p))\n",
    "        acc_meter.add(pred, y)\n",
    "        loss_meter.add(loss.item())\n",
    "\n",
    "        if (i + 1) % config['display_step'] == 0:\n",
    "            print('Step [{}/{}], Loss: {:.4f}, Acc : {:.2f}'.format(i + 1, len(data_loader), loss_meter.value()[0], acc_meter.value()[0]))\n",
    "        \n",
    "        print(\"Iteration {} completed in {:.4f} second\".format(i + 1, time.time() - start_time))\n",
    "        # if i +1 == int(len(data_loader)/config['factor']):  break\n",
    "        # if i +1 >= config['factor']:  break\n",
    "    epoch_metrics = {'train_loss': loss_meter.value()[0],\n",
    "                     'train_accuracy': acc_meter.value()[0],\n",
    "                     'train_IoU': mIou(y_true, y_pred, n_classes=19)}\n",
    "    wandb.log({\"train_loss\": epoch_metrics['train_loss'], \"train_accuracy\": epoch_metrics['train_accuracy'], \"train_IoU\": epoch_metrics['train_IoU']})\n",
    "    \n",
    "    return epoch_metrics\n",
    "\n",
    "def evaluation(model, criterion, loader, device, config, mode='val'):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    acc_meter = tnt.meter.ClassErrorMeter(accuracy=True)\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "    label = [\"Dense built-up area\", \"Diffuse built-up area\", \"Industrial and commercial areas\", \"Roads\", \"Oilseeds (Rapeseed)\", \"Straw cereals (Wheat, Triticale, Barley)\", \"Protein crops (Beans / Peas)\", \"Soy\", \"Sunflower\", \"Corn\",  \"Tubers/roots\", \"Grasslands\", \"Orchards and fruit growing\", \"Vineyards\", \"Hardwood forest\", \"Softwood forest\", \"Natural grasslands and pastures\", \"Woody moorlands\", \"Water\"]\n",
    "    for (x, y) in loader:\n",
    "        start_time = time.time()\n",
    "        y_true.extend(list(map(int, y)))\n",
    "        x = recursive_todevice(x, device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(x)\n",
    "            loss = criterion(prediction, y)\n",
    "\n",
    "        acc_meter.add(prediction, y)\n",
    "        loss_meter.add(loss.item())\n",
    "\n",
    "        y_p = prediction.argmax(dim=1).cpu().numpy()\n",
    "        y_pred.extend(list(y_p))\n",
    "        \n",
    "        print(\"evaluation iteration completed in {:.4f} seconds\".format(time.time() - start_time))\n",
    "    metrics = {'{}_accuracy'.format(mode): acc_meter.value()[0],\n",
    "               '{}_loss'.format(mode): loss_meter.value()[0],\n",
    "               '{}_IoU'.format(mode): mIou(y_true, y_pred, 19)}\n",
    "    \n",
    "    if mode == 'val':\n",
    "        return metrics\n",
    "    elif mode == 'test':\n",
    "        return metrics, confusion_matrix(y_true, y_pred, labels=list(range(19))), classification_report(y_true, y_pred, target_names=label, digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a469aaa8-8e4d-48c5-8451-19e0666e52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(train_dt, val_dt, test_dt):\n",
    "    \n",
    "    loader_seq = []\n",
    "    train_loader = data.DataLoader(train_dt, batch_size=2048,\n",
    "                                       num_workers=10,\n",
    "                                       shuffle=True,\n",
    "                                          pin_memory=True)\n",
    "    validation_loader = data.DataLoader(val_dt, batch_size=2048,\n",
    "                                       num_workers=10,\n",
    "                                       shuffle=True,\n",
    "                                          pin_memory=True)\n",
    "    test_loader = data.DataLoader(test_dt, batch_size=2048,\n",
    "                                       num_workers=10,\n",
    "                                       shuffle=True,\n",
    "                                          pin_memory=True)\n",
    "    loader_seq.append((train_loader, validation_loader, test_loader))\n",
    "    return loader_seq\n",
    "\n",
    "def recursive_todevice(x, device):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device)\n",
    "    else:\n",
    "        return [recursive_todevice(c, device) for c in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ac04c9-c082-4ea7-8bba-c263efd1f15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset completed\n",
      "val dataset completed\n",
      "test dataset completed\n"
     ]
    }
   ],
   "source": [
    "data_folder_2018 = \"../../../data/theiaL2A_zip_img/output/2018\"\n",
    "mean_ = np.loadtxt(glob.glob(data_folder_2018 + '/*mean.txt')[0])\n",
    "std_ = np.loadtxt(glob.glob(data_folder_2018 + '/*std.txt')[0])\n",
    "transform = transforms.Compose([standardize(mean_, std_)])\n",
    "    \n",
    "sits_data = glob.glob(os.path.join(data_folder_2018, 'Seed_{}'.format(seed)  + '/*.npz'))\n",
    "doy = glob.glob(data_folder_2018 + '/gapfilled*.txt')[0]\n",
    "\n",
    "train_dt = SITSData(sits_data[2], doy, transform = transform)\n",
    "print(\"train dataset completed\")\n",
    "val_dt = SITSData(sits_data[1], doy,  transform = transform)\n",
    "print(\"val dataset completed\")\n",
    "test_dt = SITSData(sits_data[0], doy,  transform = transform)\n",
    "print(\"test dataset completed\")\n",
    "del sits_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f66e05-e1e0-4402-ad11-aab587ff13bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset completed\n",
      "val dataset completed\n",
      "test dataset completed\n"
     ]
    }
   ],
   "source": [
    "data_folder_2019 = \"../../../data/theiaL2A_zip_img/output/2019\"\n",
    "mean_ = np.loadtxt(glob.glob(data_folder_2019 + '/*mean.txt')[0])\n",
    "std_ = np.loadtxt(glob.glob(data_folder_2019 + '/*std.txt')[0])\n",
    "transform = transforms.Compose([standardize(mean_, std_)])\n",
    "    \n",
    "sits_data_2 = glob.glob(os.path.join(data_folder_2019, 'Seed_{}'.format(seed)  + '/*.npz'))\n",
    "doy_2 = glob.glob(data_folder_2019 + '/gapfilled*.txt')[0]\n",
    "    \n",
    "train_dt_2 = SITSData(sits_data_2[2], doy_2, transform = transform)\n",
    "print(\"train dataset completed\")\n",
    "val_dt_2 = SITSData(sits_data_2[1], doy_2,  transform = transform)\n",
    "print(\"val dataset completed\")\n",
    "test_dt_2 = SITSData(sits_data_2[0], doy_2,  transform = transform)\n",
    "print(\"test dataset completed\")\n",
    "del sits_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca5629e8-cff7-464b-98b0-d84da8a6c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = []\n",
    "train_datasets.append((train_dt, train_dt_2))\n",
    "val_datasets = []\n",
    "val_datasets.append((val_dt, val_dt_2))\n",
    "test_datasets = []\n",
    "test_datasets.append((test_dt, test_dt_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b4244d3-7d0b-4264-92ad-634d02c86807",
   "metadata": {},
   "outputs": [],
   "source": [
    "trd = data.ConcatDataset(train_datasets)\n",
    "vd = data.ConcatDataset(val_datasets)\n",
    "ted = data.ConcatDataset(test_datasets)\n",
    "loaders_ = get_loader(trd, vd, ted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "158197b7-ab5c-4fa0-a578-f58dffbb7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b240983-3041-4d85-b66e-b797e6b7b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "tloader = torch.utils.data.DataLoader(ConcatDataset(train_dt, train_dt_2), batch_size=2048, num_workers=10,\n",
    "                                       shuffle=True,\n",
    "                                          pin_memory=True)\n",
    "# vloader = torch.utils.data.DataLoader(ConcatDataset(val_dt, val_dt_2), batch_size=2048, num_workers=10,\n",
    "#                                        shuffle=True,\n",
    "#                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2986af05-dfb3-4506-a9c2-b97eca68cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = []\n",
    "train_datasets.append((train_dt, train_dt_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc46f464-8891-4727-9649-82ecf806ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = torch.utils.data.ConcatDataset(train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8952990-034c-4201-9db6-ba6843ea7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "tloader = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset([train_dt, train_dt_2]), batch_size=2048, num_workers=10,\n",
    "                                       shuffle=True,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d6d78a-5cbe-49a8-9359-51fe6c12d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.data.ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cae14da-aa71-4517-86b3-f2dec67cc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "for i, (x,y) in enumerate(tloader):\n",
    "    y_true.extend(list(map(int,y)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7f467dd-cb55-42b3-bb9c-bede3e622680",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data.DataLoader(ted, batch_size=2048,\n",
    "                    num_workers=10,\n",
    "                    shuffle=True,\n",
    "                    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9626a0d-4677-479b-83fe-9a591319aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in tloader:\n",
    "#     print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a68ea1cb-b9ab-4fb8-bba1-e18983e91ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1, Val 1, Test 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madebowale\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.16 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/adebowale/DeepChange-ltae/runs/3aqb8lcn\" target=\"_blank\">skilled-yogurt-46</a></strong> to <a href=\"https://wandb.ai/adebowale/DeepChange-ltae\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../results/ltae/model/2018_2019/Seed_0/conf.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-34adec1eb667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Seed_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'conf.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../results/ltae/model/2018_2019/Seed_0/conf.json'"
     ]
    }
   ],
   "source": [
    "for train_loader, val_loader, test_loader in loaders_:\n",
    "        print('Train {}, Val {}, Test {}'.format(len(train_loader), len(val_loader), len(test_loader)))#, int(len(train_loader)/config['factor'])))\n",
    "\n",
    "        model_config = dict(in_channels=10, n_head=16, d_k=8,\n",
    "                            n_neurons=[256,128], dropout=0.2, d_model=256, mlp= [128, 64, 32, 19], T=1000, len_max_seq=53,\n",
    "                            positions=train_dt.date_positions if 'bespoke' == 'bespoke' else None)\n",
    "        \n",
    "        model = dLtae(**model_config)\n",
    "        wandb.init()\n",
    "        \n",
    "        with open(os.path.join(res_dir, 'Seed_{}'.format(seed), 'conf.json'), 'w') as file:\n",
    "            file.write(json.dumps(config, indent=4))\n",
    "        # break\n",
    "        model = model.to(device)\n",
    "        model.apply(weight_init)\n",
    "        steps_per_epoch = len(train_loader)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "            \n",
    "        criterion = FocalLoss(config['gamma'])\n",
    "        \n",
    "        model = model.double() #RuntimeError: expected scalar type Double but found Float \n",
    "        \n",
    "        trainlog = {}\n",
    "        \n",
    "        best_mIoU = 0\n",
    "        st_ = time.time()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print('EPOCH {}/{}'.format(epoch, epochs))\n",
    "            st__ = time.time()\n",
    "            model.train()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            # print(torch.get_num_threads())\n",
    "            # sys.exit()\n",
    "            train_metrics = train_epoch(model, optimizer, criterion, train_loader, device=device, config=config)\n",
    "            print(\"Training time for {} is {}\".format(epoch, (time.time() - start_time)/60))\n",
    "            \n",
    "            print('Validation . . . ')\n",
    "            start_time = time.time()\n",
    "            model.eval()\n",
    "            val_metrics = evaluation(model, criterion, val_loader, device=device, config=config, mode='val')\n",
    "\n",
    "            print('Loss {:.4f},  Acc {:.2f},  IoU {:.4f}'.format(val_metrics['val_loss'], val_metrics['val_accuracy'],\n",
    "                                                                 val_metrics['val_IoU']))\n",
    "            print(\"Validation time for {} is {}\".format(epoch, (time.time() - start_time)/60))\n",
    "            wandb.log({\"val_loss\": val_metrics['val_loss'], \"val_acc\": val_metrics['val_accuracy'], \"val_IoU\": val_metrics['val_IoU']})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "\n",
    "            trainlog[epoch] = {**train_metrics, **val_metrics}\n",
    "            checkpoint(trainlog, config)\n",
    "\n",
    "            if val_metrics['val_IoU'] >= best_mIoU:\n",
    "                best_mIoU = val_metrics['val_IoU']\n",
    "                torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "                           os.path.join(res_dir, 'Seed_{}'.format(seed), 'model.pth.tar'))\n",
    "            print(\"total time taken for {} epoch: {:.3f} mins.\".format(epoch, (time.time() - st__)/60))\n",
    "            \n",
    "            print('Testing best epoch . . .') #test on the best model only and that should be once.\n",
    "        model.load_state_dict(\n",
    "                torch.load(os.path.join(res_dir, 'Seed_{}'.format(seed), 'model.pth.tar'))['state_dict'])\n",
    "        start_time = time.time()\n",
    "        model.eval()\n",
    "\n",
    "        test_metrics, conf_mat, report_ = evaluation(model, criterion, test_loader, device=device, mode='test', config=config)\n",
    "\n",
    "        print('Loss {:.4f},  Acc {:.2f},  IoU {:.4f}'.format(test_metrics['test_loss'], test_metrics['test_accuracy'], test_metrics['test_IoU']))\n",
    "        print(\"Test time for {} is {}\".format(epoch, (time.time() - start_time)/60))\n",
    "        wandb.log({\"test_loss\": test_metrics['test_loss'], \"test_accuracy\": test_metrics['test_accuracy'], \"test_IoU\": test_metrics['test_IoU']})\n",
    "        \n",
    "        \n",
    "        save_results(test_metrics, conf_mat, report_, config)\n",
    "        \n",
    "        print(\"total time taken for all {} epochs: {:.3f} mins.\".format(epochs, (time.time() - st_)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c56814f-41d7-4c11-89da-0776893e9d0b",
   "metadata": {},
   "source": [
    "## Test dataset retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6592965-bc38-4639-a80d-1b9d734487cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataset import SITSData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b786ebae-400b-413d-b5b7-c0d98d0f515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_todevice(x, device):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device)\n",
    "    else:\n",
    "        return [recursive_todevice(c, device) for c in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21981948-11d9-404f-aa81-1ba6958d3b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, criterion, loader, device, config, mode='val'):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    acc_meter = tnt.meter.ClassErrorMeter(accuracy=True)\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    for (x, y) in loader:\n",
    "        start_time = time.time()\n",
    "        y_true.extend(list(map(int, y)))\n",
    "        x = recursive_todevice(x, device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(x)\n",
    "            loss = criterion(prediction, y)\n",
    "\n",
    "        acc_meter.add(prediction, y)\n",
    "        loss_meter.add(loss.item())\n",
    "\n",
    "        y_p = prediction.argmax(dim=1).cpu().numpy()\n",
    "        y_pred.extend(list(y_p))\n",
    "        \n",
    "        print(\"evaluation iteration completed in {:.4f} seconds\".format(time.time() - start_time))\n",
    "    metrics = {'{}_accuracy'.format(mode): acc_meter.value()[0],\n",
    "               '{}_loss'.format(mode): loss_meter.value()[0],\n",
    "               '{}_IoU'.format(mode): mIou(y_true, y_pred, config['num_classes'])}\n",
    "    label = [\"Dense built-up area\", \"Diffuse built-up area\", \"Industrial and commercial areas\", \"Roads\", \"Oilseeds (Rapeseed)\", \"Straw cereals (Wheat, Triticale, Barley)\", \"Protein crops (Beans / Peas)\", \"Soy\", \"Sunflower\", \"Corn\",  \"Tubers/roots\", \"Grasslands\", \"Orchards and fruit growing\", \"Vineyards\", \"Hardwood forest\", \"Softwood forest\", \"Natural grasslands and pastures\", \"Woody moorlands\", \"Water\"]\n",
    "    \n",
    "\n",
    "    if mode == 'val':\n",
    "        return metrics\n",
    "    elif mode == 'test':\n",
    "        return metrics, confusion_matrix(y_true, y_pred, labels=list(range(config['num_classes']))), f1_score(y_true, y_pred, average='macro'), classification_report(y_true, y_pred, target_names=label, digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec926309-3b89-4cb0-aaad-6f175a0d2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018\n",
    "case=\"1\"\n",
    "# m = '../../../results/ltae/results/2018/Seed_0/model.pth.tar'\n",
    "# state_dict = torch.load(m)['state_dict']\n",
    "c = \"../../../results/ltae/results/2018/Seed_0/conf.json\"\n",
    "config = json.load(open(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "055b3231-6f74-4479-bb32-6ff651dab674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_folder': '../../../results/ltae',\n",
       " 'npy': '../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz',\n",
       " 'dates': 'dates.txt',\n",
       " 'res_dir': '../../../results/ltae/results/2018',\n",
       " 'num_workers': 10,\n",
       " 'seed': 0,\n",
       " 'device': 'cuda',\n",
       " 'display_step': 100,\n",
       " 'factor': 5266,\n",
       " 'scheduler_': True,\n",
       " 'epochs': 15,\n",
       " 'batch_size': 2048,\n",
       " 'lr': 0.001,\n",
       " 'gamma': 1,\n",
       " 'in_channels': 10,\n",
       " 'n_head': 16,\n",
       " 'd_k': 8,\n",
       " 'n_neurons': [256, 128],\n",
       " 'T': 1000,\n",
       " 'positions': 'Bespoke',\n",
       " 'len_max_seq': 33,\n",
       " 'dropout': 0.2,\n",
       " 'd_model': 256,\n",
       " 'num_classes': 19,\n",
       " 'mlp4': [128, 64, 32, 19],\n",
       " 'preload': False,\n",
       " 'N_params': 97319,\n",
       " 'Train_loader_size': 5266,\n",
       " 'Val_loader_size': 785,\n",
       " 'Test_loader_size': 831}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aef48ef-3dce-4ef6-9318-72ff2a28f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019\n",
    "case=\"2\"\n",
    "m = '../../../results/ltae/results/2019/Seed_0/model.pth.tar'\n",
    "state_dict = torch.load(m)['state_dict']\n",
    "c = \"../../../results/ltae/results/2019/Seed_0/conf.json\"\n",
    "config = json.load(open(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e82c86bc-bc5b-47af-9399-52e1e53c8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = \"../../../results/ltae/results/2019/Seed_0/conf.json\"\n",
    "# config = json.load(open(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e770dff-37bb-42df-a7ff-59df17cf3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_ = np.loadtxt(glob.glob(config['dataset_folder'] + '/*mean.txt')[0])\n",
    "# std_ = np.loadtxt(glob.glob(config['dataset_folder'] + '/*std.txt')[0])\n",
    "# transform = transforms.Compose([standardize(mean_, std_)])\n",
    "    \n",
    "# sits_data = glob.glob(os.path.join(config['dataset_folder'], 'Seed_{}'.format(config['seed'])  + '/*.npz'))\n",
    "# doy = glob.glob(config['dataset_folder'] + '/gapfilled*.txt')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbf65c33-d6b3-4337-872e-d72a31405781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ = dLtae(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f42d384e-a314-4967-845b-6d380f4f8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model\n",
    "model = dLtae(in_channels = config['in_channels'], n_head = config['n_head'], d_k= config['d_k'], n_neurons=config['n_neurons'], dropout=config['dropout'], d_model= config['d_model'],\n",
    "                 mlp = config['mlp4'], T =config['T'], len_max_seq = config['len_max_seq'], \n",
    "              positions=date_positions if config['positions'] == 'bespoke' else None, return_att=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0eec3c-b654-4c71-84ad-bb363d17776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfada01-de5c-40eb-8287-3b81da233cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = model.double()\n",
    "model.load_state_dict(state_dict)\n",
    "    \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0af890d-a90e-4207-9fc6-57039a879c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 2018\n",
    "year = \"2018\"\n",
    "mean = np.loadtxt('../ltae/mean_std/source_mean.txt')\n",
    "std = np.loadtxt('../ltae/mean_std/source_std.txt')\n",
    "npz_ = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74e7ab78-6bae-4ec2-a264-bab0d86b0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019\n",
    "year = \"2019\"\n",
    "mean = np.loadtxt('../ltae/mean_std/target_mean.txt')\n",
    "std = np.loadtxt('../ltae/mean_std/target_std.txt')\n",
    "npz_ = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f1a2852-c04b-430c-b0d2-acea5dadf679",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_ = \"../../../data/theiaL2A_zip_img/output/2019\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee2e34fd-a56e-40bc-bd99-4a248d6ce854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019']\n"
     ]
    }
   ],
   "source": [
    "print(os.path.basename(npz_).split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7019e45d-a9c3-4ea4-9ba5-2f57bfe35dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 2019\n",
    "# case = \"2\"\n",
    "# mean = np.loadtxt('../ltae/mean_std/source_mean.txt')\n",
    "# std = np.loadtxt('../ltae/mean_std/source_std.txt')\n",
    "# npz_ = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3244125-0ef5-4a8a-a684-0f74f13700be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 2018\n",
    "# case = \"3\"\n",
    "# mean = np.loadtxt('../ltae/mean_std/target_mean.txt')\n",
    "# std = np.loadtxt('../ltae/mean_std/target_std.txt')\n",
    "# npz_ = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40227a8a-dee1-4ded-b98d-05506f066c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([standardize(mean, std)])\n",
    "\n",
    "dataset_test_alt = SITSData(npz_, config['seed'], config['dates'], partition='test', transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ce9d097-bdba-4d9d-9f77-62e95d3dc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = data.DataLoader(dataset_test_alt, batch_size=config['batch_size'],\n",
    "                                       num_workers=config['num_workers'], \n",
    "                                        shuffle=True,\n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74e6aaf9-2003-4fa9-8358-533ce8c82a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (x, y) in test_loader:\n",
    "#     print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b93ef03-21aa-4857-88f0-fc0c707970f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1518343\n",
      "742\n",
      "741.37841796875\n",
      "1519616\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_test_alt))\n",
    "print(len(test_loader))\n",
    "print(len(dataset_test_alt)/2048)\n",
    "print(len(test_loader)*2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b8501-02f6-4ce9-8936-052de7d7528a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ab9c662-659a-4013-be24-3e1ea4bfc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss(config['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4cdaa-cbbd-49b6-8a27-9bb3aceac9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics, conf_mat, fs, report_= evaluation(model, criterion, test_loader, device=device, mode='test', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104057cb-c228-44d8-9098-7da03c18e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loss {:.4f},  Acc {:.2f},  IoU {:.4f}'.format(test_metrics['test_loss'], test_metrics['test_accuracy'], test_metrics['test_IoU']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c084061-cf8d-4175-8fec-c2a2e333654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reports/LTAE_case_{}_{}_report\".format(case, year), \"w\") as f:\n",
    "    f.write(report_)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47b55fdf-c71b-444b-9607-c8f2654c87d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../results/ltae/results/2019'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['res_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a4bbc6e-95c3-46cf-a250-e8bb5d47c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(metrics, conf_mat, config):\n",
    "    with open(os.path.join(config['res_dir'], 'Seed_{}'.format(config['seed']), 'seed_{}_case_{}_batchsize_{}_epochs_{}_factor_{}_test_metrics.json'.format(config['seed'], case, config['batch_size'], config['epochs'], config['factor'])), 'w') as outfile:\n",
    "        json.dump(metrics, outfile, indent=4)\n",
    "    # pkl.dump(conf_mat, open(os.path.join(config['res_dir'], 'Seed_{}'.format(config['seed']), 'seed_{}_batchsize_{}_epochs_{}_factor_{}_conf_mat.pkl'.format(config['seed'], config['batch_size'], config['epochs'], config['factor'])), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4d6da4d-addc-4d98-bc34-cdc25abc2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(test_metrics, conf_mat, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2161d2cf-0b4e-498a-8735-da1570520d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = '../../../results/ltae/trials/Seed_0/model.pth.tar'\n",
    "\n",
    "# model.load_state_dict(torch.load(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e22c9fe-bd63-47f3-ab11-e2fca1c0fe8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def date_positions(gfdate_path):\n",
    "    with open(gfdate_path, \"r\") as f:\n",
    "        out_date_list = f.readlines()\n",
    "    out_date_list = [x.strip() for x in out_date_list]\n",
    "    out_date_list = [datetime.datetime.strptime(x, \"%Y%m%d\").timetuple().tm_yday for x in out_date_list]\n",
    "    string_date_list = [x for x in out_date_list]\n",
    "    return string_date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba89faf-2ff1-4f60-a5e8-e0de805f4875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# st = time.time()\n",
    "# t = date_positions(\"dates.txt\")\n",
    "# s = date_positions(\"dates.txt\")\n",
    "# print(t)\n",
    "# print(s)\n",
    "# print(\"time: \", time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4bae7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df0354f6-6fab-4408-9bf8-049de825ae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed value:  4\n",
      "Read set ids completed: 0.004056215286254883 second\n"
     ]
    }
   ],
   "source": [
    "def generate_ids():\n",
    "    \"\"\"\n",
    "    Descr: \n",
    "        Aim: To write and returns the partition (Train, Validation and Test) ids \n",
    "        with respect to the grid split index (range 1 - 100)\n",
    "        \n",
    "        - A random seed value is set within a random intger 1-10,\n",
    "        - the set is spltted into 80:10:10,\n",
    "        - save into a text file with the seed value used\n",
    "    \"\"\"\n",
    "    # set a random seed value within the range 1 -10 \n",
    "    start_time = time.time()\n",
    "    seed_value = 4\n",
    "    # seed_value = np.random.randint(0,10)\n",
    "    np.random.seed(seed_value)\n",
    "    # # block id range 1 - 100 (splitted grid)\n",
    "    block_range = np.arange(1, 101)\n",
    "\n",
    "    # Train, Validation and Test\n",
    "    random.shuffle(block_range)\n",
    "    train_id = block_range[:60] # 60%\n",
    "    val_id = block_range[60:70] # 10%\n",
    "    test_id = block_range[70:] # 30%\n",
    "    print(\"Seed value: \", seed_value)\n",
    "    \n",
    "    if not os.path.exists(\"./ids/train_val_eval_seed_\" + str(seed_value)+\".txt\"):\n",
    "        with open(\"./ids/train_val_eval_seed_\" + str(seed_value)+\".txt\", \"w\") as f:\n",
    "            f.write(\"Training: \" + str(list(train_id)) + \"\\n\")\n",
    "            f.write(\"Validation: \" + str(list(val_id)) + \"\\n\")\n",
    "            f.write(\"Testing: \" + str(list(test_id)) + \"\\n\")\n",
    "            f.close()\n",
    "    print('Read set ids completed: %s second' % (time.time() - start_time))\n",
    "generate_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276ef82d-965b-4daa-bb1d-29d4d017d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ids(seed_value):\n",
    "    \"\"\"\n",
    "    Read ids from file\n",
    "    \"\"\"\n",
    "    assert seed_value >= 0 and seed_value <= 10\n",
    "    \n",
    "    with open(\"./ids/train_val_eval_seed_\" + str(seed_value)+\".txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        Train_ids = eval(lines[0].split(\":\")[1])\n",
    "        Val_ids = eval(lines[1].split(\":\")[1])\n",
    "        test_ids = eval(lines[2].split(\":\")[1])\n",
    "    return Train_ids, Val_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09eda6b1-6b84-4958-bedc-d1d7b09ab679",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_ids, Val_ids, test_ids = read_ids(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c54f52d-491e-4370-a655-fbe313d4a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = Val_ids + test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fec2129-501a-494e-9391-0082163b4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "s = glob.glob(\"../../../data/theiaL2A_zip_img/output/2018/Seed_0\" + \"/*.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cfa68ac-d630-4be7-9663-86f3e22e37dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../data/theiaL2A_zip_img/output/2018/Seed_0/val.npz'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d45cfe-a8f6-4922-9964-ce63bed9c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(source_sits, target_sits, case):\n",
    "    \"\"\"\n",
    "    Descr: Compute mean and std for each channel\n",
    "    Input: both SITS dataset(.npz) paths\n",
    "            Case[1 - 3]:\n",
    "            1 - concatenate both dataset, while 2 & 3 rep source and target respectively\n",
    "    The data(from N,LxD) is reshaped into (N,D,L);\n",
    "        where N - pixel, D - Bands (10), L - Time (33)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # case = 1: both, case = 2: target, case = 3: target\n",
    "    if case == 1:\n",
    "        sits = [source_sits, target_sits]\n",
    "    elif case == 2:\n",
    "        sits = source_sits\n",
    "    elif case == 3:\n",
    "        sits = target_sits\n",
    "    else:\n",
    "        print('Select case between 1-3')\n",
    "        return None\n",
    "    \n",
    "    # if sits is a list, then it's a list of paths\n",
    "    if isinstance(sits, list):\n",
    "        # load data\n",
    "        X_source = np.load(sits[0])['X']\n",
    "        X_target = np.load(sits[1])['X']\n",
    "        # concatenate the data\n",
    "        X = np.concatenate((X_source, X_target), axis=0)\n",
    "    # if sits is a string, then it's a path\n",
    "    else: \n",
    "        with np.load(sits) as data:\n",
    "            X = data['X']\n",
    "\n",
    "    X = X.reshape(X.shape[0], n_channel, int(X.shape[1]/n_channel))\n",
    "    # compute mean and std\n",
    "    X_mean = np.mean(X, axis=(0,2))\n",
    "    X_std = np.std(X, axis=(0,2))\n",
    "    print('mean shape: ', X_mean.shape)\n",
    "    print('std shape: ', X_std.shape)\n",
    "    # save X_mean and X_std sepearately for sits as txt file\n",
    "    np.savetxt(os.path.join('mean_'+ str(case) +'.txt'), X_mean)\n",
    "    np.savetxt(os.path.join('std_'+ str(case) +'.txt'), X_std)\n",
    "\n",
    "# for i in [1,2,3]:\n",
    "#     start_time = time.time()\n",
    "#     source_path = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "#     target_path = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "#     compute_mean_std(source_path, target_path, i)\n",
    "#     print(\"run time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d277504e-263a-44ba-81b3-ecc67ccf65a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data completed:  92.74277639389038\n",
      "mean shape:  (10,)\n",
      "std shape:  (10,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "def compute_mean_stdv2(sits, domain='source'):\n",
    "    \"\"\"\n",
    "    Descr: Compute mean and std for each channel\n",
    "    Input: both SITS dataset(.npz) paths\n",
    "    The data(from N,LxD) is reshaped into (N,D,L);\n",
    "        where N - pixel, D - Bands (10), L - Time (33)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    with np.load(sits) as data:\n",
    "            X = data['X']\n",
    "    print(\"data completed: \", time.time() - start_time)\n",
    "    \n",
    "    n_channel = 10\n",
    "    \n",
    "    X = X.reshape(X.shape[0], n_channel, int(X.shape[1]/n_channel))\n",
    "    # compute mean and std\n",
    "    X_mean = np.mean(X, axis=(0,2))\n",
    "    X_std = np.std(X, axis=(0,2))\n",
    "    print('mean shape: ', X_mean.shape)\n",
    "    print('std shape: ', X_std.shape)\n",
    "    # save X_mean and X_std sepearately for sits as txt file\n",
    "    np.savetxt(os.path.join('./mean_std/', domain + '_mean.txt'), X_mean)\n",
    "    np.savetxt(os.path.join('./mean_std/', domain + '_std.txt'), X_std)\n",
    "\n",
    "source_path = '../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz'\n",
    "# target_path = '../../../data/theiaL2A_zip_img/output/2019/2019_SITS_subset_data.npz'\n",
    "compute_mean_stdv2(source_path, '2019')\n",
    "# compute_mean_stdv2(target_path, domain = 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d4055b3-c150-4215-9576-9569a415ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SITSDatav2(data.Dataset):\n",
    "#     def __init__(self, sits, seed, partition='train', transform=None):\n",
    "        \n",
    "#         self.sits = sits\n",
    "#         self.seed = seed\n",
    "#         self.transform = transform\n",
    "        \n",
    "#         # get partition ids using the read_id() func\n",
    "#         start_time = time.time()\n",
    "#         self.train_ids, self.val_ids, self.test_ids = read_ids(self.seed)\n",
    "#         print(\"read ids completed: %s second\" % (time.time() - start_time))\n",
    "\n",
    "#         # select partition\n",
    "#         if partition == 'train':\n",
    "#             ids = self.train_ids\n",
    "#         elif partition == 'val':\n",
    "#             ids = self.val_ids\n",
    "#         elif partition == 'test':\n",
    "#             ids = self.test_ids\n",
    "#         else:\n",
    "#             raise ValueError('Invalid partition: {}'.format(partition))\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         print('reading files....')\n",
    "#         X, y, block_ids = load_npz(self.sits)\n",
    "#         print(\"load npz: %s seconds\" % (time.time() - start_time))\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         y = np.unique(y, return_inverse=True)[1]\n",
    "#         print(\"reassigning %s seconds\" % (time.time() - start_time))\n",
    "        \n",
    "#         # concatenate the data\n",
    "#         start_time = time.time()\n",
    "#         data_ = np.concatenate((X, y[:, None], block_ids[:, None]), axis=1)\n",
    "#         print(\"Concatenating completed: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "#         # filter by block_id\n",
    "#         start_time = time.time()\n",
    "#         data_ = data_[np.isin(data_[:, -1], ids)]\n",
    "#         print(\"filtering ids completed: %s seconds\" % (time.time() - start_time))\n",
    "        \n",
    "#         self.X_ = data_[:, :-2]\n",
    "#         self.y_ = data_[:, -2]                          \n",
    "        \n",
    "#         del X\n",
    "#         del y\n",
    "#         del block_ids\n",
    "#         del data_\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.y_)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         start_time = time.time()\n",
    "#         self.X = self.X_[idx]\n",
    "#         self.y = self.y_[idx]\n",
    "#         print(\"getting data: %s seconds\" % ((time.time() - start_time)*100))\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         self.X = np.array(self.X).astype('float32')\n",
    "#         self.y = np.array(self.y).astype('float32')\n",
    "#         print(\"conversion: %s seconds\" % ((time.time() - start_time)*100))\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         self.X = self.X.reshape(int(self.X.shape[0]/n_channel), n_channel)\n",
    "#         print(\"reshape data: %s seconds\" % ((time.time() - start_time)*100))\n",
    "\n",
    "#         # transform\n",
    "#         start_time = time.time()\n",
    "#         if self.transform:\n",
    "#             self.X = self.transform(self.X)\n",
    "#         print(\"transform data: %s seconds\" % ((time.time() - start_time)*100))\n",
    "#         print(self.X.shape)\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         torch_x = torch.from_numpy(self.X)\n",
    "#         torch_y = torch.from_numpy(self.y)\n",
    "#         print(\"tensor: %s seconds\" % ((time.time() - start_time)*100))\n",
    "        \n",
    "#         return torch_x, torch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d3d372-5a12-47a2-9c2f-6ae05d8a235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(\"../../../data/theiaL2A_zip_img/output/2018/Seed_0/test.npz\") as f:\n",
    "    X = f['X']\n",
    "    y = f['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929f2705-02e7-4283-8e82-1b1e4cd82635",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, val_ids, test_ids = read_ids(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5432b2dc-a06b-4188-8a60-7340a15da117",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.unique(y, return_inverse=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ce84b9-2afb-409c-ae31-bbc4b893ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import numpy as np\n",
    "import os\n",
    "def separate_data(sits, seed, outdir):\n",
    "\n",
    "    train_ids, val_ids, test_ids = read_ids(seed)\n",
    "\n",
    "    # if partition == 'train':\n",
    "    #     ids = train_ids\n",
    "    # elif partition == 'val':\n",
    "    #     ids = val_ids\n",
    "    # elif partition == 'test':\n",
    "    #     ids = test_ids\n",
    "    # else:\n",
    "    #     raise ValueError('Invalid partition: {}'.format(partition))\n",
    "\n",
    "    X, y, block_ids = load_npz(sits)\n",
    "    print(\"npz....\")\n",
    "\n",
    "    y = np.unique(y, return_inverse=True)[1] # reassigning label [1,23] to [0,18]\n",
    "    print(\"reaass....\")\n",
    "\n",
    "    # concatenate the data\n",
    "    data_ = np.concatenate((X, y[:, None], block_ids[:, None]), axis=1)\n",
    "    print(\"concatenating....\")\n",
    "    \n",
    "    del X\n",
    "    del y\n",
    "\n",
    "    def pickk(partition, dt_):\n",
    "        \n",
    "        if partition == 'train':\n",
    "            ids = train_ids\n",
    "        elif partition == 'val':\n",
    "            ids = val_ids\n",
    "        elif partition == 'test':\n",
    "            ids = test_ids\n",
    "        else:\n",
    "            raise ValueError('Invalid partition: {}'.format(partition))\n",
    "        \n",
    "        print(\"filter....\")    \n",
    "        dat = dt_[np.isin(dt_[:, -1], ids)]\n",
    "        print(\"filter done....\")    \n",
    "        \n",
    "        print(\"selecting....\")    \n",
    "        X_ = dat[:, :-2]\n",
    "        y_ = dat[:, -2]\n",
    "        print(\"selecting done....\")    \n",
    "        \n",
    "        print(\"savinng....\")    \n",
    "        np.savez_compressed(os.path.join(outdir, \"{}.npz\".format(partition)), X=X_, y=y_)\n",
    "        print(\"completed....\")    \n",
    "    pickk('train', data_)\n",
    "    print(\"1 done....\")    \n",
    "    pickk('val', data_)\n",
    "    print(\"2 done....\")    \n",
    "    pickk('test', data_)\n",
    "    print(\"3 done....\")    \n",
    "\n",
    "    # train_data = data_[np.isin(data_[:, -1], train_ids)]\n",
    "    # np.save(os.path.join(outdir, \"Xtrain.npy\".format(partition)), X_)\n",
    "    # np.save(os.path.join(outdir, \"ytrain.npy\")), y_)\n",
    "    # val_data = data_[np.isin(data_[:, -1], val_ids)]\n",
    "    # test_data = data_[np.isin(data_[:, -1], test_ids)]\n",
    "\n",
    "#     X_ = data_[:, :-2]\n",
    "#     y_ = data_[:, -2]\n",
    "    \n",
    "#     np.save(os.path.join(outdir, \"X{}.npy\".format(partition)), X_)\n",
    "#     np.save(os.path.join(outdir, \"y{}.npy\".format(partition)), y_)\n",
    "\n",
    "# def prepare_output():\n",
    "#     os.makedirs(os.path.join(outdir,  exist_ok=True)\n",
    "#     os.makedirs(os.path.join(config['res_dir'], 'Seed_{}'.format(config['seed'])), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db64ce0-0dc9-4653-8095-e59f8c168b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npz....\n",
      "reaass....\n",
      "concatenating....\n",
      "filter....\n",
      "filter done....\n",
      "selecting....\n",
      "selecting done....\n",
      "savinng....\n",
      "completed....\n",
      "1 done....\n",
      "filter....\n",
      "filter done....\n",
      "selecting....\n",
      "selecting done....\n",
      "savinng....\n",
      "completed....\n",
      "2 done....\n",
      "filter....\n",
      "filter done....\n",
      "selecting....\n",
      "selecting done....\n",
      "savinng....\n",
      "completed....\n",
      "3 done....\n"
     ]
    }
   ],
   "source": [
    "sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "seed =0\n",
    "# partition = \"train\"\n",
    "outdir = \"../../../data/theiaL2A_zip_img/output/2018/Seed_0\"\n",
    "separate_data(sits, seed, outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc04f1-d3de-423d-8173-16c56baf80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "# seed =0\n",
    "# partition = \"val\"\n",
    "# outdir = \"../../../data/theiaL2A_zip_img/output/2018/Seed_0\"\n",
    "# separate_data(sits, seed, partition, outdir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bbf9ea-d133-44ef-949f-8a5af87a6383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "# seed =0\n",
    "# partition = \"test\"\n",
    "# outdir = \"../../../data/theiaL2A_zip_img/output/2018/Seed_0\"\n",
    "# separate_data(sits, seed, partition, outdir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8190c2ef-d5ca-4ace-8b0b-34c6d4dc3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class standardize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return (sample - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72891200-63fa-4824-8af8-eb075be09135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.loadtxt(glob.glob(\"../../../data/theiaL2A_zip_img/output/2018\" + \"/gapfilled*.txt\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bfc5221-b32b-4f7c-8423-0fadd26f2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __init__(self, sits, seed, date_, partition='train', transform=None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9aa04b1-cd97-4f62-8bef-cb40c4c0b9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init completed...\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "mean = np.loadtxt('./mean_std/2018_mean.txt')\n",
    "std = np.loadtxt('./mean_std/2018_std.txt')\n",
    "seed = 0 \n",
    "transform = transforms.Compose([standardize(mean, std)])\n",
    "\n",
    "# paths\n",
    "source_path = '../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz'\n",
    "# target_path = '../../../data/theiaL2A_zip_img/output/2019/2019_SITS_subset_data.npz'\n",
    "\n",
    "# doy\n",
    "doy = glob.glob(\"../../../data/theiaL2A_zip_img/output/2018\" + \"/gapfilled*.txt\")[0]\n",
    "train_dataset = SITSData(source_path, seed, doy, partition='train', transform=transform)\n",
    "# val_dataset = SITSDatav2(source_path, seed, partition='val', transform=transform)\n",
    "# test_dataset = SITSDatav2(source_path, seed, partition='test', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12e93daf-6a7a-4a1c-860c-48315dd43f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init completed...\n"
     ]
    }
   ],
   "source": [
    "train_datasetv = SITSData(source_path, seed, doy, partition='train', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e15d232-0a63-4860-976c-11a22f99cf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init completed...\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SITSData(source_path, seed, doy, partition='test', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "620bb70f-0b57-49ae-8b91-72b2b2d3cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_dataset, batch_size=2048,\n",
    "                                       num_workers=10,\n",
    "                                       shuffle=True,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd1385b-6909-4257-b631-68d733493857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.67607998847961\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "source_path = '../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz'\n",
    "X, y, _ = load_npz(source_path)\n",
    "print(time.time() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
