{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e40c887-a0e1-425e-9a17-5ddaf831bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "# from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from utils import load_npz, read_ids\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e32007-c4ac-4705-af7b-fbf1f48693ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a .npz file\n",
    "    \"\"\"\n",
    "    with np.load(file_path) as data:\n",
    "        X = data[\"X\"]\n",
    "        y = data[\"y\"]\n",
    "        # polygon_ids = data[\"polygon_ids\"]\n",
    "        block_ids = data[\"block_ids\"]\n",
    "    return X, y, block_ids#, polygon_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508ec429-bfa4-486e-918d-e376bba22a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 33\n",
    "n_channel = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8ee8c9-1d5d-4ddb-9a4c-ce8236e38d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    \"\"\"\n",
    "    X: ()\n",
    "    \"\"\"\n",
    "    m = X.mean(axis=0)\n",
    "    s = X.std(axis=0)\n",
    "    X = (X - m) / s\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7200bc74-efc7-4ce5-a22f-0d8ed3662b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.99216985702515\n"
     ]
    }
   ],
   "source": [
    "source_sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "train_val_eval = \"../RF_model/train_val_eval_rf.txt\"\n",
    "\n",
    "train_ids, test_ids = read_ids(train_val_eval)\n",
    "X_source, y_source, block_ids_source = load_npz(source_sits)\n",
    "_source = np.concatenate((X_source, y_source[:, None], block_ids_source[:, None]), axis=1)\n",
    "_source = _source[np.isin(_source[:, -1], train_ids)]\n",
    "Xtrain_source = _source[:, :-2]\n",
    "ytrain_source = _source[:, -2]\n",
    "# Xtrain = np.concatenate((Xtrain_source, Xtrain_target), axis=0)\n",
    "# ytrain = np.concatenate((ytrain_source, ytrain_target), axis=0)\n",
    "Xtrain = Xtrain_source\n",
    "ytrain = ytrain_source\n",
    "x, y = Xtrain, ytrain\n",
    "x = x.reshape(x.shape[0], L, n_channel)\n",
    "x = x.transpose(0,2,1)\n",
    "startime =time.time()\n",
    "x_ = (standardize(x)).astype('float16')\n",
    "print(time.time() - startime)\n",
    "# x_ = x[idx]\n",
    "# y = y.astype('int8')[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26eec111-16a7-4474-83cf-bbee7ca334ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x.mean(axis=0)).shape\n",
    "# x.std(axis=0).astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fd08a6-8638-41d7-ad39-401df2c8064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e36058-37f9-4a04-8c3d-60e0e5093537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15abcb2a-934e-4f66-a1f4-9da76894afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_sits = \"../subset_data/2018_subset.npz\"\n",
    "# target_sits = \"../subset_data/2019_subset.npz\"\n",
    "# train_val_eval = \"train_val_eval_rf.txt\"\n",
    "# case = 1\n",
    "# set_ = \"trainval\"\n",
    "# # loader = data.DataLoader(dataset, batch_size = 10)\n",
    "# transform = None\n",
    "# import time\n",
    "# st = time.time()\n",
    "# dataset = SitsData(case, source_sits, target_sits, train_val_eval, set_, transform)\n",
    "# loader = data.DataLoader(dataset, batch_size = 10)\n",
    "# v, y = next(iter(loader))\n",
    "# print(\"run time: \", st -time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed07ee55-9a88-4f0b-bcfe-f3f51359f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = data.DataLoader(dataset, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcbca819-e08b-4f3c-b0d6-85a25225bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v, y = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c625e9da-8b79-407c-a726-4e3a3ac1d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c980a301-d9bc-42a8-8207-8bfc5598a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SITSData(data.Dataset):\n",
    "    def __init__(self, case: int, source_sits, target_sits, train_val_eval, set_= 'trainval', transform=None):\n",
    "        ## OPTION 1 (good)\n",
    "        # sits\n",
    "        ## OPTION 2 (not good)\n",
    "        # [source_sits] case 2\n",
    "        # [source_list, target_list] case 1\n",
    "        self.source_sits = source_sits\n",
    "        self.target_sits = target_sits\n",
    "        self.train_val_eval = train_val_eval\n",
    "        self.transform = transform\n",
    "        self.case = case\n",
    "        self.set_ = set_\n",
    "\n",
    "        # read the set ids\n",
    "        start_time = time.time()\n",
    "        self.train_ids, self.test_ids = read_ids(self.train_val_eval)\n",
    "        print(\"read ids %s seconds ---\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "        # case selection\n",
    "        if self.set_ == 'trainval':\n",
    "            ids = self.train_ids\n",
    "        elif self.set_ == 'test':\n",
    "            ids = self.test_ids\n",
    "        else:\n",
    "            raise ValueError(\"Please choose a set between trainval and test\")\n",
    "        print(\"case %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        # read the data \n",
    "        start_time = time.time()\n",
    "        X_source, y_source, block_ids_source = load_npz(self.source_sits)\n",
    "        X_target, y_target, block_ids_target = load_npz(self.target_sits)\n",
    "        print(\"load npz %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "        start_time = time.time()\n",
    "        _source = np.concatenate((X_source, y_source[:, None], block_ids_source[:, None]), axi =1)\n",
    "        _target = np.concatenate((X_target, y_target[:, None], block_ids_target[:, None]), axis=1)\n",
    "        print(\"concatenate %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "        start_time = time.time()\n",
    "        _source = _source[np.isin(_source[:, -1], ids)]\n",
    "        _target = _target[np.isin(_target[:, -1], ids)]\n",
    "        print(\"isin %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.Xtrain_source = _source[:, :-2]\n",
    "        self.ytrain_source = _source[:, -2]\n",
    "        print(\"remove ids source %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.Xtrain_target = _target[:, :-2]\n",
    "        self.ytrain_target = _target[:, -2]\n",
    "        print(\"remove ids target %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        start_time = time.time()\n",
    "        if self.case == 1:\n",
    "            self.Xtrain = np.concatenate((self.Xtrain_source, self.Xtrain_target), axis=0)\n",
    "            self.ytrain = np.concatenate((self.ytrain_source, self.ytrain_target), axis=0)\n",
    "        elif self.case == 2:\n",
    "            self.Xtrain = self.Xtrain_source\n",
    "            self.ytrain = self.ytrain_source\n",
    "        elif self.case == 3:\n",
    "            self.Xtrain = self.Xtrain_target\n",
    "            self.ytrain = self.ytrain_target\n",
    "        else:\n",
    "            raise ValueError(\"Please choose a case between 1 and 3\") \n",
    "        print(\"concatenate %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "        \n",
    "        # Reqd meqn qnd std\n",
    "        if year = \"2018\":\n",
    "            read(...)\n",
    "        else:\n",
    "            reas(...)\n",
    "        mean = np.mean(...)\n",
    "        std = np.mean(...)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ytrain)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = time.time()\n",
    "        x = self.Xtrain[idx]\n",
    "        x, y = self.Xtrain, self.ytrain\n",
    "        x = x.reshape(x.shape[0], L, n_channel)\n",
    "        x = x.transpose(0,2,1)\n",
    "        print(\"reshape and transpose %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        x = (standardize(x)).astype('float32')\n",
    "        x_ = x[idx]\n",
    "        y = y.astype('int8')[idx]\n",
    "        print(\"stardardization %s seconds ---\" % (time.time() - start_time))\n",
    "        # start_time = time.time()\n",
    "        # x_ = torch.from_numpy(np.array(x_))\n",
    "        # y = torch.from_numpy(np.array(y))\n",
    "        # print(\"to tenors %s seconds ---\" % (time.time() - start_time))\n",
    "        # return x, y\n",
    "        return torch.from_numpy(np.array(x_)), torch.from_numpy(np.array(y))\n",
    "\n",
    "def standardize(X, year=\"2018\"):\n",
    "    \"\"\"\n",
    "    X: (b,D,L)\n",
    "    \"\"\"\n",
    "    #print(X)\n",
    "    print(X.shape)\n",
    "    m = X.mean(axis=0) # axis=(0,2)\n",
    "    #print(\"mean: \", m)\n",
    "    s = X.std(axis=0)\n",
    "    X = (X - m) / s\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4086c53-dfba-4496-83ce-db2cf0611111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "# load_npz(source_sits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0989d84e-934d-4d12-8f2e-2ab369d62214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read ids 0.0020072460174560547 seconds ---\n",
      "case 2.6226043701171875e-06 seconds ---\n",
      "load npz 2.311331498622894 minutes ---\n",
      "concatenate 0.12044020891189575 minutes ---\n",
      "isin 0.21219375928243 minutes ---\n",
      "remove ids source 3.457069396972656e-05 seconds ---\n",
      "remove ids target 2.0265579223632812e-05 seconds ---\n",
      "concatenate 0.06584848562876383 minutes ---\n",
      "run time:  2.7105690161387126\n"
     ]
    }
   ],
   "source": [
    "#source_sits = \"../subset_data/2018_subset.npz\"\n",
    "#target_sits = \"../subset_data/2019_subset.npz\"\n",
    "# train_val_eval = \"train_val_eval_rf.txt\"\n",
    "source_sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "target_sits = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "train_val_eval = \"../RF_model/train_val_eval_rf.txt\"\n",
    "case = 1\n",
    "set_ = \"trainval\"\n",
    "transform = None\n",
    "import time\n",
    "start_time = time.time()\n",
    "train_dataset = SITSData(case, source_sits, target_sits, train_val_eval, set_, transform)\n",
    "test_dataset = \n",
    "train_loader = data.DataLoader(train_dataset, batch_size = 1, pin_memory=True)\n",
    "\n",
    "print(\"run time: \", ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9293ac4-3893-4fa6-b16f-a48753263070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape and transpose 1.3828277587890625e-05 seconds ---\n",
      "(16415862, 10, 33)\n",
      "stardardization 82.17333197593689 seconds ---\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train= next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f9929-58af-472e-87e1-c39bd052ff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape and transpose 4.029273986816406e-05 seconds ---\n",
      "stardardization 82.94411087036133 seconds ---\n",
      "reshape and transpose 3.147125244140625e-05 seconds ---\n",
      "stardardization 82.9876160621643 seconds ---\n",
      "reshape and transpose 4.673004150390625e-05 seconds ---\n"
     ]
    }
   ],
   "source": [
    "for (x,y) in enumerate(train_loader):\n",
    "    y_ry = y\n",
    "    x_hr = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a54688b-cbdf-4b7d-beb7-bdffcd037e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read ids 0.002400636672973633 seconds ---\n",
      "case 1.9073486328125e-06 seconds ---\n",
      "load npz 1.9369140625 minutes ---\n",
      "concatenate 0.12013090848922729 minutes ---\n",
      "isin 0.14379287560780843 minutes ---\n",
      "remove ids source 4.100799560546875e-05 seconds ---\n",
      "remove ids target 5.245208740234375e-06 seconds ---\n",
      "concatenate 0.0441957155863444 minutes ---\n",
      "run time:  134.75104451179504\n"
     ]
    }
   ],
   "source": [
    "source_sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "target_sits = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "train_val_eval = \"../RF_model/train_val_eval_rf.txt\"\n",
    "case = 1\n",
    "set_ = \"test\"\n",
    "transform = None\n",
    "import time\n",
    "st = time.time()\n",
    "dataset_test = SITSData(case, source_sits, target_sits, train_val_eval, set_, transform)\n",
    "test_loader = data.DataLoader(dataset_test, batch_size = 1, pin_memory=True)\n",
    "# x_test,y_test= next(iter(loader))\n",
    "print(\"run time: \", time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "239ec5e1-816a-44ae-922b-d69a9ffdd8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape and transpose 2.7179718017578125e-05 seconds ---\n",
      "stardardization 54.03708863258362 seconds ---\n",
      "to tenors 0.030365467071533203 seconds ---\n",
      "run time:  0.9012147784233093\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "x_test,y_test= next(iter(test_loader))\n",
    "print(\"run time: \", ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ec355-7ff2-4751-a020-f5f5e57a2a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40035ce-2ad3-4cdf-8c7f-ab0f124a7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # useful functions that are needed in the dataset class\n",
    "# class SitsData(data.Dataset):\n",
    "#     def __init__(self, case: int, source_sits, target_sits, train_val_eval, set_= 'trainval', transform=None):\n",
    "#         super(SitsData, self).__init__()\n",
    "#         self.source_sits = source_sits\n",
    "#         self.target_sits = target_sits\n",
    "#         self.train_val_eval = train_val_eval\n",
    "#         self.transform = transform\n",
    "#         self.case = case\n",
    "#         self.set_ = set_\n",
    "        \n",
    "#         self.x, self.y = self._init_dataset()\n",
    "        \n",
    "#     def load_set(self, total_set):\n",
    "#         \"\"\"\n",
    "#             Load a set of data:\n",
    "#             to be used in prepare_data(), it organise dataset according to the set_name based block_ids\n",
    "#             - set_name: train or test (reads from the block_ids using read_ids() utils) \n",
    "#             - total_set: the concatenated set of data for source or target\n",
    "#             - returns X and Y:\n",
    "#                 X is the data, Y is the label\n",
    "#         \"\"\"\n",
    "#         self.train_ids, self.test_ids = read_ids(self.train_val_eval)\n",
    "\n",
    "#         if self.set_ == \"trainval\":\n",
    "#             ids = self.train_ids\n",
    "#         elif self.set_ == \"test\":\n",
    "#             ids = self.test_ids\n",
    "#         else:\n",
    "#             raise ValueError(\"Please choose a set between trainval and test\")\n",
    "\n",
    "#         set = total_set[np.isin(total_set[:, -1], ids)]\n",
    "#         X = set[:, :-2]\n",
    "#         Y = set[:, -2]\n",
    "            \n",
    "#         return X, Y\n",
    "        \n",
    "#     def _init_dataset(self):\n",
    "#         \"\"\"\n",
    "#             Prepare data:\n",
    "#             - load data from npz files(load_npz() in utils): returns X, Y and block_ids\n",
    "#             - concatenate the data from source and target [X, Y, block_ids] == total_set\n",
    "#             - split the data into train and test sets for source and target\n",
    "#             - returns Xtrain and Ytrain base on CASE value\n",
    "#             CASE: \n",
    "#             - 1: Train on Soucre and target, test on source and target\n",
    "#             - 2: Train on Source only, test on Source and target\n",
    "#             - 3: Train on Target only, test on Source and target\n",
    "#         \"\"\"\n",
    "#             # print(\"Preparing data.........\")\n",
    "\n",
    "#         X_s, Y_s, block_ids_s = load_npz(self.source_sits)\n",
    "#         X_t, Y_t, block_ids_t = load_npz(self.target_sits)\n",
    "#         # print(\"Loading npz files done.........\")\n",
    "            \n",
    "\n",
    "#         self.total_set_s = np.concatenate((X_s, Y_s[:, None], block_ids_s[:, None]), axis=1)\n",
    "#         self.total_set_t = np.concatenate((X_t, Y_t[:, None], block_ids_t[:, None]), axis=1)\n",
    "#             # print(\"Concatenating data done.........\")\n",
    "\n",
    "#             # Training set for target and source\n",
    "#         self.Xtrain_s, self.Ytrain_s = self.load_set(self.total_set_s)\n",
    "#         self.Xtrain_t, self.Ytrain_t = self.load_set(self.total_set_t)\n",
    "#             # print(\"Loading train set done.........\")\n",
    "\n",
    "#             # Test set for target and source\n",
    "#         self.Xtest_s, self.Ytest_s = self.load_set(self.total_set_s)\n",
    "#         self.Xtest_t, self.Ytest_t = self.load_set(self.total_set_t)\n",
    "#             # print(\"Loading test set done.........\")\n",
    "\n",
    "#         if self.case == 1:\n",
    "#                 # concatenate training set for target and source\n",
    "#             self.Xtrain = np.concatenate((self.Xtrain_s, self.Xtrain_t), axis=0)\n",
    "#             self.Ytrain = np.concatenate((self.Ytrain_s, self.Ytrain_t), axis=0)\n",
    "\n",
    "#         elif self.case == 2:\n",
    "#                 # Xtrain is the training set for source only\n",
    "#             self.Xtrain = self.Xtrain_s\n",
    "#             self.Ytrain = self.Ytrain_s\n",
    "\n",
    "#         elif self.case == 3:\n",
    "#                 # Xtrain is the training set for target only\n",
    "#             self.Xtrain = self.Xtrain_t\n",
    "#             self.Ytrain = self.Ytrain_t\n",
    "\n",
    "#         else:\n",
    "#             raise ValueError(\"Please choose a case between 1 and 3\")\n",
    "            \n",
    "#         return self.Xtrain, self.Ytrain\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         x_, y_ = self.x[idx], self.y[idx]\n",
    "            \n",
    "#         x = torch.from_numpy(np.array(x_, dtype=int))\n",
    "#         y = torch.from_numpy(np.array(y_, dtype=int))\n",
    "#         return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca095d2-1c7b-4568-a706-98852de197f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file= '../RF_model/models/rf_model_2.pkl'\n",
    "ref_file = '../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz'\n",
    "in_img = '../../../data/theiaL2A_zip_img/output/2018/2018_GapFilled_Image.tif'\n",
    "out_path = '../../../results/RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962f76d-0228-4f15-acfe-402346e0f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "import csv\n",
    "import optparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from osgeo import gdal, osr\n",
    "from osgeo.gdalconst import *\n",
    "\n",
    "\n",
    "def load_npz(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a .npz file\n",
    "    \"\"\"\n",
    "    with np.load(file_path) as data:\n",
    "        X = data[\"X\"]\n",
    "        y = data[\"y\"]\n",
    "       \n",
    "    return X, y\n",
    "\n",
    "def reshape_data(X, n_channel):\n",
    "    \"\"\"\n",
    "    Reshape data to fit the model\n",
    "    \"\"\"\n",
    "    X = X.reshape(X.shape[0], int(X.shape[1]/n_channel), n_channel)\n",
    "    return X\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def reshape_data(X, nchannels):\n",
    "\t\"\"\"\n",
    "\t\tReshaping (feature format (3 bands): d1.b1 d1.b2 d1.b3 d2.b1 d2.b2 d2.b3 ...)\n",
    "\t\tINPUT:\n",
    "\t\t\t-X: original feature vector ()\n",
    "\t\t\t-feature_strategy: used features (options: SB, NDVI, SB3feat)\n",
    "\t\t\t-nchannels: number of channels\n",
    "\t\tOUTPUT:\n",
    "\t\t\t-new_X: data in the good format for Keras models\n",
    "\t\"\"\"\n",
    "\t\n",
    "\treturn X.reshape(X.shape[0],int(X.shape[1]/nchannels),nchannels) # x: row, y: time, z: band\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def read_minMaxVal(file):\n",
    "\t\n",
    "\twith open(file, 'r') as f:\n",
    "\t\treader = csv.reader(f, delimiter=',')\n",
    "\t\tmin_per = next(reader)\n",
    "\t\tmax_per = next(reader)\n",
    "\tmin_per = [float(k) for k in min_per]\n",
    "\tmin_per = np.array(min_per)\n",
    "\tmax_per = [float(k) for k in max_per]\n",
    "\tmax_per = np.array(max_per)\n",
    "\treturn min_per, max_per\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def save_minMaxVal(file, min_per, max_per):\n",
    "\t\n",
    "\twith open(file, 'w') as f:\n",
    "\t\twriter = csv.writer(f, delimiter=',')\n",
    "\t\twriter.writerow(min_per)\n",
    "\t\twriter.writerow(max_per)\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def computingMinMax(X, per=2):\n",
    "\tmin_per = np.percentile(X, per, axis=(0,1))\n",
    "\tmax_per = np.percentile(X, 100-per, axis=(0,1))\n",
    "\treturn min_per, max_per\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def normalizingData(X, min_per, max_per):\n",
    "\treturn (X-min_per)/(max_per-min_per)\n",
    "\n",
    "def class_mapping(y_label):\n",
    "\t\"\"\"\n",
    "\t\"\"\"\n",
    "\tunique_class = np.unique(y_label)\n",
    "\tnclass = len(unique_class)\n",
    "\tmax_ylabel = np.unique(y_label)[-1]+1 #-- +1 to take into account the case where y=0\n",
    "\t\n",
    "\tclass_map = [0]*max_ylabel\n",
    "\trevert_class_map = unique_class.tolist()\n",
    "\t#-- Insert in class_map values from 1 to c, with c the number of classes\n",
    "\tn = nclass\n",
    "\twhile n>0:\n",
    "\t\tinsert_val = revert_class_map[n-1]\n",
    "\t\tclass_map[insert_val] = n\n",
    "\t\tn = n-1\t\n",
    "\treturn class_map, revert_class_map\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def read_class_map(file):\n",
    "\t\n",
    "\twith open(file, 'r') as f:\n",
    "\t\treader = csv.reader(f, delimiter=',')\n",
    "\t\tclass_map = next(reader)\n",
    "\t\trevert_class_map = next(reader)\n",
    "\tclass_map = [int(k) for k in class_map]\n",
    "\trevert_class_map = [int(k) for k in revert_class_map]\n",
    "\treturn class_map, revert_class_map\n",
    "\n",
    "\n",
    "def save_class_map(file, class_map, revert_class_map):\n",
    "\t\n",
    "\twith open(file, 'w') as f:\n",
    "\t\twriter = csv.writer(f, delimiter=',')\n",
    "\t\twriter.writerow(class_map)\n",
    "\t\twriter.writerow(revert_class_map)\n",
    "\n",
    "nchannels = 10\n",
    "\n",
    "# out_path = options.output\n",
    "# model_file = options.model\n",
    "# in_img = options.in_img # -i 54HWE_img.tif\n",
    "# ref_file = options.ref_file #-t 54HWE_train.sqlite\n",
    "\n",
    "model_name = model_file.split('/')[-1]\n",
    "model_name = model_name.split('.')[0]\n",
    "\n",
    "image_name = in_img.split('/')\n",
    "image_name = image_name[-1].split('_')[0]\n",
    "\n",
    "# out_map = out_path + '/' + image_name + '_' + model_name + '_map' + '.tif'\n",
    "# print(\"out_map: \", out_map)\n",
    "# if os.path.exists(out_map):\n",
    "# \tprint(\"out_map \",out_map,\"already exists => exit\")\n",
    "# \tsys.exit(\"\\n*** not overwriting out_map ***\\n\")\n",
    "\n",
    "# out_confmap = out_path + '/' + image_name + '_' + model_name + '_proba' + '.tif'\n",
    "\n",
    "\n",
    "model = joblib.load(model_file)\n",
    "\n",
    "# flag_del = False #-- deleting the training data\n",
    "# class_map_file = '.'.join(ref_file.split('.')[0:-1])\n",
    "# class_map_file = class_map_file + '_classMap.txt'\n",
    "# print(\"class_map_file: \", class_map_file)\n",
    "# if not os.path.exists(class_map_file): \n",
    "# \tX_train, y_train = load_npz(ref_file)\n",
    "# \tclass_map, revert_class_map = class_mapping(y_train)\n",
    "# \tsave_class_map(class_map_file, class_map, revert_class_map)\n",
    "# \tflag_del = True\n",
    "# else:\n",
    "# \tclass_map, revert_class_map = read_class_map(class_map_file)\n",
    "\n",
    "# print(\"class_map: \", class_map)\n",
    "# print(\"revert_class_map: \", revert_class_map)\n",
    "\n",
    "# if flag_del:\n",
    "# \tdel X_train\n",
    "# \tdel y_train\n",
    "\n",
    "# #get image info about gps coordinates for origin plus size pixels\n",
    "image = gdal.Open(in_img, gdal.GA_ReadOnly) #, NUM_THREADS=8\n",
    "geotransform = image.GetGeoTransform()\n",
    "originX = geotransform[0]\n",
    "originY = geotransform[3]\n",
    "spacingX = geotransform[1]\n",
    "spacingY = geotransform[5]\n",
    "r, c = image.RasterYSize, image.RasterXSize\n",
    "out_raster_SRS = osr.SpatialReference()\n",
    "out_raster_SRS.ImportFromWkt(image.GetProjectionRef())\n",
    "\n",
    "# print(\"r=\", r, \" -- c=\", c)\n",
    "# print(\"originX: \", originX)\n",
    "# print(\"originY: \", originY)\n",
    "# print(\"spacingX: \", spacingX)\n",
    "# print(\"spacingY: \", spacingY)\n",
    "# print(\"geotransform: \", geotransform)\n",
    "\n",
    "#-- Set up the characteristics of the output image\n",
    "# driver = gdal.GetDriverByName('GTiff')\n",
    "# out_map_raster = driver.Create(out_map, c, r, 1, gdal.GDT_Byte)\n",
    "# out_map_raster.SetGeoTransform([originX, spacingX, 0, originY, 0, spacingY])\n",
    "# out_map_raster.SetProjection(out_raster_SRS.ExportToWkt())\n",
    "# out_map_band = out_map_raster.GetRasterBand(1)\n",
    "\n",
    "# out_confmap_raster = driver.Create(out_confmap, c, r, 1, gdal.GDT_Float32)\n",
    "# out_confmap_raster.SetGeoTransform([originX, spacingX, 0, originY, 0, spacingY])\n",
    "# out_confmap_raster.SetProjection(out_raster_SRS.ExportToWkt())\n",
    "# out_confmap_band = out_confmap_raster.GetRasterBand(1)\n",
    "\n",
    "\n",
    "#convert gps corners into image (x,y)\n",
    "# def gps_2_image_xy(x,y):\n",
    "# \treturn (x-originX)/spacingX,(y-originY)/spacingY\n",
    "# def gps_2_image_p(point):\n",
    "# \treturn gps_2_image_xy(point[0],point[1])\n",
    "\n",
    "size_areaX = 256\n",
    "size_areaY = 256\n",
    "x_vec = list(range(int(c/size_areaX)))\n",
    "x_vec = [x*size_areaX for x in x_vec]\n",
    "y_vec = list(range(int(r/size_areaY)))\n",
    "y_vec = [y*size_areaY for y in y_vec]\n",
    "x_vec.append(c)\n",
    "y_vec.append(r)\n",
    "\n",
    "proba_dist = []\n",
    "for x in range(len(x_vec)-1):\n",
    "\tfor y in range(len(y_vec)-1):\n",
    "\t\n",
    "\t\txy_top_left = (x_vec[x],y_vec[y])\n",
    "\t\txy_bottom_right = (x_vec[x+1],y_vec[y+1])\n",
    "\t\t\n",
    "\t\tprint('top_left=',xy_top_left,' to bottom_right=',xy_bottom_right)\n",
    "\n",
    "\t\t#now loading associated data\n",
    "\t\txoff = xy_top_left[0]\n",
    "\t\tyoff = xy_top_left[1]\n",
    "\t\txsize = xy_bottom_right[0]-xy_top_left[0]\n",
    "\t\tysize = xy_bottom_right[1]-xy_top_left[1]\n",
    "\t\tstart_time = time.time()\n",
    "\t\tX_test = image.ReadAsArray(xoff=xoff, yoff=yoff, xsize=xsize, ysize=ysize) #, gdal.GDT_Float32\n",
    "\t\tprint(\"Reading: \", time.time()-start_time)\n",
    "        \n",
    "        #-- reshape the cube in a column vector\n",
    "\t\tX_test = X_test.transpose((1,2,0))\n",
    "\t\tsX = X_test.shape[0]\n",
    "\t\tsY = X_test.shape[1]\n",
    "\t\tX_test = X_test.reshape(X_test.shape[0]*X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "\t\tstart_time = time.time()\n",
    "\t\tp_img = model.predict_proba(X_test)\n",
    "    # proba_dist.append(p_img)\n",
    "\t\tproba_dist.append(p_img)\n",
    "print('done')\n",
    "# \t\t# y_test = p_img.argmax(axis=1)\n",
    "# \t\ty_prob = p_img.max(axis=1)\n",
    "\n",
    "#         confpred_array = y_prob.reshape(sX,sY)\n",
    "# \t\t# y_test = [revert_class_map[k] for k in y_test]\n",
    "# \t\t# y_test = np.array(y_test, dtype=np.uint8)\n",
    "# \t\t# pred_array = y_test.reshape(sX,sY)\n",
    "\t\t\n",
    "# \t\tstart_time = time.time()\n",
    "# \t\t# out_map_band.WriteArray(pred_array, xoff=xoff, yoff=yoff)\n",
    "#         out_confmap_band.WriteArray(confpred_array, xoff=xoff, yoff=yoff)\n",
    "# \t\tprint(\"Writing array: \", time.time()-start_time)\n",
    "\n",
    "# \t\tstart_time = time.time()\n",
    "# \t\tout_map_band.FlushCache()\n",
    "#         out_confmap_band.FlushCache()\n",
    "# \t\tprint(\"Writing disk: \", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac98618d-be33-42c5-b15a-73ff90c552f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "npx = '../../../results/RF/simliarity_measure/2018_case_2.npy'\n",
    "data = np.load(npx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ced256fb-4c0c-4666-8748-bfa4ee6b7928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87177979 0.87177979 0.87177979 ... 0.87177979 0.87177979 0.87177979]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(p_img - hy, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b84a79-828e-458e-b584-7fbd5ccef57c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
