{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82866ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "from utils import load_npz\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4bae7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df0354f6-6fab-4408-9bf8-049de825ae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed value:  9\n",
      "Read set ids completed: 0.0035910606384277344 second\n"
     ]
    }
   ],
   "source": [
    "def generate_ids():\n",
    "    \"\"\"\n",
    "    Descr: \n",
    "        Aim: To write and returns the partition (Train, Validation and Test) ids \n",
    "        with respect to the grid split index (range 1 - 100)\n",
    "        \n",
    "        - A random seed value is set within a random intger 1-10,\n",
    "        - the set is spltted into 80:10:10,\n",
    "        - save into a text file with the seed value used\n",
    "    \"\"\"\n",
    "    # set a random seed value within the range 1 -10 \n",
    "    start_time = time.time()\n",
    "    seed_value = np.random.randint(0,10)\n",
    "    np.random.seed(seed_value)\n",
    "    # # block id range 1 - 100 (splitted grid)\n",
    "    block_range = np.arange(1, 101)\n",
    "\n",
    "    # Train, Validation and Test\n",
    "    random.shuffle(block_range)\n",
    "    train_id = block_range[:80] # 80%\n",
    "    val_id = block_range[80:90] # 10%\n",
    "    test_id = block_range[90:] # 10%\n",
    "    print(\"Seed value: \", seed_value)\n",
    "    \n",
    "    if not os.path.exists(\"train_val_eval_seed_\" + str(seed_value)+\".txt\"):\n",
    "        with open(\"train_val_eval_seed_\" + str(seed_value)+\".txt\", \"w\") as f:\n",
    "            f.write(\"Training: \" + str(list(train_id)) + \"\\n\")\n",
    "            f.write(\"Validation: \" + str(list(val_id)) + \"\\n\")\n",
    "            f.write(\"Testing: \" + str(list(test_id)) + \"\\n\")\n",
    "            f.close()\n",
    "    print('Read set ids completed: %s second' % (time.time() - start_time))\n",
    "generate_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276ef82d-965b-4daa-bb1d-29d4d017d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ids(seed_value):\n",
    "    \"\"\"\n",
    "    Read ids from file\n",
    "    \"\"\"\n",
    "    assert seed_value >= 0 and seed_value <= 10\n",
    "    \n",
    "    with open(\"train_val_eval_seed_\" + str(seed_value)+\".txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        Train_ids = eval(lines[0].split(\":\")[1])\n",
    "        Val_ids = eval(lines[1].split(\":\")[1])\n",
    "        test_ids = eval(lines[2].split(\":\")[1])\n",
    "    return Train_ids, Val_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d45cfe-a8f6-4922-9964-ce63bed9c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(source_sits, target_sits, case):\n",
    "    \"\"\"\n",
    "    Descr: Compute mean and std for each channel\n",
    "    Input: both SITS dataset(.npz) paths\n",
    "            Case[1 - 3]:\n",
    "            1 - concatenate both dataset, while 2 & 3 rep source and target respectively\n",
    "    The data(from N,LxD) is reshaped into (N,D,L);\n",
    "        where N - pixel, D - Bands (10), L - Time (33)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # case = 1: both, case = 2: target, case = 3: target\n",
    "    if case == 1:\n",
    "        sits = [source_sits, target_sits]\n",
    "    elif case == 2:\n",
    "        sits = source_sits\n",
    "    elif case == 3:\n",
    "        sits = target_sits\n",
    "    else:\n",
    "        print('Select case between 1-3')\n",
    "        return None\n",
    "    \n",
    "    # if sits is a list, then it's a list of paths\n",
    "    if isinstance(sits, list):\n",
    "        # load data\n",
    "        X_source = np.load(sits[0])['X']\n",
    "        X_target = np.load(sits[1])['X']\n",
    "        # concatenate the data\n",
    "        X = np.concatenate((X_source, X_target), axis=0)\n",
    "    # if sits is a string, then it's a path\n",
    "    else: \n",
    "        with np.load(sits) as data:\n",
    "            X = data['X']\n",
    "\n",
    "    X = X.reshape(X.shape[0], n_channel, int(X.shape[1]/n_channel))\n",
    "    # compute mean and std\n",
    "    X_mean = np.mean(X, axis=(0,2))\n",
    "    X_std = np.std(X, axis=(0,2))\n",
    "    print('mean shape: ', X_mean.shape)\n",
    "    print('std shape: ', X_std.shape)\n",
    "    # save X_mean and X_std sepearately for sits as txt file\n",
    "    np.savetxt(os.path.join('mean_'+ str(case) +'.txt'), X_mean)\n",
    "    np.savetxt(os.path.join('std_'+ str(case) +'.txt'), X_std)\n",
    "\n",
    "for i in [1,2,3]:\n",
    "    start_time = time.time()\n",
    "    source_path = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "    target_path = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "    compute_mean_std(source_path, target_path, i)\n",
    "    print(\"run time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2855aae-a3dc-4f32-9632-0998ac02417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SITSData(data.Dataset):\n",
    "    def __init__(self, case_: int,source_path, target_path, seed, partition='train', transform=None):\n",
    "        self.case_ = case_\n",
    "        self.source_path = source_path\n",
    "        self.target_path = target_path\n",
    "        self.seed = seed\n",
    "        self.transform = transform\n",
    "        \n",
    "        # get partition ids using the read_id() func\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.train_ids, self.val_ids, self.test_ids = read_ids(self.seed)\n",
    "        print(\"read ids completed: %s second\" % (time.time() - start_time))\n",
    "\n",
    "        # select partition\n",
    "        if partition == 'train':\n",
    "            self.ids = self.train_ids\n",
    "        elif partition == 'val':\n",
    "            self.ids = self.val_ids\n",
    "        elif partition == 'test':\n",
    "            self.ids = self.test_ids\n",
    "        else:\n",
    "            raise ValueError('Invalid partition: {}'.format(partition))\n",
    "\n",
    "        # sits = either source_path or target or both based on the case (1,2,3)\n",
    "        # case = 1: both, case = 2: target, case = 3: target\n",
    "        if self.case_ == 1:\n",
    "            sits = [self.source_path, self.target_path]\n",
    "        elif self.case_ == 2:\n",
    "            sits = self.source_path\n",
    "        elif self.case_ == 3:\n",
    "            sits = self.target_path\n",
    "        else:\n",
    "            print('Wrong case!')\n",
    "\n",
    "        if isinstance(sits, list):\n",
    "            self.sits = sits\n",
    "            print('reading files....')\n",
    "            X_source, y_source, block_ids_source = load_npz(self.sits[0])\n",
    "            X_target, y_target, block_ids_target = load_npz(self.sits[1])\n",
    "            \n",
    "            # concatenate the data\n",
    "            start_time = time.time()\n",
    "            data_source = np.concatenate((X_source, y_source[:, None], block_ids_source[:, None]), axis=1)\n",
    "            data_target = np.concatenate((X_target, y_target[:, None], block_ids_target[:, None]), axis=1)\n",
    "            print(\"Concatenating completed: %s seconds\" % (time.time() - start_time))\n",
    "            \n",
    "            # filter by block_id\n",
    "            start_time = time.time()\n",
    "            data_source = data_source[np.isin(data_source[:, -1], self.ids)]\n",
    "            data_target = data_target[np.isin(data_target[:, -1], self.ids)]\n",
    "            print(\"filtering ids completed: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "            self.X_ = np.concatenate((data_source[:, :-2], data_target[:, :-2]), axis=0)\n",
    "            self.y_ = np.concatenate((data_source[:, -2], data_target[:, -2]), axis=0)\n",
    "            \n",
    "            del X_source\n",
    "            del y_source\n",
    "            del block_ids_source\n",
    "            del data_source\n",
    "            del data_target\n",
    "        else:\n",
    "            self.sits = sits\n",
    "            start_time = time.time()\n",
    "            print('reading files....')\n",
    "            X, y, block_ids = load_npz(self.sits)\n",
    "            print(\"load npz: %s seconds\" % (time.time() - start_time))\n",
    "            print(X.dtype)\n",
    "            \n",
    "            # concatenate the data\n",
    "            start_time = time.time()\n",
    "            data_ = np.concatenate((X, y[:, None], block_ids[:, None]), axis=1)\n",
    "            print(\"Concatenating completed: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "            # filter by block_id\n",
    "            start_time = time.time()\n",
    "            data_ = data_[np.isin(data_[:, -1], self.ids)]\n",
    "            print(\"filtering ids completed: %s seconds\" % (time.time() - start_time))\n",
    "            \n",
    "            self.X_ = data_[:, :-2]\n",
    "            self.y_ = data_[:, -2]\n",
    "            print(\"%s dataset shape: \" % partition,self.X_.shape)\n",
    "            \n",
    "            del X\n",
    "            del y\n",
    "            del block_ids\n",
    "            del data_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = time.time()\n",
    "        self.X = self.X_[idx]\n",
    "        self.y = self.y_[idx]\n",
    "        print(\"getting data: %s seconds\" % ((time.time() - start_time)*100))\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.X = np.array(self.X).astype('float32')\n",
    "        self.y = np.array(self.y).astype('float32')\n",
    "        print(\"conversion: %s seconds\" % ((time.time() - start_time)*100))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.X = self.X.reshape(int(self.X.shape[0]/n_channel), n_channel)\n",
    "        print(\"reshape data: %s seconds\" % ((time.time() - start_time)*100))\n",
    "        \n",
    "\n",
    "        # transform\n",
    "        start_time = time.time()\n",
    "        if self.transform:\n",
    "            self.X = self.transform(self.X)\n",
    "        print(\"transform data: %s seconds\" % ((time.time() - start_time)*100))\n",
    "        print(self.X.shape)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        torch_x = torch.from_numpy(self.X)\n",
    "        torch_y = torch.from_numpy(self.y)\n",
    "        print(\"tensor: %s seconds\" % ((time.time() - start_time)*100))\n",
    "        \n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8190c2ef-d5ca-4ace-8b0b-34c6d4dc3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class standardize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return (sample - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85439710-1994-4049-ad54-04d0f385fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset........\n",
      "read ids completed: 0.0010828971862792969 second\n",
      "reading files....\n",
      "load npz: 68.20331120491028 seconds\n",
      "uint16\n",
      "Concatenating completed: 5.404128313064575 seconds\n",
      "filtering ids completed: 16.801015377044678 seconds\n",
      "train dataset shape:  (10784283, 330)\n",
      "total running time: 90.4606704711914\n",
      "Validation dataset.........\n",
      "read ids completed: 0.0011830329895019531 second\n",
      "reading files....\n",
      "load npz: 68.02338194847107 seconds\n",
      "uint16\n",
      "Concatenating completed: 4.7882819175720215 seconds\n",
      "filtering ids completed: 2.1778595447540283 seconds\n",
      "val dataset shape:  (1606360, 330)\n",
      "total running time: 75.014643907547\n",
      "Test dataset........\n",
      "read ids completed: 0.001016378402709961 second\n",
      "reading files....\n",
      "load npz: 69.27445673942566 seconds\n",
      "uint16\n",
      "Concatenating completed: 5.248713731765747 seconds\n",
      "filtering ids completed: 2.356628894805908 seconds\n",
      "test dataset shape:  (1701000, 330)\n",
      "total running time: 76.92407989501953\n"
     ]
    }
   ],
   "source": [
    "# testing for a single domain (case 2 or 3)\n",
    "case = 2\n",
    "#read mean and std files\n",
    "mean = np.loadtxt('mean_'+str(case)+'.txt')\n",
    "std = np.loadtxt('std_'+str(case)+'.txt')\n",
    "seed = 0\n",
    "transform = transforms.Compose([standardize(mean, std)])\n",
    "\n",
    "# paths\n",
    "source_path = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "target_path = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "\n",
    "# start_time = time.time()\n",
    "print('train dataset........')\n",
    "start_time = time.time()\n",
    "train_dataset = SITSData(case, source_path, target_path, seed, partition='train', transform=transform)\n",
    "print('total running time: %s' % (time.time() - start_time))\n",
    "print('Validation dataset.........')\n",
    "start_time = time.time()\n",
    "val_dataset = SITSData(case, source_path, target_path, seed, partition='val', transform=transform)\n",
    "print('total running time: %s' % (time.time() - start_time))\n",
    "print('Test dataset........')\n",
    "start_time = time.time()\n",
    "test_dataset = SITSData(case, source_path, target_path, seed, partition='test', transform=transform)\n",
    "print('total running time: %s' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4906a91-60ba-46c3-83e0-ffc84ef3dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5194dd4b-2894-4e8d-a7f1-01944eee3c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data: 0.0024318695068359375 seconds\n",
      "conversion: 0.007724761962890625 seconds\n",
      "reshape data: 0.0017881393432617188 seconds\n",
      "transform data: 0.007271766662597656 seconds\n",
      "(33, 10)\n",
      "tensor: 0.0030517578125 seconds\n",
      "Train dataLoader time:  0.004945516586303711\n",
      "getting data: 0.008893013000488281 seconds\n",
      "conversion: 0.013375282287597656 seconds\n",
      "reshape data: 0.0018835067749023438 seconds\n",
      "transform data: 0.011587142944335938 seconds\n",
      "(33, 10)\n",
      "tensor: 0.01938343048095703 seconds\n",
      "getting data: 0.00133514404296875 seconds\n",
      "conversion: 0.0034093856811523438 seconds\n",
      "reshape data: 0.0010728836059570312 seconds\n",
      "transform data: 0.005078315734863281 seconds\n",
      "(33, 10)\n",
      "tensor: 0.00476837158203125 seconds\n",
      "val dataLoader time:  0.45595383644104004\n",
      "getting data: 0.009322166442871094 seconds\n",
      "conversion: 0.016999244689941406 seconds\n",
      "reshape data: 0.002574920654296875 seconds\n",
      "transform data: 0.015234947204589844 seconds\n",
      "(33, 10)\n",
      "tensor: 0.020742416381835938 seconds\n",
      "getting data: 0.002765655517578125 seconds\n",
      "conversion: 0.004887580871582031 seconds\n",
      "reshape data: 0.0019550323486328125 seconds\n",
      "transform data: 0.007319450378417969 seconds\n",
      "(33, 10)\n",
      "tensor: 0.004458427429199219 seconds\n",
      "getting data: 0.0025033950805664062 seconds\n",
      "conversion: 0.004863739013671875 seconds\n",
      "reshape data: 0.0019550323486328125 seconds\n",
      "transform data: 0.007343292236328125 seconds\n",
      "(33, 10)\n",
      "tensor: 0.004124641418457031 seconds\n",
      "test dataLoader time:  0.48351287841796875\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "x_train,y_train= next(iter(train_loader))\n",
    "print('Train dataLoader time: ', time.time() - start_time)\n",
    "start_time = time.time()\n",
    "x_val, y_val = next(iter(val_loader))\n",
    "print('val dataLoader time: ', time.time() - start_time)\n",
    "start_time = time.time()\n",
    "x_test, y_test = next(iter(test_loader))\n",
    "print('test dataLoader time: ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e679fc4-c3fc-4339-8eff-945372a81f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d78f09d-aaec-46a4-87ff-531043db3913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data: 0.05061626434326172 seconds\n",
      "conversion: 0.013589859008789062 seconds\n",
      "reshape data: 0.002384185791015625 seconds\n",
      "transform data: 0.01819133758544922 seconds\n",
      "(33, 10)\n",
      "tensor: 0.021076202392578125 seconds\n",
      "getting data: 0.04856586456298828 seconds\n",
      "conversion: 0.004410743713378906 seconds\n",
      "reshape data: 0.0016689300537109375 seconds\n",
      "transform data: 0.005745887756347656 seconds\n",
      "(33, 10)\n",
      "tensor: 0.005125999450683594 seconds\n",
      "getting data: 0.056672096252441406 seconds\n",
      "conversion: 0.022411346435546875 seconds\n",
      "reshape data: 0.0032663345336914062 seconds\n",
      "transform data: 0.023484230041503906 seconds\n",
      "(33, 10)\n",
      "tensor: 0.02033710479736328 seconds\n",
      "getting data: 0.0537872314453125 seconds\n",
      "conversion: 0.0034332275390625 seconds\n",
      "reshape data: 0.0010728836059570312 seconds\n",
      "transform data: 0.004410743713378906 seconds\n",
      "(33, 10)\n",
      "tensor: 0.0025987625122070312 seconds\n",
      "getting data: 0.04596710205078125 seconds\n",
      "conversion: 0.0026702880859375 seconds\n",
      "reshape data: 0.000858306884765625 seconds\n",
      "transform data: 0.003719329833984375 seconds\n",
      "(33, 10)\n",
      "tensor: 0.0021457672119140625 seconds\n",
      "getting data: 0.04756450653076172 seconds\n",
      "conversion: 0.016546249389648438 seconds\n",
      "reshape data: 0.0036954879760742188 seconds\n",
      "transform data: 0.021505355834960938 seconds\n",
      "(33, 10)\n",
      "tensor: 0.018548965454101562 seconds\n",
      "getting data: 0.04355907440185547 seconds\n",
      "conversion: 0.00400543212890625 seconds\n",
      "reshape data: 0.0012636184692382812 seconds\n",
      "transform data: 0.0051021575927734375 seconds\n",
      "(33, 10)\n",
      "tensor: 0.0028133392333984375 seconds\n",
      "getting data: 0.050568580627441406 seconds\n",
      "conversion: 0.0031709671020507812 seconds\n",
      "reshape data: 0.0010013580322265625 seconds\n",
      "transform data: 0.0037908554077148438 seconds\n",
      "(33, 10)\n",
      "tensor: 0.0024080276489257812 seconds\n",
      "getting data: 0.052928924560546875 seconds\n",
      "conversion: 0.01277923583984375 seconds\n",
      "reshape data: 0.002288818359375 seconds\n",
      "transform data: 0.01583099365234375 seconds\n",
      "(33, 10)\n",
      "tensor: 0.0202178955078125 seconds\n",
      "getting data: 0.049757957458496094 seconds\n",
      "conversion: 0.003910064697265625 seconds\n",
      "reshape data: 0.0012159347534179688 seconds\n",
      "transform data: 0.006771087646484375 seconds\n",
      "(33, 10)\n",
      "tensor: 0.003910064697265625 seconds\n",
      "getting data: 0.053119659423828125 seconds\n",
      "conversion: 0.01220703125 seconds\n",
      "reshape data: 0.004029273986816406 seconds\n",
      "transform data: 0.021219253540039062 seconds\n",
      "(33, 10)\n",
      "tensor: 0.026082992553710938 seconds\n",
      "getting data: 0.046324729919433594 seconds\n",
      "conversion: 0.00476837158203125 seconds\n",
      "reshape data: 0.0014781951904296875 seconds\n",
      "transform data: 0.0049114227294921875 seconds\n",
      "(33, 10)\n",
      "tensor: 0.00286102294921875 seconds\n",
      "getting data: 0.04799365997314453 seconds\n",
      "conversion: 0.009679794311523438 seconds\n",
      "reshape data: 0.0011205673217773438 seconds\n",
      "transform data: 0.0058650970458984375 seconds\n",
      "(33, 10)\n",
      "tensor: 0.00362396240234375 seconds\n",
      "getting data: 0.05061626434326172 seconds\n",
      "conversion: 0.013208389282226562 seconds\n",
      "reshape data: 0.0027894973754882812 seconds\n",
      "transform data: 0.018715858459472656 seconds\n",
      "(33, 10)\n",
      "tensor: 0.02086162567138672 seconds\n",
      "getting data: 0.045037269592285156 seconds\n",
      "conversion: 0.0036001205444335938 seconds\n",
      "reshape data: 0.0019311904907226562 seconds\n",
      "transform data: 0.005745887756347656 seconds\n",
      "(33, 10)\n",
      "tensor: 0.007081031799316406 seconds\n",
      "getting data: 0.05135536193847656 seconds\n",
      "conversion: 0.014853477478027344 seconds\n",
      "reshape data: 0.002956390380859375 seconds\n",
      "transform data: 0.016188621520996094 seconds\n",
      "(33, 10)\n",
      "tensor: 0.01823902130126953 seconds\n",
      "getting data: 0.05023479461669922 seconds\n",
      "conversion: 0.004410743713378906 seconds\n",
      "reshape data: 0.00133514404296875 seconds\n",
      "transform data: 0.0058650970458984375 seconds\n",
      "(33, 10)\n",
      "tensor: 0.0036001205444335938 seconds\n",
      "getting data: 0.05640983581542969 seconds\n",
      "conversion: 0.016045570373535156 seconds\n",
      "reshape data: 0.0021696090698242188 seconds\n",
      "transform data: 0.013637542724609375 seconds\n",
      "(33, 10)\n",
      "tensor: 0.01709461212158203 seconds\n",
      "getting data: 0.04265308380126953 seconds\n",
      "conversion: 0.0031948089599609375 seconds\n",
      "reshape data: 0.0009298324584960938 seconds\n",
      "transform data: 0.0041961669921875 seconds\n",
      "(33, 10)\n",
      "tensor: 0.0045299530029296875 seconds\n",
      "getting data: 0.05125999450683594 seconds\n",
      "conversion: 0.00457763671875 seconds\n",
      "reshape data: 0.00133514404296875 seconds\n",
      "transform data: 0.005173683166503906 seconds\n",
      "(33, 10)\n",
      "tensor: 0.0035047531127929688 seconds\n",
      "468 ms ± 51.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95ee8141-0add-41da-85de-16a4522b4174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data: 0.007748603820800781 seconds\n",
      "conversion: 0.008797645568847656 seconds\n",
      "reshape data: 0.002002716064453125 seconds\n",
      "transform data: 0.01201629638671875 seconds\n",
      "(33, 10)\n",
      "tensor: 0.009441375732421875 seconds\n",
      "Train dataLoader time:  2.2028355598449707\n",
      "getting data: 0.007271766662597656 seconds\n",
      "conversion: 0.012230873107910156 seconds\n",
      "reshape data: 0.00247955322265625 seconds\n",
      "transform data: 0.010013580322265625 seconds\n",
      "(33, 10)\n",
      "tensor: 0.017881393432617188 seconds\n",
      "getting data: 0.0013589859008789062 seconds\n",
      "conversion: 0.0036478042602539062 seconds\n",
      "reshape data: 0.0012636184692382812 seconds\n",
      "transform data: 0.005316734313964844 seconds\n",
      "(33, 10)\n",
      "tensor: 0.00362396240234375 seconds\n",
      "getting data: 0.0024557113647460938 seconds\n",
      "conversion: 0.014710426330566406 seconds\n",
      "reshape data: 0.0020742416381835938 seconds\n",
      "transform data: 0.005984306335449219 seconds\n",
      "(33, 10)\n",
      "tensor: 0.004363059997558594 seconds\n",
      "val dataLoader time:  0.5583875179290771\n",
      "getting data: 0.008893013000488281 seconds\n",
      "conversion: 0.013780593872070312 seconds\n",
      "reshape data: 0.0033617019653320312 seconds\n",
      "transform data: 0.014328956604003906 seconds\n",
      "(33, 10)\n",
      "tensor: 0.020384788513183594 seconds\n",
      "getting data: 0.0020503997802734375 seconds\n",
      "conversion: 0.004887580871582031 seconds\n",
      "reshape data: 0.0023365020751953125 seconds\n",
      "transform data: 0.0074863433837890625 seconds\n",
      "(33, 10)\n",
      "tensor: 0.004220008850097656 seconds\n",
      "test dataLoader time:  0.5204024314880371\n",
      "getting data: 0.005984306335449219 seconds\n",
      "conversion: 0.08349418640136719 seconds\n",
      "reshape data: 0.0025510787963867188 seconds\n",
      "transform data: 0.010728836059570312 seconds\n",
      "(33, 10)\n",
      "tensor: 0.008153915405273438 seconds\n",
      "Train dataLoader time:  2.302988052368164\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "x_train,y_train= next(iter(train_loader))\n",
    "print('Train dataLoader time: ', time.time() - start_time)\n",
    "start_time = time.time()\n",
    "x_val, y_val = next(iter(val_loader))\n",
    "print('val dataLoader time: ', time.time() - start_time)\n",
    "start_time = time.time()\n",
    "x_test, y_test = next(iter(test_loader))\n",
    "print('test dataLoader time: ', time.time() - start_time)\n",
    "start_time = time.time()\n",
    "x_train,y_train= next(iter(train_loader))\n",
    "print('Train dataLoader time: ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38aa7f74-7bee-4096-8878-e853bb9b31ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9,  9,  3,  5, 15,  6, 23, 16,  9,  9,  9,  9, 18,  9,  6,  6, 10,  8,\n",
       "         2,  9,  9,  9,  8, 16,  9,  9, 23,  9,  8,  9,  5, 13, 10,  9,  2,  9,\n",
       "         9,  9,  6,  2,  9,  5,  8,  5,  9,  9,  9,  9,  9,  2,  8, 23,  2,  9,\n",
       "         5,  9, 23,  5,  9,  9,  2,  8,  9,  9,  5,  9,  9, 23, 10,  6,  9,  9,\n",
       "         9,  8,  9,  9,  3, 10,  9,  8,  9,  5,  5,  6,  6, 10,  6,  2,  9,  6,\n",
       "         8,  6,  9,  9,  9,  7,  9,  6,  2,  9,  9,  9,  5,  6,  9, 16,  9,  8,\n",
       "         9,  9,  9,  9,  9,  9,  6,  7,  9,  9,  5,  7,  9,  9,  3, 15,  9, 23,\n",
       "         9,  9], dtype=torch.int16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c3144-71ed-4173-b35e-0a3645f7d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b56b98-6cfc-469b-9ded-ecd8a8cdd8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset........\n",
      "read ids completed: 0.001753091812133789 second\n",
      "reading files....\n",
      "total running time: 134.69096612930298\n",
      "Validation dataset.........\n",
      "read ids completed: 0.002171039581298828 second\n",
      "reading files....\n",
      "total running time: 116.40252137184143\n",
      "Test dataset........\n",
      "read ids completed: 0.0019958019256591797 second\n",
      "reading files....\n",
      "total running time: 117.6309289932251\n"
     ]
    }
   ],
   "source": [
    "# testing for a case 1 (combined both domains)\n",
    "case = 1\n",
    "#read mean and std files\n",
    "mean = np.loadtxt('mean_'+str(case)+'.txt')\n",
    "std = np.loadtxt('std_'+str(case)+'.txt')\n",
    "seed = 0\n",
    "transform = transforms.Compose([standardize(mean, std)])\n",
    "\n",
    "# paths\n",
    "source_path = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "target_path = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "\n",
    "# start_time = time.time()\n",
    "print('train dataset........')\n",
    "start_time = time.time()\n",
    "train_dataset = SITSData(case, source_path, target_path, seed, partition='train', transform=transform)\n",
    "print('total running time: %s' % (time.time() - start_time))\n",
    "print('Validation dataset.........')\n",
    "start_time = time.time()\n",
    "val_dataset = SITSData(case, source_path, target_path, seed, partition='val', transform=transform)\n",
    "print('total running time: %s' % (time.time() - start_time))\n",
    "print('Test dataset........')\n",
    "start_time = time.time()\n",
    "test_dataset = SITSData(case, source_path, target_path, seed, partition='test', transform=transform)\n",
    "print('total running time: %s' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aee6c47-4e00-43f9-b5a9-e5828231e882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataLoader time:  1.5588064193725586\n",
      "val dataLoader time:  0.37462925910949707\n",
      "test dataLoader time:  0.3885047435760498\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "x_train,y_train= next(iter(train_loader))\n",
    "print('Train dataLoader time: ', time.time() - start_time)\n",
    "start_time = time.time()\n",
    "x_val, y_val = next(iter(val_loader))\n",
    "print('val dataLoader time: ', time.time() - start_time)\n",
    "start_time = time.time()\n",
    "x_test, y_test = next(iter(test_loader))\n",
    "print('test dataLoader time: ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e420189-2f15-4093-8e1b-b2e16d3ee2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
