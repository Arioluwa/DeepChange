{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e40c887-a0e1-425e-9a17-5ddaf831bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "# from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from utils import load_npz, read_ids\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e32007-c4ac-4705-af7b-fbf1f48693ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a .npz file\n",
    "    \"\"\"\n",
    "    with np.load(file_path) as data:\n",
    "        X = data[\"X\"]\n",
    "        y = data[\"y\"]\n",
    "        # polygon_ids = data[\"polygon_ids\"]\n",
    "        block_ids = data[\"block_id\"]\n",
    "    return X, y, block_ids#, polygon_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508ec429-bfa4-486e-918d-e376bba22a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 33\n",
    "n_channel = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8ee8c9-1d5d-4ddb-9a4c-ce8236e38d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    m = X.mean(axis=0)\n",
    "    s = X.std(axis=0)\n",
    "    X = (X - m) / s\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7200bc74-efc7-4ce5-a22f-0d8ed3662b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.99216985702515\n"
     ]
    }
   ],
   "source": [
    "source_sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "train_val_eval = \"../RF_model/train_val_eval_rf.txt\"\n",
    "\n",
    "train_ids, test_ids = read_ids(train_val_eval)\n",
    "X_source, y_source, block_ids_source = load_npz(source_sits)\n",
    "_source = np.concatenate((X_source, y_source[:, None], block_ids_source[:, None]), axis=1)\n",
    "_source = _source[np.isin(_source[:, -1], train_ids)]\n",
    "Xtrain_source = _source[:, :-2]\n",
    "ytrain_source = _source[:, -2]\n",
    "# Xtrain = np.concatenate((Xtrain_source, Xtrain_target), axis=0)\n",
    "# ytrain = np.concatenate((ytrain_source, ytrain_target), axis=0)\n",
    "Xtrain = Xtrain_source\n",
    "ytrain = ytrain_source\n",
    "x, y = Xtrain, ytrain\n",
    "x = x.reshape(x.shape[0], L, n_channel)\n",
    "x = x.transpose(0,2,1)\n",
    "startime =time.time()\n",
    "x_ = (standardize(x)).astype('float16')\n",
    "print(time.time() - startime)\n",
    "# x_ = x[idx]\n",
    "# y = y.astype('int8')[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26eec111-16a7-4474-83cf-bbee7ca334ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x.mean(axis=0)).shape\n",
    "# x.std(axis=0).astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fd08a6-8638-41d7-ad39-401df2c8064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80e36058-37f9-4a04-8c3d-60e0e5093537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 319,  319,  319,  386,  454,  522,  437,  394,  310,  318,  326,\n",
       "         468,  523,  617,  579,  538,  469,  421,  373,  356,  790,  538,\n",
       "         525,  512,  458,  416,  375,  534,  488,  341,  366,  399,  399],\n",
       "       [ 468,  468,  468,  583,  698,  814,  669,  596,  452,  471,  490,\n",
       "         716,  790, 1013,  975,  913,  702,  653,  604,  551, 1362,  870,\n",
       "         857,  845,  747,  661,  575,  883,  785,  556,  587,  609,  609],\n",
       "       [ 301,  301,  301,  439,  578,  717,  490,  377,  151,  181,  212,\n",
       "         584,  709, 1088, 1004,  871,  374,  331,  289,  286, 1221,  500,\n",
       "         511,  522,  450,  367,  284,  607,  508,  261,  324,  489,  489],\n",
       "       [ 187,  187,  187,  288,  389,  490,  294,  196,    0,   46,   93,\n",
       "         478,  578, 1027,  874,  687,  332,  382,  433,  294,  980,  438,\n",
       "         466,  495,  441,  359,  277,  503,  428,  158,  191,  305,  305],\n",
       "       [  26,   26,   26,   17,    8,    0,    0,    0,    0,   23,   47,\n",
       "         254,  310,  560,  388,  237,  307,  508,  710,  429,  640,  439,\n",
       "         526,  614,  531,  361,  191,  744,  644,  106,  116,  117,  117],\n",
       "       [  23,   23,   23,   15,    7,    0,    0,    0,    0,    7,   14,\n",
       "         130,  174,  557,  345,  221,  317,  559,  801,  577,  674,  390,\n",
       "         569,  749,  696,  464,  232,  856,  771,   84,  101,  127,  127],\n",
       "       [  14,   14,   14,    9,    4,    0,    0,    0,    0,    6,   12,\n",
       "         149,  205,  498,  275,  111,  118,  117,  116,  168,  427,  263,\n",
       "         249,  236,  182,  141,  101,  227,  170,   14,   29,  119,  119],\n",
       "       [  13,   13,   13,    8,    4,    0,    0,    0,    0,    2,    4,\n",
       "          63,   89,  517,  384,  250,  453,  657,  861,  529,  538,  277,\n",
       "         489,  702,  740,  565,  391,  799,  688,   39,   47,  111,  111],\n",
       "       [  97,   97,   97,   64,   32,    0,    0,    0,    0,   30,   61,\n",
       "          87,   39,  201,  176,  183,  337,  429,  521,  209,  253,  199,\n",
       "         289,  379,  359,  250,  141,  399,  358,   40,   45,   43,   43],\n",
       "       [  71,   71,   71,   47,   23,    0,   18,   27,   45,   59,   74,\n",
       "          73,   43,   90,   79,   82,  204,  270,  336,  115,  144,  154,\n",
       "         166,  178,  155,  119,   84,  199,  185,   61,   59,   24,   24]],\n",
       "      dtype=uint16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15abcb2a-934e-4f66-a1f4-9da76894afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_sits = \"../subset_data/2018_subset.npz\"\n",
    "# target_sits = \"../subset_data/2019_subset.npz\"\n",
    "# train_val_eval = \"train_val_eval_rf.txt\"\n",
    "# case = 1\n",
    "# set_ = \"trainval\"\n",
    "# # loader = data.DataLoader(dataset, batch_size = 10)\n",
    "# transform = None\n",
    "# import time\n",
    "# st = time.time()\n",
    "# dataset = SitsData(case, source_sits, target_sits, train_val_eval, set_, transform)\n",
    "# loader = data.DataLoader(dataset, batch_size = 10)\n",
    "# v, y = next(iter(loader))\n",
    "# print(\"run time: \", st -time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed07ee55-9a88-4f0b-bcfe-f3f51359f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = data.DataLoader(dataset, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcbca819-e08b-4f3c-b0d6-85a25225bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v, y = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c625e9da-8b79-407c-a726-4e3a3ac1d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c980a301-d9bc-42a8-8207-8bfc5598a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SITSData(data.Dataset):\n",
    "    def __init__(self, case: int, source_sits, target_sits, train_val_eval, set_= 'trainval', transform=None):\n",
    "        self.source_sits = source_sits\n",
    "        self.target_sits = target_sits\n",
    "        self.train_val_eval = train_val_eval\n",
    "        self.transform = transform\n",
    "        self.case = case\n",
    "        self.set_ = set_\n",
    "\n",
    "        # read the set ids\n",
    "        start_time = time.time()\n",
    "        self.train_ids, self.test_ids = read_ids(self.train_val_eval)\n",
    "        print(\"read ids %s seconds ---\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "        # case selection\n",
    "        if self.set_ == 'trainval':\n",
    "            ids = self.train_ids\n",
    "        elif self.set_ == 'test':\n",
    "            ids = self.test_ids\n",
    "        else:\n",
    "            raise ValueError(\"Please choose a set between trainval and test\")\n",
    "        print(\"case %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        # read the data \n",
    "        start_time = time.time()\n",
    "        X_source, y_source, block_ids_source = load_npz(self.source_sits)\n",
    "        X_target, y_target, block_ids_target = load_npz(self.target_sits)\n",
    "        print(\"load npz %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "        start_time = time.time()\n",
    "        _source = np.concatenate((X_source, y_source[:, None], block_ids_source[:, None]), axis=1)\n",
    "        _target = np.concatenate((X_target, y_target[:, None], block_ids_target[:, None]), axis=1)\n",
    "        print(\"concatenate %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "        start_time = time.time()\n",
    "        _source = _source[np.isin(_source[:, -1], ids)]\n",
    "        _target = _target[np.isin(_target[:, -1], ids)]\n",
    "        print(\"isin %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.Xtrain_source = _source[:, :-2]\n",
    "        self.ytrain_source = _source[:, -2]\n",
    "        print(\"remove ids source %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.Xtrain_target = _target[:, :-2]\n",
    "        self.ytrain_target = _target[:, -2]\n",
    "        print(\"remove ids target %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        start_time = time.time()\n",
    "        if self.case == 1:\n",
    "            self.Xtrain = np.concatenate((self.Xtrain_source, self.Xtrain_target), axis=0)\n",
    "            self.ytrain = np.concatenate((self.ytrain_source, self.ytrain_target), axis=0)\n",
    "        elif self.case == 2:\n",
    "            self.Xtrain = self.Xtrain_source\n",
    "            self.ytrain = self.ytrain_source\n",
    "        elif self.case == 3:\n",
    "            self.Xtrain = self.Xtrain_target\n",
    "            self.ytrain = self.ytrain_target\n",
    "        else:\n",
    "            raise ValueError(\"Please choose a case between 1 and 3\") \n",
    "        print(\"concatenate %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ytrain)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = time.time()\n",
    "        x, y = self.Xtrain, self.ytrain\n",
    "        x = x.reshape(x.shape[0], L, n_channel)\n",
    "        x = x.transpose(0,2,1)\n",
    "        print(\"reshape and transpose %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        x = (standardize(x)).astype('float32')\n",
    "        x_ = x[idx]\n",
    "        y = y.astype('int8')[idx]\n",
    "        print(\"stardardization %s seconds ---\" % (time.time() - start_time))\n",
    "        # start_time = time.time()\n",
    "        # x_ = torch.from_numpy(np.array(x_))\n",
    "        # y = torch.from_numpy(np.array(y))\n",
    "        # print(\"to tenors %s seconds ---\" % (time.time() - start_time))\n",
    "        # return x, y\n",
    "        return torch.from_numpy(np.array(x_)), torch.from_numpy(np.array(y))\n",
    "\n",
    "def standardize(X):\n",
    "    m = X.mean(axis=0)\n",
    "    s = X.std(axis=0)\n",
    "    X = (X - m) / s\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4086c53-dfba-4496-83ce-db2cf0611111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "# load_npz(source_sits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0989d84e-934d-4d12-8f2e-2ab369d62214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read ids 0.0021495819091796875 seconds ---\n",
      "case 2.86102294921875e-06 seconds ---\n",
      "load npz 1.9483068982760112 minutes ---\n",
      "concatenate 0.12075953880945842 minutes ---\n",
      "isin 0.21867297093073526 minutes ---\n",
      "remove ids source 3.0279159545898438e-05 seconds ---\n",
      "remove ids target 1.9788742065429688e-05 seconds ---\n",
      "concatenate 0.06620307763417561 minutes ---\n",
      "run time:  2.354695975780487\n"
     ]
    }
   ],
   "source": [
    "# source_sits = \"../subset_data/2018_subset.npz\"\n",
    "# target_sits = \"../subset_data/2019_subset.npz\"\n",
    "# train_val_eval = \"train_val_eval_rf.txt\"\n",
    "source_sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "target_sits = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "train_val_eval = \"../RF_model/train_val_eval_rf.txt\"\n",
    "case = 1\n",
    "set_ = \"trainval\"\n",
    "transform = None\n",
    "import time\n",
    "start_time = time.time()\n",
    "train_dataset = SITSData(case, source_sits, target_sits, train_val_eval, set_, transform)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size = 1, pin_memory=True)\n",
    "\n",
    "print(\"run time: \", ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9293ac4-3893-4fa6-b16f-a48753263070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape and transpose 2.5033950805664062e-05 seconds ---\n",
      "stardardization 84.97270584106445 seconds ---\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train= next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f9929-58af-472e-87e1-c39bd052ff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape and transpose 4.029273986816406e-05 seconds ---\n",
      "stardardization 82.94411087036133 seconds ---\n",
      "reshape and transpose 3.147125244140625e-05 seconds ---\n",
      "stardardization 82.9876160621643 seconds ---\n",
      "reshape and transpose 4.673004150390625e-05 seconds ---\n"
     ]
    }
   ],
   "source": [
    "for (x,y) in enumerate(train_loader):\n",
    "    y_ry = y\n",
    "    x_hr = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a54688b-cbdf-4b7d-beb7-bdffcd037e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read ids 0.002400636672973633 seconds ---\n",
      "case 1.9073486328125e-06 seconds ---\n",
      "load npz 1.9369140625 minutes ---\n",
      "concatenate 0.12013090848922729 minutes ---\n",
      "isin 0.14379287560780843 minutes ---\n",
      "remove ids source 4.100799560546875e-05 seconds ---\n",
      "remove ids target 5.245208740234375e-06 seconds ---\n",
      "concatenate 0.0441957155863444 minutes ---\n",
      "run time:  134.75104451179504\n"
     ]
    }
   ],
   "source": [
    "source_sits = \"../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz\"\n",
    "target_sits = \"../../../data/theiaL2A_zip_img/output/2019/2019_SITS_data.npz\"\n",
    "train_val_eval = \"../RF_model/train_val_eval_rf.txt\"\n",
    "case = 1\n",
    "set_ = \"test\"\n",
    "transform = None\n",
    "import time\n",
    "st = time.time()\n",
    "dataset_test = SITSData(case, source_sits, target_sits, train_val_eval, set_, transform)\n",
    "test_loader = data.DataLoader(dataset_test, batch_size = 1, pin_memory=True)\n",
    "# x_test,y_test= next(iter(loader))\n",
    "print(\"run time: \", time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "239ec5e1-816a-44ae-922b-d69a9ffdd8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape and transpose 2.7179718017578125e-05 seconds ---\n",
      "stardardization 54.03708863258362 seconds ---\n",
      "to tenors 0.030365467071533203 seconds ---\n",
      "run time:  0.9012147784233093\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "x_test,y_test= next(iter(test_loader))\n",
    "print(\"run time: \", ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ec355-7ff2-4751-a020-f5f5e57a2a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40035ce-2ad3-4cdf-8c7f-ab0f124a7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # useful functions that are needed in the dataset class\n",
    "# class SitsData(data.Dataset):\n",
    "#     def __init__(self, case: int, source_sits, target_sits, train_val_eval, set_= 'trainval', transform=None):\n",
    "#         super(SitsData, self).__init__()\n",
    "#         self.source_sits = source_sits\n",
    "#         self.target_sits = target_sits\n",
    "#         self.train_val_eval = train_val_eval\n",
    "#         self.transform = transform\n",
    "#         self.case = case\n",
    "#         self.set_ = set_\n",
    "        \n",
    "#         self.x, self.y = self._init_dataset()\n",
    "        \n",
    "#     def load_set(self, total_set):\n",
    "#         \"\"\"\n",
    "#             Load a set of data:\n",
    "#             to be used in prepare_data(), it organise dataset according to the set_name based block_ids\n",
    "#             - set_name: train or test (reads from the block_ids using read_ids() utils) \n",
    "#             - total_set: the concatenated set of data for source or target\n",
    "#             - returns X and Y:\n",
    "#                 X is the data, Y is the label\n",
    "#         \"\"\"\n",
    "#         self.train_ids, self.test_ids = read_ids(self.train_val_eval)\n",
    "\n",
    "#         if self.set_ == \"trainval\":\n",
    "#             ids = self.train_ids\n",
    "#         elif self.set_ == \"test\":\n",
    "#             ids = self.test_ids\n",
    "#         else:\n",
    "#             raise ValueError(\"Please choose a set between trainval and test\")\n",
    "\n",
    "#         set = total_set[np.isin(total_set[:, -1], ids)]\n",
    "#         X = set[:, :-2]\n",
    "#         Y = set[:, -2]\n",
    "            \n",
    "#         return X, Y\n",
    "        \n",
    "#     def _init_dataset(self):\n",
    "#         \"\"\"\n",
    "#             Prepare data:\n",
    "#             - load data from npz files(load_npz() in utils): returns X, Y and block_ids\n",
    "#             - concatenate the data from source and target [X, Y, block_ids] == total_set\n",
    "#             - split the data into train and test sets for source and target\n",
    "#             - returns Xtrain and Ytrain base on CASE value\n",
    "#             CASE: \n",
    "#             - 1: Train on Soucre and target, test on source and target\n",
    "#             - 2: Train on Source only, test on Source and target\n",
    "#             - 3: Train on Target only, test on Source and target\n",
    "#         \"\"\"\n",
    "#             # print(\"Preparing data.........\")\n",
    "\n",
    "#         X_s, Y_s, block_ids_s = load_npz(self.source_sits)\n",
    "#         X_t, Y_t, block_ids_t = load_npz(self.target_sits)\n",
    "#         # print(\"Loading npz files done.........\")\n",
    "            \n",
    "\n",
    "#         self.total_set_s = np.concatenate((X_s, Y_s[:, None], block_ids_s[:, None]), axis=1)\n",
    "#         self.total_set_t = np.concatenate((X_t, Y_t[:, None], block_ids_t[:, None]), axis=1)\n",
    "#             # print(\"Concatenating data done.........\")\n",
    "\n",
    "#             # Training set for target and source\n",
    "#         self.Xtrain_s, self.Ytrain_s = self.load_set(self.total_set_s)\n",
    "#         self.Xtrain_t, self.Ytrain_t = self.load_set(self.total_set_t)\n",
    "#             # print(\"Loading train set done.........\")\n",
    "\n",
    "#             # Test set for target and source\n",
    "#         self.Xtest_s, self.Ytest_s = self.load_set(self.total_set_s)\n",
    "#         self.Xtest_t, self.Ytest_t = self.load_set(self.total_set_t)\n",
    "#             # print(\"Loading test set done.........\")\n",
    "\n",
    "#         if self.case == 1:\n",
    "#                 # concatenate training set for target and source\n",
    "#             self.Xtrain = np.concatenate((self.Xtrain_s, self.Xtrain_t), axis=0)\n",
    "#             self.Ytrain = np.concatenate((self.Ytrain_s, self.Ytrain_t), axis=0)\n",
    "\n",
    "#         elif self.case == 2:\n",
    "#                 # Xtrain is the training set for source only\n",
    "#             self.Xtrain = self.Xtrain_s\n",
    "#             self.Ytrain = self.Ytrain_s\n",
    "\n",
    "#         elif self.case == 3:\n",
    "#                 # Xtrain is the training set for target only\n",
    "#             self.Xtrain = self.Xtrain_t\n",
    "#             self.Ytrain = self.Ytrain_t\n",
    "\n",
    "#         else:\n",
    "#             raise ValueError(\"Please choose a case between 1 and 3\")\n",
    "            \n",
    "#         return self.Xtrain, self.Ytrain\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         x_, y_ = self.x[idx], self.y[idx]\n",
    "            \n",
    "#         x = torch.from_numpy(np.array(x_, dtype=int))\n",
    "#         y = torch.from_numpy(np.array(y_, dtype=int))\n",
    "#         return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca095d2-1c7b-4568-a706-98852de197f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file= '../RF_model/models/rf_model_2.pkl'\n",
    "ref_file = '../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data.npz'\n",
    "in_img = '../../../data/theiaL2A_zip_img/output/2018/2018_Image.tif'\n",
    "out_path = '../../../results/RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4962f76d-0228-4f15-acfe-402346e0f51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/etud/e2008987/.conda/envs/python_bkup/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.22 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_map_file:  ../../../data/theiaL2A_zip_img/output/2018/2018_SITS_data_classMap.txt\n",
      "class_map:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 11, 12, 13, 14, 15, 16, 17, 18, 0, 0, 0, 19]\n",
      "revert_class_map:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 23]\n",
      "r= 10980  -- c= 10980\n",
      "originX:  300000.0\n",
      "originY:  4900020.0\n",
      "spacingX:  10.0\n",
      "spacingY:  -10.0\n",
      "geotransform:  (300000.0, 10.0, 0.0, 4900020.0, 0.0, -10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/etud/e2008987/.conda/envs/python_bkup/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.22 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "import csv\n",
    "import optparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from osgeo import gdal, osr\n",
    "from osgeo.gdalconst import *\n",
    "\n",
    "\n",
    "def load_npz(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a .npz file\n",
    "    \"\"\"\n",
    "    with np.load(file_path) as data:\n",
    "        X = data[\"X\"]\n",
    "        y = data[\"y\"]\n",
    "       \n",
    "    return X, y\n",
    "\n",
    "def reshape_data(X, n_channel):\n",
    "    \"\"\"\n",
    "    Reshape data to fit the model\n",
    "    \"\"\"\n",
    "    X = X.reshape(X.shape[0], int(X.shape[1]/n_channel), n_channel)\n",
    "    return X\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def reshape_data(X, nchannels):\n",
    "\t\"\"\"\n",
    "\t\tReshaping (feature format (3 bands): d1.b1 d1.b2 d1.b3 d2.b1 d2.b2 d2.b3 ...)\n",
    "\t\tINPUT:\n",
    "\t\t\t-X: original feature vector ()\n",
    "\t\t\t-feature_strategy: used features (options: SB, NDVI, SB3feat)\n",
    "\t\t\t-nchannels: number of channels\n",
    "\t\tOUTPUT:\n",
    "\t\t\t-new_X: data in the good format for Keras models\n",
    "\t\"\"\"\n",
    "\t\n",
    "\treturn X.reshape(X.shape[0],int(X.shape[1]/nchannels),nchannels) # x: row, y: time, z: band\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def read_minMaxVal(file):\n",
    "\t\n",
    "\twith open(file, 'r') as f:\n",
    "\t\treader = csv.reader(f, delimiter=',')\n",
    "\t\tmin_per = next(reader)\n",
    "\t\tmax_per = next(reader)\n",
    "\tmin_per = [float(k) for k in min_per]\n",
    "\tmin_per = np.array(min_per)\n",
    "\tmax_per = [float(k) for k in max_per]\n",
    "\tmax_per = np.array(max_per)\n",
    "\treturn min_per, max_per\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def save_minMaxVal(file, min_per, max_per):\n",
    "\t\n",
    "\twith open(file, 'w') as f:\n",
    "\t\twriter = csv.writer(f, delimiter=',')\n",
    "\t\twriter.writerow(min_per)\n",
    "\t\twriter.writerow(max_per)\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def computingMinMax(X, per=2):\n",
    "\tmin_per = np.percentile(X, per, axis=(0,1))\n",
    "\tmax_per = np.percentile(X, 100-per, axis=(0,1))\n",
    "\treturn min_per, max_per\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def normalizingData(X, min_per, max_per):\n",
    "\treturn (X-min_per)/(max_per-min_per)\n",
    "\n",
    "def class_mapping(y_label):\n",
    "\t\"\"\"\n",
    "\t\"\"\"\n",
    "\tunique_class = np.unique(y_label)\n",
    "\tnclass = len(unique_class)\n",
    "\tmax_ylabel = np.unique(y_label)[-1]+1 #-- +1 to take into account the case where y=0\n",
    "\t\n",
    "\tclass_map = [0]*max_ylabel\n",
    "\trevert_class_map = unique_class.tolist()\n",
    "\t#-- Insert in class_map values from 1 to c, with c the number of classes\n",
    "\tn = nclass\n",
    "\twhile n>0:\n",
    "\t\tinsert_val = revert_class_map[n-1]\n",
    "\t\tclass_map[insert_val] = n\n",
    "\t\tn = n-1\t\n",
    "\treturn class_map, revert_class_map\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def read_class_map(file):\n",
    "\t\n",
    "\twith open(file, 'r') as f:\n",
    "\t\treader = csv.reader(f, delimiter=',')\n",
    "\t\tclass_map = next(reader)\n",
    "\t\trevert_class_map = next(reader)\n",
    "\tclass_map = [int(k) for k in class_map]\n",
    "\trevert_class_map = [int(k) for k in revert_class_map]\n",
    "\treturn class_map, revert_class_map\n",
    "\n",
    "\n",
    "def save_class_map(file, class_map, revert_class_map):\n",
    "\t\n",
    "\twith open(file, 'w') as f:\n",
    "\t\twriter = csv.writer(f, delimiter=',')\n",
    "\t\twriter.writerow(class_map)\n",
    "\t\twriter.writerow(revert_class_map)\n",
    "\n",
    "nchannels = 10\n",
    "\n",
    "# out_path = options.output\n",
    "# model_file = options.model\n",
    "# in_img = options.in_img # -i 54HWE_img.tif\n",
    "# ref_file = options.ref_file #-t 54HWE_train.sqlite\n",
    "\n",
    "model_name = model_file.split('/')[-1]\n",
    "model_name = model_name.split('.')[0]\n",
    "\n",
    "image_name = in_img.split('/')\n",
    "image_name = image_name[-1].split('_')[0]\n",
    "\n",
    "# out_map = out_path + '/' + image_name + '_' + model_name + '_map' + '.tif'\n",
    "# print(\"out_map: \", out_map)\n",
    "# if os.path.exists(out_map):\n",
    "# \tprint(\"out_map \",out_map,\"already exists => exit\")\n",
    "# \tsys.exit(\"\\n*** not overwriting out_map ***\\n\")\n",
    "\n",
    "out_confmap = out_path + '/' + image_name + '_' + model_name + '_proba' + '.tif'\n",
    "\n",
    "\n",
    "model = joblib.load(model_file)\n",
    "\n",
    "flag_del = False #-- deleting the training data\n",
    "class_map_file = '.'.join(ref_file.split('.')[0:-1])\n",
    "class_map_file = class_map_file + '_classMap.txt'\n",
    "print(\"class_map_file: \", class_map_file)\n",
    "if not os.path.exists(class_map_file): \n",
    "\tX_train, y_train = load_npz(ref_file)\n",
    "\tclass_map, revert_class_map = class_mapping(y_train)\n",
    "\tsave_class_map(class_map_file, class_map, revert_class_map)\n",
    "\tflag_del = True\n",
    "else:\n",
    "\tclass_map, revert_class_map = read_class_map(class_map_file)\n",
    "\n",
    "print(\"class_map: \", class_map)\n",
    "print(\"revert_class_map: \", revert_class_map)\n",
    "\n",
    "if flag_del:\n",
    "\tdel X_train\n",
    "\tdel y_train\n",
    "\n",
    "#get image info about gps coordinates for origin plus size pixels\n",
    "image = gdal.Open(in_img, gdal.GA_ReadOnly) #, NUM_THREADS=8\n",
    "geotransform = image.GetGeoTransform()\n",
    "originX = geotransform[0]\n",
    "originY = geotransform[3]\n",
    "spacingX = geotransform[1]\n",
    "spacingY = geotransform[5]\n",
    "r, c = image.RasterYSize, image.RasterXSize\n",
    "out_raster_SRS = osr.SpatialReference()\n",
    "out_raster_SRS.ImportFromWkt(image.GetProjectionRef())\n",
    "\n",
    "print(\"r=\", r, \" -- c=\", c)\n",
    "print(\"originX: \", originX)\n",
    "print(\"originY: \", originY)\n",
    "print(\"spacingX: \", spacingX)\n",
    "print(\"spacingY: \", spacingY)\n",
    "print(\"geotransform: \", geotransform)\n",
    "\n",
    "#-- Set up the characteristics of the output image\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "# out_map_raster = driver.Create(out_map, c, r, 1, gdal.GDT_Byte)\n",
    "# out_map_raster.SetGeoTransform([originX, spacingX, 0, originY, 0, spacingY])\n",
    "# out_map_raster.SetProjection(out_raster_SRS.ExportToWkt())\n",
    "# out_map_band = out_map_raster.GetRasterBand(1)\n",
    "\n",
    "out_confmap_raster = driver.Create(out_confmap, c, r, 1, gdal.GDT_Float32)\n",
    "out_confmap_raster.SetGeoTransform([originX, spacingX, 0, originY, 0, spacingY])\n",
    "out_confmap_raster.SetProjection(out_raster_SRS.ExportToWkt())\n",
    "out_confmap_band = out_confmap_raster.GetRasterBand(1)\n",
    "\n",
    "\n",
    "#convert gps corners into image (x,y)\n",
    "# def gps_2_image_xy(x,y):\n",
    "# \treturn (x-originX)/spacingX,(y-originY)/spacingY\n",
    "# def gps_2_image_p(point):\n",
    "# \treturn gps_2_image_xy(point[0],point[1])\n",
    "\n",
    "size_areaX = 256\n",
    "size_areaY = 256\n",
    "x_vec = list(range(int(c/size_areaX)))\n",
    "x_vec = [x*size_areaX for x in x_vec]\n",
    "y_vec = list(range(int(r/size_areaY)))\n",
    "y_vec = [y*size_areaY for y in y_vec]\n",
    "x_vec.append(c)\n",
    "y_vec.append(r)\n",
    "\n",
    "for x in range(len(x_vec)-1):\n",
    "\tfor y in range(len(y_vec)-1):\n",
    "\t\n",
    "\t\txy_top_left = (x_vec[x],y_vec[y])\n",
    "\t\txy_bottom_right = (x_vec[x+1],y_vec[y+1])\n",
    "\t\t\n",
    "\t\tprint('top_left=',xy_top_left,' to bottom_right=',xy_bottom_right)\n",
    "\n",
    "\t\t#now loading associated data\n",
    "\t\txoff = xy_top_left[0]\n",
    "\t\tyoff = xy_top_left[1]\n",
    "\t\txsize = xy_bottom_right[0]-xy_top_left[0]\n",
    "\t\tysize = xy_bottom_right[1]-xy_top_left[1]\n",
    "\t\tstart_time = time.time()\n",
    "\t\tX_test = image.ReadAsArray(xoff=xoff, yoff=yoff, xsize=xsize, ysize=ysize) #, gdal.GDT_Float32\n",
    "\t\tprint(\"Reading: \", time.time()-start_time)\n",
    "        \n",
    "        #-- reshape the cube in a column vector\n",
    "\t\tX_test = X_test.transpose((1,2,0))\n",
    "\t\tsX = X_test.shape[0]\n",
    "\t\tsY = X_test.shape[1]\n",
    "\t\tX_test = X_test.reshape(X_test.shape[0]*X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "\t\tstart_time = time.time()\n",
    "\t\tp_img = model.predict_proba(X_test)\n",
    "\t\tprint(\"Prediction: \", time.time()-start_time)\n",
    "\n",
    "\t\ty_test = p_img.argmax(axis=1)\n",
    "\t\ty_prob = p_img.max(axis=1)\n",
    "\n",
    "        confpred_array = y_prob.reshape(sX,sY)\n",
    "\t\ty_test = [revert_class_map[k] for k in y_test]\n",
    "\t\ty_test = np.array(y_test, dtype=np.uint8)\n",
    "\t\tpred_array = y_test.reshape(sX,sY)\n",
    "\t\t\n",
    "\t\tstart_time = time.time()\n",
    "\t\t# out_map_band.WriteArray(pred_array, xoff=xoff, yoff=yoff)\n",
    "        out_confmap_band.WriteArray(confpred_array, xoff=xoff, yoff=yoff)\n",
    "\t\tprint(\"Writing array: \", time.time()-start_time)\n",
    "\n",
    "\t\tstart_time = time.time()\n",
    "\t\tout_map_band.FlushCache()\n",
    "        out_confmap_band.FlushCache()\n",
    "\t\tprint(\"Writing disk: \", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300e3a5e-2eec-4a9b-86af-a3391ee0be7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_left= (0, 0)  to bottom_right= (256, 256)\n",
      "Reading:  3.9295737743377686\n",
      "top_left= (256, 0)  to bottom_right= (512, 256)\n",
      "Reading:  0.09470653533935547\n",
      "top_left= (512, 0)  to bottom_right= (768, 256)\n",
      "Reading:  0.09515547752380371\n",
      "top_left= (768, 0)  to bottom_right= (1024, 256)\n",
      "Reading:  0.10379409790039062\n",
      "top_left= (1024, 0)  to bottom_right= (1280, 256)\n",
      "Reading:  0.10392594337463379\n",
      "top_left= (1280, 0)  to bottom_right= (1536, 256)\n",
      "Reading:  0.1144721508026123\n",
      "top_left= (1536, 0)  to bottom_right= (1792, 256)\n",
      "Reading:  0.1111917495727539\n",
      "top_left= (1792, 0)  to bottom_right= (2048, 256)\n",
      "Reading:  0.1030125617980957\n",
      "top_left= (2048, 0)  to bottom_right= (2304, 256)\n",
      "Reading:  0.10456609725952148\n",
      "top_left= (2304, 0)  to bottom_right= (2560, 256)\n",
      "Reading:  0.10457491874694824\n",
      "top_left= (2560, 0)  to bottom_right= (2816, 256)\n",
      "Reading:  0.09899020195007324\n",
      "top_left= (2816, 0)  to bottom_right= (3072, 256)\n",
      "Reading:  0.09741687774658203\n",
      "top_left= (3072, 0)  to bottom_right= (3328, 256)\n",
      "Reading:  0.09719967842102051\n",
      "top_left= (3328, 0)  to bottom_right= (3584, 256)\n",
      "Reading:  0.09722495079040527\n",
      "top_left= (3584, 0)  to bottom_right= (3840, 256)\n",
      "Reading:  0.09736061096191406\n",
      "top_left= (3840, 0)  to bottom_right= (4096, 256)\n",
      "Reading:  0.09717988967895508\n",
      "top_left= (4096, 0)  to bottom_right= (4352, 256)\n",
      "Reading:  0.09717130661010742\n",
      "top_left= (4352, 0)  to bottom_right= (4608, 256)\n",
      "Reading:  0.09708786010742188\n",
      "top_left= (4608, 0)  to bottom_right= (4864, 256)\n",
      "Reading:  0.09711670875549316\n",
      "top_left= (4864, 0)  to bottom_right= (5120, 256)\n",
      "Reading:  0.09717965126037598\n",
      "top_left= (5120, 0)  to bottom_right= (5376, 256)\n",
      "Reading:  0.09712576866149902\n",
      "top_left= (5376, 0)  to bottom_right= (5632, 256)\n",
      "Reading:  0.09717679023742676\n",
      "top_left= (5632, 0)  to bottom_right= (5888, 256)\n",
      "Reading:  0.09812068939208984\n",
      "top_left= (5888, 0)  to bottom_right= (6144, 256)\n",
      "Reading:  0.09813785552978516\n",
      "top_left= (6144, 0)  to bottom_right= (6400, 256)\n",
      "Reading:  0.09704399108886719\n",
      "top_left= (6400, 0)  to bottom_right= (6656, 256)\n",
      "Reading:  0.09731292724609375\n",
      "top_left= (6656, 0)  to bottom_right= (6912, 256)\n",
      "Reading:  0.09738588333129883\n",
      "top_left= (6912, 0)  to bottom_right= (7168, 256)\n",
      "Reading:  0.09728288650512695\n",
      "top_left= (7168, 0)  to bottom_right= (7424, 256)\n",
      "Reading:  0.09760332107543945\n",
      "top_left= (7424, 0)  to bottom_right= (7680, 256)\n",
      "Reading:  0.10062932968139648\n",
      "top_left= (7680, 0)  to bottom_right= (7936, 256)\n",
      "Reading:  0.10380148887634277\n",
      "top_left= (7936, 0)  to bottom_right= (8192, 256)\n",
      "Reading:  0.10530233383178711\n",
      "top_left= (8192, 0)  to bottom_right= (8448, 256)\n",
      "Reading:  0.09976553916931152\n",
      "top_left= (8448, 0)  to bottom_right= (8704, 256)\n",
      "Reading:  0.09757709503173828\n",
      "top_left= (8704, 0)  to bottom_right= (8960, 256)\n",
      "Reading:  0.09776687622070312\n",
      "top_left= (8960, 0)  to bottom_right= (9216, 256)\n",
      "Reading:  0.09937548637390137\n",
      "top_left= (9216, 0)  to bottom_right= (9472, 256)\n",
      "Reading:  0.10210919380187988\n",
      "top_left= (9472, 0)  to bottom_right= (9728, 256)\n",
      "Reading:  0.10537505149841309\n",
      "top_left= (9728, 0)  to bottom_right= (9984, 256)\n",
      "Reading:  0.1055448055267334\n",
      "top_left= (9984, 0)  to bottom_right= (10240, 256)\n",
      "Reading:  0.09663510322570801\n",
      "top_left= (10240, 0)  to bottom_right= (10496, 256)\n",
      "Reading:  0.09656262397766113\n",
      "top_left= (10496, 0)  to bottom_right= (10980, 256)\n",
      "Reading:  0.11275005340576172\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(x_vec)-1):\n",
    "\tfor y in range(len(y_vec)-1):\n",
    "\t\n",
    "\t\txy_top_left = (x_vec[x],y_vec[y])\n",
    "\t\txy_bottom_right = (x_vec[x+1],y_vec[y+1])\n",
    "\t\t\n",
    "\t\tprint('top_left=',xy_top_left,' to bottom_right=',xy_bottom_right)\n",
    "\n",
    "\t\t#now loading associated data\n",
    "\t\txoff = xy_top_left[0]\n",
    "\t\tyoff = xy_top_left[1]\n",
    "\t\txsize = xy_bottom_right[0]-xy_top_left[0]\n",
    "\t\tysize = xy_bottom_right[1]-xy_top_left[1]\n",
    "\t\tstart_time = time.time()\n",
    "\t\tX_test = image.ReadAsArray(xoff=xoff, yoff=yoff, xsize=xsize, ysize=ysize) #, gdal.GDT_Float32\n",
    "\t\tprint(\"Reading: \", time.time()-start_time)\n",
    "\n",
    "\t\tX_test\n",
    "\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df811f55-a0f1-4bb5-a7e0-3bab0ce0ccff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 256, 484)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09bd8ef1-dde6-4c38-99c6-66b4897e029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ = X_test.transpose((1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e76da67f-3ac4-4e75-851b-75f8bdabbab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sX = X_test_.shape[0]\n",
    "sY = X_test_.shape[1]\n",
    "X_test_ = X_test_.reshape(X_test_.shape[0]*X_test_.shape[1],X_test_.shape[2])\n",
    "p_img = model.predict_proba(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d38ea7-a4b2-4cc7-832b-a17abb2c16c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = p_img.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa1ea41b-926e-4654-896f-b07049a0e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = p_img.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc54b228-02bb-41d6-a2ca-f5daaebb181c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 484)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hs.reshape(sX,sY)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e8aea07-c85a-4c32-a32e-dafb6ccf146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hy = p_img + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ced256fb-4c0c-4666-8748-bfa4ee6b7928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.34332428126266"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(p_img -hy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b84a79-828e-458e-b584-7fbd5ccef57c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
